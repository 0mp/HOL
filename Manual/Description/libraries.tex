\chapter{Libraries}\label{HOLlibraries}

% LaTeX macros in HOL manuals
%
% \holtxt{..}     for typewriter text that is HOL types or terms.  To
%                 produce backslashes, for /\, \/ and \x. x + 1, use \bs
% \ml{..}         for typewriter text that is ML input, including the
%                 names of HOL API functions, such as mk_const
% \theoryimp{..}  for names of HOL theories.


\newcommand{\simpset}{simpset}
\newcommand{\Simpset}{Simpset}

{
 \newcommand{\term}      {\mbox{\it term}}
 \newcommand{\vstr}      {\mbox{\it vstr}}

A \emph{library} is intended to provide a higher level of
organization for \HOL{} applications. In general, a library
can contain a collection of theories, proof procedures,
and supporting material, such as documentation. Some libraries
simply provide proof procedures, such as \ml{simpLib},
while others provide theories and proof procedures, such as
\ml{intLib}. Libraries can include other libraries.

In the \HOL{} system, libraries are represented by \ML{} structures
named following the convention that library \emph{x} will be found in
the \ML{} structure \ml{xLib}. Loading this structure should load all
the relevant sub-components of the library and set whatever system
parameters are suitable for use of the library.

When the \HOL{} system is invoked in its normal configuration, several
useful libraries are automatically loaded. The most basic \HOL{}
library is \ml{boolLib}, which supports the definitions of
the \HOL{} logic, found in the theory \theoryimp{bool}, and provides a
useful suite of definition and reasoning tools.

Another pervasively used library is found in the structure \ml{Parse}
(the reader can see that we are not strictly faithful to our
convention about library naming). The parser library provides support
for parsing and `pretty-printing' of \HOL{} types, terms, and
theorems.

The \ml{boss} library provides a basic collection of standard
theories and high-level proof procedures, and serves as a standard
platform on which to work. It is preloaded and opened when the \HOL{}
system starts up. It includes \ml{boolLib} and
\ml{Parse}. Theories provided include \theoryimp{pair},
\theoryimp{sum}, \theoryimp{option}; the arithmetic theories
\theoryimp{num}, \theoryimp{prim\_rec}, \theoryimp{arithmetic},
and \theoryimp{numeral}; and \theoryimp{list}. Other libraries
included in \ml{bossLib} are \ml{goalstackLib}, which provides
a proof manager for tactic proofs; \ml{simpLib}, which provides
a variety of simplifiers; \ml{numLib}, which provides a decision
procedure for arithmetic; \ml{Datatype}, which provides
high-level support for defining algebraic datatypes; and \ml{tflLib},
which provides support for defining recursive functions.


\section{Parsing and Prettyprinting}

\HOL{} types and terms must be well-typed; the \HOL{} implementation
depends on the strong type system of SML to ensure this property.
Every type and term is ultimately built by application of the
primitive (abstract) constructors for types and terms. However, in
order to accomodate a wide variety of mathematical expression,
\HOL{} provides flexible infrastracture for parsing and prettyprinting
types and terms through the \ml{Parse} structure.

The term parser supports type inference, overloading, binders, and
various fixity declaration (infix, prefix, postfix, and
combinations). There are also flags for controlling the behaviour
of the parser. Further, the structure of the parser is exposed so that
new parsers can be quickly constructed to support applications.

The parser is parameterized by grammars for types and terms. The
behaviour of the parser is therefore usually altered by grammar
manipulations. These can be of two kinds: \emph{temporary} or
\emph{permanent}.  Temporary changes do not persist after the current
session, while permanent changes will be in force in all descendant
theories.  Functions making temporary alterations are signified
by a leading \ml{temp\_} to their names.

\subsection{Parsing Types}

There is little to say about the type parser. Infixes may be
introduced with the function \ml{add\_infix\_type} (see \REFERENCE{}
for more details).  The function \ml{type\_grammar} may be used to
see the current grammar for types.

\subsection{Parsing Terms}

Usually, the \HOL{} grammar gets extended when a new definition or
constant specification is made. However, any identifier can have a
parsing status attached. The introduction of new constants is
discussed in Sections~\ref{sec:constant-definitions} and
\ref{conspec}.


\subsubsection{Type constraints}

A term can be constrained to be of a certain type.  For example,
\verb+X:bool+ constrains the variable \verb+X+ to have type
\verb+bool+. Similarly, \verb+T:bool+ performs a (vacuous) constraint
of the constant \verb+T+ to \verb+bool+. An attempt to constrain a
term inappropriately will raise an exception: for example,
\begin{verbatim}
   if T then (X:ind) else (Y:bool)
\end{verbatim}
will fail because both branches of a conditional must be of the same
type.  Type constraints can be seen as a suffix that binds more
tightly than everything except function application.  Thus $\term\
\ldots\ \term \ : \type$ is equal to $(\term\ \ldots\ \term)\ :
\type$, but $x < y:\holtxt{num}$ is a legitimate constraint on just
the variable $y$.

The inclusion of \holtxt{:} in the symbolic identifiers means that some
constraints may need to be separated by white space. For example,
\begin{verbatim}
   $=:bool->bool->bool
\end{verbatim}
will be broken up by the \HOL{} lexer as
\begin{verbatim}
   $=: bool -> bool -> bool
\end{verbatim}
and parsed as an application of the symbolic identifier \holtxt{\$=:} to
the argument list of terms [\holtxt{bool}, \holtxt{->}, \holtxt{bool},
\holtxt{->}, \holtxt{bool}]. A well-placed space will avoid this problem:
\begin{verbatim}
   $= :bool->bool->bool
\end{verbatim}
is parsed as the symbolic identifier ``='' constrained by a type.
Instead of the \holtxt{\$}, one can also use parentheses to remove
special parsing behaviour from lexemes:
\begin{verbatim}
   (=):bool->bool->bool
\end{verbatim}

\subsubsection{Type inference}

Consider the term \holtxt{x = T}.  Each term (and all of its subterms),
has a type in the \HOL{} logic. Now, \holtxt{T} has type \holtxt{bool}. This
means that the constant \holtxt{=} has type \holtxt{xty -> bool -> bool},
for some type \holtxt{xty}. Since the type scheme for \holtxt{=} is
\holtxt{'a -> 'a -> bool}, we know that \holtxt{xty} must in fact be
\holtxt{bool} in order for the type instance to be well-formed. Knowing
this, we can deduce that the type of \holtxt{x} must be \holtxt{bool}.

Ignoring the jargon (``scheme'' and ``instance'') in the previous
paragraph, we have conducted a type assignment to the term structure,
ending up with a well-typed term. It would be very tedious for users
to conduct such argumentation by hand for each term entered to \HOL{}.
Thus, \HOL{} uses an adaptation of Milner's type inference algorithm
for \ML{} when constructing terms via parsing. At the end of type
inference, unconstrained type variables get assigned by the system.
Usually, this assignment does the right thing. However, at times, the
most general type is not what is desired and the user must add type
constraints to the relevant subterms. For tricky situations, the
global variable \ml{show\_types} can be assigned. When this flag is
set, the prettyprinters for terms and theorems will show how types
have been assigned to subterms. If you do not want the system to
assign type variables for you, the global variable
\ml{guessing\_tyvars} can be set to \ml{false}, in which case the
existence of unassigned type variables at the end of type inference
will raise an exception.

\subsubsection{Overloading}

A limited amount of overloading resolution is performed by the quotation
parser for terms. For example, the tilde symbol (\holtxt{\~{}})
denotes boolean negation in the initial theory of \HOL, and it also denotes
the additive inverse in the \ml{integer} and
\ml{real} theories. If we load the \ml{integer}
theory and enter an ambiguous term featuring \holtxt{\~{}}, the
system will inform us that overloading resolution is being performed.

\setcounter{sessioncount}{1}
\begin{session}
\begin{verbatim}
- load "integerTheory";
> val it = () : unit

- Term `~~x`;
<<HOL message: more than one resolution of overloading was possible.>>
> val it = `~~x` : term

- type_of it;
> val it = `:bool` : hol_type
\end{verbatim}
\end{session}

A priority mechanism is used to resolve multiple possible choices. In
the example, \holtxt{\~{}} could be consistently chosen to have type
\holtxt{:bool -> bool} or \holtxt{:int -> int}, and the
mechanism has chosen the former. For finer control, explicit type
constraints may be used. In the following session, the
\holtxt{\~{}\~{}x} in the first quotation has type \holtxt{:bool},
while in the second, a type constraint ensures that \holtxt{\~{}\~{}x} has
type \holtxt{:int}.

\begin{session}
\begin{verbatim}
- show_types := true;
> val it = () : unit

- Term `~(x = ~~x)`;
<<HOL message: more than one resolution of overloading was possible.>>
> val it = `~((x :bool) = ~~x)` : term

- Term `~(x:int = ~~x)`;
> val it = `~((x :int) = ~~x)` : term
\end{verbatim}
\end{session}

Note that the symbol \holtxt{\~{}} stands for two different constants in
the second quotation; its first occurrence is boolean negation, while
the other two occurrences are the additive inverse operation for integers.
For more information on how to set up and use overloading, consult
\REFERENCE.

\subsubsection{Fixities}

In order to provide some notational flexibility, constants come in
various flavours or {\it fixities}: besides being an ordinary constant
(with a fixity of {\sf Prefix}), constants can also be {\it binders},
{\it true prefixes}\footnote{The use of the term ``true prefix'' is
forced upon us by the history of the system, which reserved the
classification ``prefix'' for terms without any special syntactic
features.}, {\it suffixes}, {\it infixes}, or {\it closefixes}.  More
generally, terms can also be represented using reasonably arbitrary
{\it mixfix} specifications.  The degree to which terms bind their
associated arguments is known as precedence.  The higher this number,
the tighter the binding.  For example, when introduced, \verb-+- has a
precedence of 500, while the tighter binding multiplication (\verb+*+)
has a precedence of 600.

\paragraph{Binders}

A binder is a construct that binds a variable; for example, the
universal quantifier. In \HOL, this is represented using a trick that
goes back to Alonzo Church: a binder is a constant that takes a lambda
abstraction as its argument. The lambda binding is used to implement
the binding of the construct. This is an elegant and uniform solution.
Thus the concrete syntax \verb+!v. M+ is represented by the
application of the constant \verb+!+ to the abstraction \verb+(\v. M)+.

The most common binders are \verb+!+, \verb+?+, \verb+?!+, and
\verb+@+. Sometimes one wants to iterate applications of the same
binder, \eg,
\begin{alltt}
  !x. !y. ?p. ?q. ?r. \term.
\end{alltt}
This can instead be rendered
\begin{alltt}
  !x y. ?p q r. \term.
\end{alltt}

\paragraph{Infixes}

Infix constants can associate in one of three different ways: right,
left or not at all.  (If \holtxt{+} were non-associative, then
\holtxt{3 + 4 + 5} would fail to parse; one would have to write
\holtxt{(3 + 4) + 5} or \holtxt{3 + (4 + 5)} depending on the desired
meaning.)  The precedence ordering for the initial set of infixes is
\holtxt{/\bs}, \holtxt{\bs/}, \holtxt{==>}, \holtxt{=},
\begin{Large}\holtxt{,}\end{Large} (comma\footnote{When
  \theoryimp{pairTheory} has been loaded.}). Moreover, all of these
constants are right associative. Thus
\begin{verbatim}
     X /\ Y ==> C \/ D, P = E, Q
\end{verbatim}
 is equal to
\begin{verbatim}
     ((X /\ Y) ==> (C \/ D)), ((P = E), Q).
\end{verbatim}

\noindent An expression
\[
\term \; \holtxt{<infix>}\; \term
\]
is internally represented as
\[
((\holtxt{<infix>}\; \term)\; \term)
\]

\paragraph{True prefixes}

Where infixes appear between their arguments, true prefixes appear
before theirs.  This might initially appear to be the same thing as
happens with normal function application (is $f$ in $f(x)$ not acting
as a prefix?), but in fact, it is useful to allow for prefixes to have
binding power less than that associated with function application.  An
example of this is \verb+~+, logical negation.  This is a prefix with
lower precedence than function application.  Normally
\[
   f\;x\; y\qquad \mbox{is parsed as}\qquad (f\; x)\; y
\] but \[
  \holtxt{\~{}}\; x\; y\qquad\mbox{is parsed as}\qquad
  \holtxt{\~{}}\; (x\; y)
\] because the precedence of \verb+~+ is lower than that of function
application.  The unary negation symbol would also typically be
defined as a true prefix, if only to allow one to write \[ {\it
  negop}\,{\it negop}\,3
\] (whatever {\it negop} happened to be) without needing extra parentheses.

\paragraph{Suffixes}

Suffixes appear after their arguments.  There are no suffixes
introduced into the standard theories available in \HOL{}, but users
are always able to introduce their own if they choose.  Suffixes are
associated with a precedence just as infixes and true prefixes are.
If \holtxt{p} is a true prefix, \holtxt{i} an infix, and \holtxt{s} a
suffix, then there are six possible orderings for the three different
operators based on their precedences, giving five parses for
$\holtxt{p}\; t_1\; \holtxt{i}\; t_2\; \holtxt{s}$ depending on the
relative precedences:
\[
\begin{array}{cl}
\mbox{\begin{tabular}{c}Precedences\\(lowest to highest)\end{tabular}} &
\multicolumn{1}{c}{\mbox{Parses}}\\
\hline
p,\;i,\;s & \holtxt{p}\;(t_1\;\holtxt{i}\;(t_2\;\holtxt{s}))\\
p,\;s,\;i & \holtxt{p}\;((t_1\;\holtxt{i}\;t_2)\;\holtxt{s})\\
i,\;p,\;s & (\holtxt{p}\;t_1)\;\holtxt{i}\;(t_2\;\holtxt{s})\\
i,\;s,\;p & (\holtxt{p}\;t_1)\;\holtxt{i}\;(t_2\;\holtxt{s})\\
s,\;p,\;i & (\holtxt{p}\;(t_1\;\holtxt{i}\;t_2))\;\holtxt{s}\\
s,\;i,\;p & ((\holtxt{p}\;t_1)\;\holtxt{i}\;t_2)\;\holtxt{s}\\
\end{array}
\]

\paragraph{Closefixes}

Closefix terms are operators that completely enclose their arguments.
An example one might use in the development of a theory of
denotational semantics is semantic brackets.  Thus, the \HOL{} parsing
facilities can be configured to allow one to write \holtxt{denotation x}
as \holtxt{[| x |]}.  Closefixes are not associated with precedences
because they can not compete for arguments with other operators.


\subsubsection{Parser tricks and magic}

Here we describe how to achieve some useful effects with the
parser in \HOL{}.

\begin{description}

\item[Aliasing] If one wants a special syntax to be an ``alias'' for a
  normal \HOL{} form, this is easy to achieve; both examples so far
  have effectively done this.  However, if one just wants to have a
  normal one-for-one substitution of one string for another, one can't
  use the grammar/syntax phase of parsing to do this.  Instead, one
  can use the overloading mechanism.  For example, let us alias
  \texttt{MEM} for \texttt{IS\_EL}.  We need to use the function
  \texttt{overload\_on} to overload the original constant for the new
  name:
\begin{verbatim}
   val _ = overload_on ("MEM", Term`IS_EL`);
\end{verbatim}

\item[Making addition right associative] If one has a number of old
  scripts that assume addition is right associative because this is
  how \HOL{} used to be, it might be too much pain to convert.  The trick
  is to remove all of the rules at the given level of the grammar, and
  put them back as right associative infixes.  The easiest way to tell
  what rules are in the grammar is by inspection (use
  \ml{term\_grammar()}).  With just \ml{arithmeticTheory}
  loaded, the only infixes at level 500 are \holtxt{+} and
  \holtxt{-}.  So, we remove the rules for them:
\begin{verbatim}
   val _ = app temp_remove_rules_for_term ["+", "-"];
\end{verbatim}
  \noindent And then we put them back with the appropriate
  associativity:
\begin{verbatim}
   val _ = app (fn s => temp_add_infix(s, 500, RIGHT)) ["+", "-"];
\end{verbatim}
\noindent Note that we use the \ml{temp\_} versions of these two
functions so that other theories depending on this one won't be
affected.  Further note that we can't have two infixes at the same
level of precedence with different associativities, so we have to
remove both operators, not just addition.

\item[Mix-fix syntax for {\it if-then-else}:]
\index{conditionals, in HOL logic@conditionals, in \HOL{} logic!printing of}
%
The first step in bringing this about is to look at the general shape
of expressions of this form.  In this case, it will be:
%
\[
  \holtxt{if}\;\; \dots \;\;\holtxt{then}\;\;\dots\;\;
  \holtxt{else}\;\;\dots
  \]
%
 Because there needs to be a ``dangling'' term to the right, the
  appropriate fixity is \ml{TruePrefix}.  Knowing that the underlying
  term constant is called \holtxt{COND}, the simplest way to achieve
  the desired syntax is:

\begin{verbatim}
   val _ = add_rule{term_name = "COND", fixity = TruePrefix 70,
                    pp_elements = [TOK "if", BreakSpace(1,0), TM,
                                   BreakSpace(1,0),
                                   TOK "then", BreakSpace(1,0), TM,
                                   BreakSpace(1,0),
                                   TOK "else", BreakSpace(1,0)],
                    paren_style = Always,
                    block_style =
                      (AroundEachPhrase, (PP.CONSISTENT, 0))};
\end{verbatim}
\noindent The actual rule is slightly more complicated, and
may be found in the sources for the theory \theoryimp{bool}.

\item[Mix-fix syntax for term substitution:]

 Here the desire is to be able
  to write something like:
  \[
  \mbox{\texttt{[}}\,t_1\,\mbox{\texttt{/}}\,t_2\,\mbox{\texttt{]}}\,t_3
  \]
  denoting the substitution of $t_1$ for $t_2$ in $t_3$, perhaps
  translating to \holtxt{SUB $t_1$ $t_2$ $t_3$}.  This looks
like it should be another \ml{TruePrefix}, but the choice of the
square brackets (\holtxt{[} and \holtxt{]}) as delimiters would
conflict with the concrete syntax for list literals if this was done.
Given that list literals are effectively of the \ml{CloseFix}
class, the new syntax must be of the same class.  This is easy enough
to do: we set up syntax
\[
\holtxt{[}\,t_1\,\holtxt{/}\,t_2\,\holtxt{]}
\]
to map to \holtxt{SUB $t_1$ $t_2$}, a value of a functional
type, that when applied to a third argument will look
right.\footnote{Note that doing the same thing for the
  \textit{if-then-else} example in the previous example would be
  inappropriate, as it would allow one to write \[
  \holtxt{if}\;P\;\holtxt{then}\;Q\;\holtxt{else}
  \] without the trailing argument}
  The rule for this is thus:
\begin{verbatim}
   val _ = add_rule {term_name = "SUB", fixity = Closefix,
                     pp_elements = [TOK "[", TM, TOK "/", TM, TOK "]"],
                     paren_style = OnlyIfNecessary,
                     block_style =
                       (AroundEachPhrase, (PP.INCONSISTENT, 2))};
\end{verbatim}


\end{description}

\subsubsection{Hiding constants}
\label{hidden}

\index{parsing, of HOL logic@parsing, of \HOL{} logic!hiding constant status in|(}
\index{HOL system@\HOL{} system!hiding constants in|(}
\index{constants, in HOL logic@constants, in \HOL{} logic!hiding status of|(}
%
The following function can be used to hide the constant status of a
name from the quotation parser.

\begin{boxed}
\index{hide@\ml{hide}|pin}
\begin{verbatim}
  val hide   : string -> ({Name : string, Thy : string} list *
                          {Name : string, Thy : string} list)
\end{verbatim}
\end{boxed}

\noindent Evaluating \ml{hide "$x$"}
makes the quotation parser treat $x$ as a variable (lexical
rules permitting), even if $x$ is the name of a constant in the current theory
(constants and variables can have the same name).
This is useful if one wants to use variables
%
\index{variables, in HOL logic@variables, in \HOL{} logic!with constant names}
%
with the same names as previously declared (or built-in) constants
(\eg\ \ml{o}, \ml{I}, \ml{S} \etc).  The name $x$ is still a constant
for the constructors, theories, etc; \ml{hide} affects only parsing.
See the \REFERENCE{} entry for \ml{hide} for more details, including
an explanation of the return type.

The function

\begin{boxed}
\index{reveal@\ml{reveal}|pin}
\begin{verbatim}
   reveal : string -> unit
\end{verbatim}
\end{boxed}

\noindent undoes hiding.

The function

\begin{boxed}
\index{hidden@\ml{hidden}|pin}
\begin{verbatim}
   hidden : string -> bool
\end{verbatim}
\end{boxed}

\noindent tests whether a string is the name of a hidden constant.
\index{HOL system@\HOL{} system!adjustment of user interface of|)}
\index{HOL system@\HOL{} system!hiding constants in|)}
\index{parsing, of HOL logic@parsing, of \HOL{} logic!hiding constant status in|)}

\subsubsection{Adjusting the pretty-print depth}
\index{printing, in HOL logic@printing, in \HOL{} logic!structural depth adjustment in}

The following \ML{} reference can be used to adjust the maximum depth
of printing

\begin{boxed}
\index{max_print_depth@\ml{max\_print\_depth}|pin}
\begin{verbatim}
   max_print_depth : int ref
\end{verbatim}
\end{boxed}

\index{default print depth, for HOL logic@default print depth, for \HOL{} logic|(}
\noindent The default print depth is $-1$, which is interpreted as
meaning no maximum.  Subterms nested more deeply than the maximum
print depth are printed as \holtxt{...}. For example:

\setcounter{sessioncount}{1}
\begin{session}
\begin{verbatim}
- ADD_CLAUSES;
> val it =
    |- (0 + m = m) /\ (m + 0 = m) /\ (SUC m + n = SUC (m + n)) /\
       (m + SUC n = SUC (m + n)) : thm

- max_print_depth := 3;
> val it = () : unit
- ADD_CLAUSES;
> val it = |- (... + ... = m) /\ (... = ...) /\ ... /\ ... : thm
\end{verbatim}
\end{session}
\index{default print depth, for HOL logic@default print depth, for \HOL{} logic|)}

}

\subsection{Backwards compatibility of syntax}

This section of the manual documents the (extensive) changes made to
the parsing of \HOL{} terms and types in the Taupo release (one of the
HOL3 releases) and beyond from the point of view of a user who doesn't
want to know how to use the new facilities, but wants to make sure
that their old code continues to work cleanly.

The changes which may cause old terms to fail to parse are:
\begin{itemize}
\newcommand\condexp{\holtxt{$p$ => $q$ | $r$}}
\item The precedence of type annotations has completely changed.  It
  is now a very tight suffix (though with a precedence weaker than
  that associated with function application), instead of a weak one.
  This means that \mbox{\tt (x,y:bool \# bool)} should now be written
  as \mbox{\tt (x,y):bool \# bool}. The previous form will now be
  parsed as a type annotation applying to just the \verb+y+.  This
  change brings the syntax of the logic closer to that of SML and
  should make it generally easier to annotate tuples, as one can now
  write \[ (x\,:\,\tau_1,\;y\,:\,\tau_2,\dots z\,:\,\tau_n)
  \] instead of \[
  (x\,:\,\tau_1, \;(y\,:\,\tau_2, \dots (z\,:\,\tau_n)))
  \] where extra parentheses have had to be added just to allow one to
  write a frequently occurring form of constraint.
\item Most arithmetic operators are now left associative instead of
  right associative.  In particular, $+$, $-$, $*$ and {\tt DIV} are
  all left associative.  Similarly, the analogous operators in other
  numeric theories such as {\tt integer} and {\tt real} are also left
  associative.  This brings the \HOL{} parser in line with standard
  mathematical practice.
\item The binding equality in {\tt let} expressions is treated exactly
  the same way as equalities in other contexts.  In previous versions
  of \HOL, equalities in this context have a different, weak binding
  precedence.  This difference can be seen in the following expression
  which parses successfully in the old version:
  \[ {\tt let} \; x \; = \; \condexp \; {\tt
  in} \;Q \] In Taupo releases and later, this expression will not
  parse because the conditional expression binds to the left more
  weakly than the equality binds to the right, and the parser ends up
  believing that the binding between the \verb+let+ and the \verb+in+
  is not an equality after all, as it should be.
\item Old style conditional expressions in the right half of set
  comprehensions have to be parenthesised to avoid confusing the
  parser.  Thus \[
  \{ \; x \; | \; \condexp \; \}
   \qquad\mbox{must be written} \qquad
  \{ \; x \; | \; (\condexp) \; \}
  \] Better yet, {\tt if}-{\tt then}-{\tt else} syntax could be used
  for the conditional expression.
\item Some lexical categories are more strictly policed.  String
  literals (strings inside double quotes) and numerals can't be used
  unless the relevant theories have been loaded.  Nor can these
  literals be used as variables inside binding scopes.
\end{itemize}


\section{A Simple Proof Manager}\label{sec:goalstack}

The \texttt{goal stack} provides a simple interface to tactic-based
proof. When one uses tactics to decompose a proof, many intermediate
states arise; the goalstack takes care of the necessary bookeeping. The
implementation of goalstacks reported here is a re-design of Larry
Paulson's original conception.

The goalstack library is automatically loaded when \HOL{} starts up.

The abstract types \textit{goalstack} and \textit{proofs} are the
focus of backwards proof operations. The type \verb+proofs+ can be
regarded as a list of independent goalstacks. Most operations act on
the head of the list of goalstacks; there are operations so that the
focus can be changed.

\subsection{Starting a goalstack proof}

\begin{verbatim}
   g        : term quotation -> proofs
   set_goal : goal -> proofs
\end{verbatim}

Recall that the type \ml{goal} is an abbreviation for
\ml{term list * term}. To start on a new goal, one gives
\ml{set\_goal} a goal. This creates a new goalstack and makes it the
focus of further operations.

A shorthand for \ml{set\_goal} is the function \ml{g}: it
invokes the parser automatically, and it doesn't allow the the goal to
have any assumptions.

Calling \ml{set\_goal}, or \ml{g}, adds a new proof attempt to the
existing ones, \textit{i.e.}, rather than overwriting the current
proof attempt, the new attempt is stacked on top.

\subsection{Applying a tactic to a goal}

\begin{verbatim}
   expandf : tactic -> goalstack
   expand  : tactic -> goalstack
   e       : tactic -> goalstack
\end{verbatim}

How does one actually do a goalstack proof then? In most cases, the
application of tactics to the current goal is done with the function
\verb+expand+. In the rare case that one wants to apply an
{\it invalid\/} tactic, then \verb+expandf+ is used. (For an
explanation of invalid tactics, see Chapter 24 of Gordon \& Melham.) The
abbreviation \verb+e+ may also be used to expand a tactic.


\subsection{Undo}

\begin{verbatim}
   b          : unit -> goalstack
   drop       : unit -> proofs
   dropn      : int  -> proofs
   backup     : unit -> goalstack
   restart    : unit -> goalstack
   set_backup : int  -> unit
\end{verbatim}

Often (we are tempted to say {\it usually}!) one takes a wrong path
in doing a proof, or makes a mistake when setting a goal. To undo a step
in the goalstack, the function \ml{backup} and its abbreviation
\ml{b} are used. This will restore the goalstack to its previous
state.


To directly back up all the way to the original goal, the function
\ml{restart} may be used. Obviously, it is also important to get
rid of proof attempts that are wrong; for that there is \ml{drop},
which gets rid of the current proof attempt, and \ml{dropn}, which
eliminates the top $n$ proof attempts.


Each proof attempt has its own \emph{undo-list} of previous
states. The undo-list for each attempt is of fixed size (initially
12). If you wish to set this value for the current proof attempt, the
function \ml{set\_backup} can be used. If the size of the backup
list is set to be smaller than it currently is, the undo list will be
immediately truncated. You can not undo a ``proofs-level'' operation, such
as \ml{set\_goal} or \ml{drop}.

\subsection{Viewing the state of the proof manager}

\begin{verbatim}
   p            : unit -> goalstack
   status       : unit -> proofs
   top_goal     : unit -> goal
   top_goals    : unit -> goal list
   initial_goal : unit -> goal
   top_thm      : unit -> thm
\end{verbatim}

To view the state of the proof manager at any time, the functions
\ml{p} and \ml{status} can be used. The former only shows
the top subgoals in the current goalstack, while the second gives a
summary of every proof attempt.

To get the top goal or goals of a proof attempt, use \ml{top\_goal}
and \ml{top\_goals}. To get the original goal of a proof attempt,
use \ml{initial\_goal}.

Once a theorem has been proved, the goalstack that was used to derive it
still exists (including its undo-list): its main job now is to
hold the theorem. This theorem can be retrieved with
\ml{top\_thm}.

\subsection{Switch focus to a different subgoal or proof attempt}

\begin{verbatim}
   r             : int -> goalstack
   R             : int -> proofs
   rotate        : int -> goalstack
   rotate_proofs : int -> proofs
\end{verbatim}

Often we want to switch our attention to a different goal in the current
proof, or a different proof. The functions that do this are
\ml{rotate} and \ml{rotate\_proofs}, respectively. The abbreviations
\ml{r} and \ml{R} are simpler to type in.

\section{High Level Proof---\texttt{bossLib}}
% would use \ml{boss} above but it puts LaTeX into fits
\label{sec:bossLib}
\newcommand\bossLib{\ml{bossLib}}

\index{bossLib@\ml{bossLib}}
The library \bossLib\ marshalls some of the most widely used theorem
proving tools in \HOL{} and provides them with a convenient interface
for interaction. The library currently focuses on three things:
definition of datatypes and functions; high-level interactive proof
operations, and composition of automated reasoners. Loading \bossLib\
commits one to working in a context that already supplies the theories
of booleans, pairs, sums, the option type, arithmetic, and lists.


\subsection{Support for high-level proof steps}
\label{sec:high-level-proof-steps}

The following functions use information in the database to ease the
application of \HOL's underlying functionality:

\begin{verbatim}
   type_rws     : string -> thm list
   Induct       : tactic
   Cases        : tactic
   Cases_on     : term quotation -> tactic
   Induct_on    : term quotation -> tactic
\end{verbatim}

\index{type_rws@\ml{type\_rws}}
\index{TypeBase@\ml{TypeBase}}
%
The function \ml{type\_rws} will search for the given type by name in
the underlying \ml{TypeBase} database and return useful rewrite rules
for that type. The rewrite rules of the datatype are built from the
injectivity and distinctness theorems, along with the case constant
definition. The simplification tactics \ml{RW\_TAC}, \ml{SRW\_TAC},
and the \simpset{} \ml{(srw\_ss())} automatically include these
theorems.  Other tactics used with other \simpset{}s will need these
theorems to be manually added.

The \ml{Induct} tactic makes it convenient to invoke
induction. When it is applied to a goal, the leading universal
quantifier is examined; if its type is that of a known datatype, the
appropriate structural induction tactic is extracted and applied.

The \ml{Cases} tactic makes it convenient to invoke case
analysis. The leading universal quantifier in the goal is examined; if
its type is that of a known datatype, the appropriate structural
case analysis theorem is extracted and applied.

The \ml{Cases\_on} tactic takes a quotation, which is
parsed into a term $M$, and then $M$ is searched for in the goal. If $M$
is a variable, then a variable with the same name is searched for. Once
the term to split over is known, its type and the associated facts are
obtained from the underlying database and used to perform the case
split. If some free variables of $M$ are bound in the goal, an attempt
is made to remove (universal) quantifiers so that the case split has
force. Finally, $M$ need not appear in the goal, although it should at
least contain some free variables already appearing in the goal. Note
that the \ml{Cases\_on} tactic is more general than \ml{Cases}, but
it does require an explicit term to be given.

The \ml{Induct\_on} tactic takes a quotation, which is parsed into a
term $M$, and then $M$ is searched for in the goal. If $M$ is a
variable, then a variable with the same name is searched for. Once the
term to induct on is known, its type and the associated facts are
obtained from the underlying database and used to perform the
induction.  If $M$ is not a variable, a new variable $v$ not already
occurring in the goal is created, and used to build a term $v = M$
which the goal is made conditional on before the induction is
performed. First however, all terms containing free variables from $M$
are moved from the assumptions to the conclusion of the goal, and all
free variables of $M$ are universally quantified. \ml{Induct\_on} is
more general than \ml{Induct}, but it does require an explicit term to
be given.

Two supplementary entrypoints have been provided for more exotic
inductions:
\begin{description}
\item [\ml{completeInduct\_on}] performs complete induction on the
  term denoted by the given quotation. Complete induction allows a
  seemingly \footnote{Complete induction and ordinary mathematical
    induction are each derivable from the other.} stronger induction
  hypothesis than ordinary mathematical induction: to wit, when
  inducting on $n$, one is allowed to assume the property holds for
  \emph{all} $m$ smaller than $n$. Formally: $\forall P.\ (\forall x.\
  (\forall y.\ y < x \supset P\, y) \supset P\,x) \supset \forall x.\
  P\,x$. This allows the inductive hypothesis to be used more than
  once, and also allows instantiating the inductive hypothesis to
  other than the predecessor.

\item [\ml{measureInduct\_on}] takes a quotation, and breaks it
  apart to find a term and a measure function with which to induct.
  For example, if one wanted to induct on the length of a list
  \holtxt{L}, the invocation \ml{measureInduct\_on~`LENGTH L`}
  would be be appropriate.

\end{description}


\subsection{Automated reasoners}
\label{sec:automated-reasoners}

\ml{bossLib} brings together the most powerful reasoners in \HOL{} and
tries to make it easy to compose them in a simple way. We take our basic
reasoners from \ml{mesonLib}, \ml{simpLib}, and \ml{numLib},
but the point of \ml{bossLib} is to provide a layer of abstraction so
the user has to know only a few entrypoints.\footnote{In the mid 1980's
Graham Birtwistle advocated such an approach, calling it `Ten Tactic
HOL'.} (These underlying libraries, and others providing similarly
powerful tools are described in detail in sections below.)

\begin{verbatim}
   PROVE      : thm list -> term quotation -> thm
   PROVE_TAC  : thm list -> tactic

   DECIDE     : term quotation -> thm
   DECIDE_TAC : tactic
\end{verbatim}

The inference rule \texttt{PROVE} (and the corresponding tactic
\texttt{PROVE\_TAC}) takes a list of theorems and a quotation, and
attempts to prove the term using a first order reasoner.  The
\texttt{PROVE} entry-points refer to the \texttt{meson} library, which
is further described in Section~\ref{sec:mesonLib} below. The
inference rule \texttt{DECIDE} (and the corresponding tactic
\texttt{DECIDE\_TAC}) applies a decision procedure that (at least)
handles statements of linear arithmetic.

\begin{verbatim}
   RW_TAC   : simpset -> thm list -> tactic
   SRW_TAC  : ssdata list -> thm list -> tactic
   &&       : simpset * thm list -> simpset  (* infix *)
   std_ss   : simpset
   arith_ss : simpset
   list_ss  : simpset
   srw_ss   : unit -> simpset
\end{verbatim}

\index{RW_TAC@\ml{RW\_TAC}} The rewriting tactic \ml{RW\_TAC} works by
first adding the given theorems into the given \simpset; then it
simplifies the goal as much as possible; then it performs case splits
on any conditional expressions in the goal; then it repeatedly (1)
eliminates all hypotheses of the form $v = M$ or $M = v$ where $v$ is
a variable not occurring in $M$, (2) breaks down any equations between
constructor terms occurring anywhere in the goal. Finally,
\ml{RW\_TAC} lifts \holtxt{let}-expressions within the goal so that
the binding equations appear as
abbreviations\index{abbreviations!tactic-based proof} in the
assumptions.

\index{SRW_TAC@\ml{SRW\_TAC}} The tactic \ml{SRW\_TAC} is similar to
\ml{RW\_TAC}, but works with respect to an underlying \simpset{}
(accessible through the function \ml{srw\_ss}) that is updated as new
context is loaded.  This \simpset{} can be augmented through the
addition of ``\simpset{} fragments'' (\ml{ssdata} values) and
theorems.  In situations where there are many large types stored in
the system, \ml{RW\_TAC}'s performance can suffer because it
repeatedly adds all of the rewrite theorems for the known types into a
\simpset{} before attacking the goal.  On the other hand,
\ml{SRW\_TAC} loads rewrites into the \simpset{} underneath
\ml{srw\_ss()} just once, making for faster operation in this
situation.

\ml{bossLib} provides a number of simplification sets. The
simpset for pure logic, sums, pairs, and the \ml{option} type is
named \ml{std\_ss}. The simpset for arithmetic is named
\ml{arith\_ss}, and the simpset for lists is named \ml{list\_ss}.
The simpsets provided by \bossLib{} strictly increase in strength:
\ml{std\_ss} is contained in \ml{arith\_ss}, and \ml{arith\_ss} is
contained in \ml{list\_ss}.  The infix combinator \ml{\&\&} is used
to build a new \simpset{} from a given \simpset{} and a list of
theorems. \HOL's simplification technology is described further in
Section~\ref{sec:simpLib} below and in the \REFERENCE.

\begin{verbatim}
   STP_TAC  : simpset -> tactic -> tactic
   ZAP_TAC  : simpset -> thm list -> tactic
\end{verbatim}

The compound reasoners of \ml{bossLib} take a basic approach: they
simplify the goal as much as possible with \ml{RW\_TAC} and then a
`finishing' tactic is applied. The primitive entrypoint for this is
\ml{STP\_TAC}. Currently, the most powerful reasoner is
\ml{ZAP\_TAC}, which features a finishing tactic that first
tries a tautology checking tactic; if that fails, \ml{DECIDE\_TAC} is
called; if that fails, \ml{PROVE\_TAC} is called with the second
argument. Although this general approach (simplify as much as possible,
then apply automated reasoners in sequence) is crude, we have found that
it allows one to make good progress in a high percentage of proof
situations.

\begin{verbatim}
   by : term quotation * tactic -> tactic (* infix 8 *)
   SPOSE_NOT_THEN : (thm -> tactic) -> tactic
\end{verbatim}

The function \ml{by} is an infix operator that takes a quotation
and a tactic $tac$. The quotation is parsed into a term $M$. When the
invocation ``\ml{$M$ by $\mathit{tac}$}'' is applied to a goal
$(A,g)$, a new subgoal $(A,M)$ is created and $tac$ is applied to it.
If the goal is proved, the resulting theorem is broken down and added
to the assumptions of the original goal; thus the proof proceeds with
the goal $((M::A), g)$. (Note however, that case-splitting will happen
if the breaking-down of $\ \vdash M$ exposes disjunctions.) Thus
\ml{by} allows a useful style of `assertional' or `Mizar-like'
reasoning to be mixed with ordinary tactic proof.\footnote{Proofs in
  the Mizar system are readable documents, unlike almost all
  tactic-based proofs.}

\ml{SPOSE\_NOT\_THEN} initiates a proof by contradiction by assuming
the negation of the goal and driving the negation inwards through
quantifiers. It provides the resulting theorem as an argument to the
supplied function, which will use the theorem to build and apply a
tactic.

\section{First Order Proof---\texttt{mesonLib}}
\label{sec:mesonLib}

\index{decision procedures!first-order logic, meson@first-order logic, \ml{meson}}
The \ml{meson} library is an implementation of the
model-elimination method for finding proofs of goals in first-order
logic.  There are three main entry-points:

\begin{verbatim}
   MESON_TAC     : thm list -> tactic
   ASM_MESON_TAC : thm list -> tactic
   GEN_MESON_TAC : int -> int -> int -> thm list -> tactic
\end{verbatim}

Each of these tactics attempts to prove the goal.  They will either
succeed in doing so, or fail with a ``depth exceeded'' exception.  If
the branching factor in the search-space is high, the \texttt{meson}
tactics may also take a very long time to reach the maximum depth.

All of the \texttt{meson} tactics take a list of theorems.  These
extra facts are used by the decision procedure to help prove the goal.
\texttt{MESON\_TAC} ignores the goal's assumptions; the other two
entry-points include the assumptions as part of the sequent to be
proved.

The extra parameters to \ml{GEN\_MESON\_TAC} provide extra control of
the behaviour of the iterative deepening that is at the heart of the
search for a proof.  In any given iteration, the algorithm searches
for a proof of depth no more than a parameter $d$.  The default
behaviour for \ml{MESON\_TAC} and \ml{ASM\_MESON\_TAC} is to start $d$
at 0, to increment it by one each time a search fails, and to fail if
$d$ exceeds the value stored in the reference value
\ml{mesonLib.max\_depth}.  By way of contrast,
\ml{GEN\_MESON\_TAC~min~max~step} starts $d$ at \ml{min}, increments
it by \ml{step}, and gives up when $d$ exceeds \ml{max}.



\section{Simplification---\texttt{simpLib}}
\label{sec:simpLib}
\index{simplification|(}

The simplifier is \HOL's most sophisticated rewriting engine.  It is
recommended as a general purpose work-horse during interactive
theorem-proving.  As a rewriting tool, the simplifier's general role
is to apply theorems of the general form
\[
\vdash l = r
\]
to terms, replacing instances of $l$ in the term with $r$. Thus, the
basic simplification routine is a \emph{conversion}, taking a term
$t$, and returning a theorem $\vdash t = t'$, or the exception
\ml{UNCHANGED}.

The basic conversion is
\begin{verbatim}
   simpLib.SIMP_CONV : simpLib.simpset -> thm list -> term -> thm
\end{verbatim}
The first argument, a \simpset, is the standard way of providing a
collection of rewrite rules (and other data, to be explained below) to
the simplifier.  There are \simpset{}s accompanying most of \HOL's
major theories.  For example, the \simpset{} \ml{boolSimps.bool\_ss}
embodies all of the usual rewrite theorems one would want over boolean
formulas:
\setcounter{sessioncount}{0}
\begin{session}
\begin{verbatim}
- SIMP_CONV bool_ss [] ``p /\ T \/ ~(q /\ r)``;
> val it = |- p /\ T \/ ~(q /\ r) = p \/ ~q \/ ~r : thm
\end{verbatim}
\end{session}
In addition, to rewriting with the obvious theorems, \ml{bool\_ss} is
also capable of performing simplifications that are not expressible as
simple theorems:
\begin{session}
\begin{verbatim}
- SIMP_CONV bool_ss [] ``?x. (\y. P (f y)) x /\ (x = z)``;
> val it = |- (?x. (\y. P (f y)) x /\ (x = z)) = P (f z) : thm
\end{verbatim}
\end{session}
In this example, the simplifier performed a $\beta$-reduction in the
first conjunct under the existential quantifier, and then did an
``unwinding'' or ``one-point'' reduction, recognising that the only
possible value for the quantified variable \holtxt{x} was the value
\holtxt{z}.

The second argument to \ml{SIMP\_CONV} is a list of theorems to be
added to the provided \simpset, and used as additional rewrite rules.
In this way, users can temporarily augment standard \simpset{}s with
their own rewrites.  If a particular set of theorems is often used as
such an argument, then it is possible to build a \simpset{} value to
embody these new rewrites.

For example, the rewrite \ml{arithmeticTheory.LEFT\_ADD\_DISTRIB}, which
states that $p(m + n) = pm + pn$ is not part of any of \HOL's standard
\simpset{}s.  This is because it can cause an unappealing increase in
term size (there are two occurrences of $p$ on the right hand
side of the theorem).  Nonetheless, it is clear that this theorem may
be appropriate on occasion:
\begin{session}
\begin{verbatim}
- SIMP_CONV bossLib.arith_ss [LEFT_ADD_DISTRIB] ``p * (n + 1)``;
> val it = |- p * (n + 1) = p + p * n : thm
\end{verbatim}
\end{session}
Note how the \ml{arith\_ss} \simpset{} has not only simplified the
intermediate \ml{(p * 1)} term, but also re-ordered the addition to
put the simpler term on the left.

\subsection{Simplification tactics}
\label{sec:simplification-tactics}
\index{simplification!tactics}

The simplifier is implemented around the conversion \ml{SIMP\_CONV},
which is a function for `converting' terms into theorems.  To apply
the simplifier to goals (alternatively, to perform tactic-based proofs
with the simplifier), \HOL{} provides five tactics, all of which are
available in \ml{bossLib}.

\subsubsection{\ml{SIMP\_TAC : simpset -> thm list -> tactic}}
\index{SIMP_TAC@\ml{SIMP\_TAC}}

\ml{SIMP\_TAC} is the simplest simplification tactic: it attempts to
simplify the current goal (ignoring the assumptions) using the given
\simpset{} and the additional theorems.  It is no more than the
lifting of the underlying \ml{SIMP\_CONV} conversion to the tactic
level through the use of the standard function \ml{CONV\_TAC}.

\subsubsection{\ml{ASM\_SIMP\_TAC : simpset -> thm list -> tactic}}
\index{ASM_SIMP_TAC@\ml{ASM\_SIMP\_TAC}}

Like \ml{SIMP\_TAC}, \ml{ASM\_SIMP\_TAC} simplifies the current goal
(leaving the assumptions untouched), but includes the goal's
assumptions as extra rewrite rules.  Thus:
\begin{session}
\begin{verbatim}
1 subgoal:
> val it =
    P x
    ------------------------------------
      x = 3
     : goalstack

- e (ASM_SIMP_TAC bool_ss []);
OK..
1 subgoal:
> val it =
    P 3
    ------------------------------------
      x = 3
     : goalstack
\end{verbatim}
\end{session}
\noindent
In this example, \ml{ASM\_SIMP\_TAC} used \holtxt{x = 3} as an
additional rewrite rule, and replaced the \holtxt{x} of \holtxt{P x}
with \holtxt{3}.  When an assumption is used by \ml{ASM\_SIMP\_TAC} it
is converted into rewrite rules in the same way as theorems passed in
the list given as the tactic's second argument.  For example, an
assumption \holtxt{\~{}P} will be treated as the rewrite \holtxt{|- P = F}.

\subsubsection{\ml{FULL\_SIMP\_TAC : simpset -> thm list -> tactic}}
\index{FULL_SIMP_TAC@\ml{FULL\_SIMP\_TAC}}

\noindent
The tactic \ml{FULL\_SIMP\_TAC} simplifies not only a goal's
conclusion but its assumptions as well.  It proceeds by simplifying
each assumption in turn, additionally using earlier assumptions in the
simplification of later assumptions.  After being simplified, each
assumption is added back into the goal's assumption list with the
tactic \ml{STRIP\_ASSUME\_TAC}.  This means that assumptions that
become conjunctions will have each conjunct assumed separately.
Assumptions that become disjunctions will cause one new sub-goal to be
created for each disjunct.  If an assumption is simplified to false,
this will solve the goal.

\ml{FULL\_SIMP\_TAC} attacks the assumptions in the order in which
they appear in the list of terms that represent the goal's
assumptions.  Typically then, the first assumption to be simplified
will be the assumption most recently added.  Viewed in the light of
\ml{goalstackLib}'s printing of goals, \ml{FULL\_SIMP\_TAC} works its
way up the list of assumptions, from bottom to top.

The following demonstrates a simple use of \ml{FULL\_SIMP\_TAC}:
\begin{session}
\begin{verbatim}
    x + y < z
    ------------------------------------
      0.  f x < 10
      1.  x = 4
     : goalstack

- e (FULL_SIMP_TAC bool_ss []);
OK..
1 subgoal:
> val it =
    4 + y < z
    ------------------------------------
      0.  f 4 < 10
      1.  x = 4
     : goalstack
\end{verbatim}
\end{session}
In this example, the assumption \holtxt{x = 4} caused the \holtxt{x}
in the assumption \holtxt{f x < 10} to be replaced by \holtxt{4}.  The
\holtxt{x} in the goal was similarly replaced.  If the assumptions had
appeared in the opposite order, only the \holtxt{x} of the goal would
have changed.

The next session more demonstrates more interesting behaviour:
\begin{session}
\begin{verbatim}
> val it =
    f x + 1 < 10
    ------------------------------------
      x <= 4
     : goalstack

- e (FULL_SIMP_TAC bool_ss [arithmeticTheory.LESS_OR_EQ]);
OK..
2 subgoals:
> val it =
    f 4 + 1 < 10
    ------------------------------------
      x = 4

    f x + 1 < 10
    ------------------------------------
      x < 4
     : goalstack
\end{verbatim}
\end{session}
In this example, the goal was rewritten with the theorem stating \[
\vdash x \leq y \equiv x < y \lor x = y
\]
Turning the assumption into a disjunction resulted in two sub-goals.
In the second of these, the assumption \holtxt{x = 4} further
simplified the rest of the goal.

\subsubsection{\ml{RW\_TAC : simpset -> thm list -> tactic}}
\index{RW_TAC@\ml{RW\_TAC}}

Though its type is the same as the simplification tactics already
described, \ml{RW\_TAC} is an ``augmented'' tactic.  It is augmented
in two ways:
\begin{itemize}
\item When simplifying the goal, the provided \simpset{} is augmented
  not only with the theorems explicitly passed in the second argument,
  but also with all of the appropriate rewrite rules from the system's
  \ml{TypeBase}, and also with the goal's assumptions.
%
  \index{TypeBase@\ml{TypeBase}}
\item \ml{RW\_TAC} also does more than just perform simplification.
  It also repeatedly ``strips'' the goal.  For example, it moves the
  antecedents of implications into the assumptions, splits
  conjunctions, and case-splits on conditional expressions.  This
  behaviour can rapidly remove a lot of syntactic complexity from
  goals, revealing the kernel of the problem.  On the other hand, this
  aggressive splitting can also result in a large number of
  sub-goals.  \ml{RW\_TAC}'s augmented behaviours are intertwined with
  phases of simplification in a way that is difficult to describe.
\end{itemize}

\subsubsection{\ml{SRW\_TAC : ssdata list -> thm list -> tactic}}
\index{SRW_TAC@\ml{SRW\_TAC}}

The tactic \ml{SRW\_TAC} has a different type from the other
simplification tactics.  It does not take a \simpset{} as an
argument.  Instead its operation always builds on the built-in
\simpset{} \ml{srw\_ss()} (further described in
Section~\ref{sec:srw_ss}).  The theorems provided as \ml{SRW\_TAC}'s
second argument are treated in the same way as the by the other
simplification tactics.  Finally, the list of \simpset{} fragments
(the \ml{ssdata} values), are merged into the underlying \simpset{},
allowing the user to merge in additional simplification capabilities
if desired.

For example, to include the Presburger decision procedure, one could
write
\begin{verbatim}
   SRW_TAC [ARITH_ss][]
\end{verbatim}
\HOL's standard \simpset{} fragments are described below in
Section~\ref{sec:simpset-fragments}.

\ml{SRW\_TAC} performs the same mixture of simplification and
goal-splitting as does \ml{RW\_TAC}.  The main differences between the
two tactics lie in the fact that the latter can be inefficient when
working with a large \ml{TypeBase}, and in the fact that working with
\ml{SRW\_TAC} saves one from having to explicitly construct
\simpset{}s that include all of the current context's ``appropriate''
rewrites.  The latter ``advantage'' is based on the assumption that
\ml{(srw\_ss())} never includes inappropriate rewrites.  The presence
of unused rewrites is never a concern: the presence of rewrites that
do the wrong thing can be a major irritation.

\subsection{The standard \simpset{}s}
\label{sec:standard-simpsets}

\HOL{} comes with a number of standard \simpset{}s.  All of these are
accessible from within \ml{bossLib}, though some originate in other
structures.

\subsubsection{\ml{pure\_ss} and \ml{bool\_ss}}
%
\index{pure_ss@\ml{pure\_ss}}
%
The \ml{pure\_ss} \simpset{} (defined in structure \ml{pureSimps})
contains no rewrite theorems at all, and plays the role of a blank
slate within the space of possible \simpset{}s.  When constructing a
completely new \simpset, \ml{pure\_ss} is a possible starting point.
The \ml{pure\_ss} \simpset{} has just two components: congruence rules
for specifying how to traverse terms, and a function that turns
theorems into rewrite rules.  Congruence rules are further described
in Section~\ref{sec:advanced-simplifier}; the generation of rewrite
rules from theorems is described in
Section~\ref{sec:simplifier-rewriting}.

\index{bool_ss@\ml{bool\_ss}}
%
The \ml{bool\_ss} \simpset{} (defined in structure \ml{boolSimps}) is
often used when other \simpset{}s might do too much.  It contains
rewrite rules for the boolean connectives, and little more.  It
contains all of the de~Morgan theorems for moving negations in over
the connectives (conjunction, disjunction, implication and conditional
expressions), including the quantifier rules that have $\neg(\forall
x.\,P(x))$ and $\neg(\exists x.\,P (x))$ on their left-hand sides.  It
also contains the rules specifying the behaviour of the connectives
when the constants \holtxt{T} and \holtxt{F} appear as their
arguments.  (One such rule is \holtxt{|- T /\bs{} p = p}.)

As in the example above, \ml{bool\_ss} also performs
$\beta$-reductions and one-point unwindings.  The latter turns terms
of the form \[
\exists x.\;P(x)\land\dots (x = e) \dots\land Q(x)
\]
into
\[
P(e) \land \dots \land Q(e)
\]
Similarly, unwinding will turn $\forall x.\;(x = e)
\Rightarrow P(x)$ into $P(e)$.

Finally, \ml{bool\_ss} also includes congruence rules that allow
the simplifier to make additional assumptions when simplifying
implications and conditional expressions.  This feature is further
explained in Section~\ref{sec:simplifier-rewriting} below, but can be
illustrated by some examples (the first also demonstrates unwinding
under a universal quantifier):
\begin{session}
\begin{verbatim}
- SIMP_CONV bool_ss [] ``!x. (x = 3) /\ P x ==> Q x /\ P 3``;
> val it = |- (!x. (x = 3) /\ P x ==> Q x /\ P 3) = P 3 ==> Q 3 : thm

- SIMP_CONV bool_ss [] ``if ~(x = 3) then P x else Q x``;
> val it = |- (if ~(x = 3) then P x else Q x) =
              (if ~(x = 3) then P x else Q 3) : thm
\end{verbatim}
\end{session}

\subsubsection{\ml{std\_ss}}
%
\index{std_ss@\ml{std\_ss}}
%
The \ml{std\_ss} \simpset{} is defined in \ml{bossLib}, and adds
rewrite rules pertinent to the types of sums, pairs and options to
\ml{bool\_ss}.  It can also perform calculations over natural numbers.
\begin{session}
\begin{verbatim}
- SIMP_CONV std_ss [] ``FST (x,y) + OUTR (INR z)``;
<<HOL message: inventing new type variable names: 'a, 'b>>
> val it = |- FST (x,y) + OUTR (INR z) = x + z : thm

- SIMP_CONV std_ss [] ``case SOME x of NONE -> P || SOME y -> f y``;
> val it = |- (case SOME x of NONE -> P || SOME v -> f v) = f x : thm

- SIMP_CONV std_ss [] ``23 * 6 + 7 ** 2 - 31 DIV 3``;
> val it = |- 23 * 6 + 7 ** 2 - 31 DIV 3 = 177 : thm
\end{verbatim}
\end{session}

\subsubsection{\ml{arith\_ss}}
%
\index{arith_ss@\ml{arith\_ss}}
%
The \ml{arith\_ss} \simpset{} (defined in \ml{bossLib}) extends
\ml{std\_ss} by adding an extensive treatment of natural number
arithmetic.  In addition to what one might call the ``obvious
rewrites'' (such as \holtxt{|- !x.\ 0 <= x}), this \simpset{} can
decide formulas of Presburger arithmetic, and also normalises
arithmetic expressions (collecting coefficients, and re-ordering
summands).  The underlying natural number decision procedure is that
described in Section~\ref{sec:numLib} below.

These two facets of the \ml{arith\_ss} \simpset{} are demonstrated
here:
\begin{session}
\begin{verbatim}
- SIMP_CONV arith_ss [] ``x < 3 /\ P x ==> x < 20 DIV 2``;
> val it = |- x < 3 /\ P x ==> x < 20 DIV 2 = T : thm

- SIMP_CONV arith_ss [] ``2 * x + y - x + y``;
> val it = |- 2 * x + y - x + y = x + 2 * y : thm
\end{verbatim}
\end{session}
Note that subtraction over the natural numbers works in ways that can
seem unintuitive.  In particular, coefficient normalisation may not
occur when first expected:
\begin{session}
\begin{verbatim}
- SIMP_CONV arith_ss [] ``2 * x + y - z + y``;
! Uncaught exception:
! UNCHANGED
\end{verbatim}
\end{session}
Over the natural numbers, the expression $2 x + y - z + y$ is not
equal to $2 x + 2 y - z$.  In particular, these expressions are not
equal when $2x + y < z$.

\subsubsection{\ml{list\_ss}}
%
\index{list_ss@\ml{list\_ss}}
%
The last pure \simpset{} value in \ml{bossLib}, \ml{list\_ss} adds
rewrite theorems about the type of lists to \ml{arith\_ss}.  These
rewrites include the obvious facts about the list type's constructors
\holtxt{NIL} and \holtxt{CONS}, such as the fact that \holtxt{CONS} is
injective:
\begin{verbatim}
   (h1 :: t1 = h2 :: t2) = (h1 = h2) /\ (t1 = t2)
\end{verbatim}
Conveniently, \ml{list\_ss} also includes rewrites for the functions
defined by primitive recursion over lists.  Examples include
\holtxt{MAP}, \holtxt{FILTER} and \holtxt{LENGTH}.  Thus:
\begin{session}
\begin{verbatim}
- SIMP_CONV list_ss [] ``MAP (\x. x + 1) [1;2;3;4]``;
> val it = |- MAP (\x. x + 1) [1; 2; 3; 4] = [2; 3; 4; 5] : thm

- SIMP_CONV list_ss [] ``FILTER (\x. x < 4) [1;2;y + 4]``;
> val it = |- FILTER (\x. x < 4) [1; 2; y + 4] = [1; 2] : thm

- SIMP_CONV list_ss [] ``LENGTH (FILTER ODD [1;2;3;4;5])``;
> val it = |- LENGTH (FILTER ODD [1; 2; 3; 4; 5]) = 3 : thm
\end{verbatim}
\end{session}
These examples demonstrate how the simplifier can be used as a general
purpose symbolic evaluator for terms that look a great deal like those
that appear in a functional programming language.  Note that
this functionality is also provided by \ml{computeLib} (see
Section~\ref{sec:computeLib} below); \ml{computeLib} is more
efficient, but less general than the simplifier.  For example:
\begin{session}
\begin{verbatim}
- EVAL ``FILTER (\x. x < 4) [1;2;y + 4]``;
> val it =
    |- FILTER (\x. x < 4) [1; 2; y + 4] =
       1::2::(if y + 4 < 4 then [y + 4] else []) : thm
\end{verbatim}
\end{session}

\subsubsection{The ``stateful'' \simpset---\ml{srw\_ss()}}
\label{sec:srw_ss}
\index{srw_ss@\ml{srw\_ss}}

The last \simpset{} exported by \ml{bossLib} is hidden behind a
function.  The \ml{srw\_ss} value has type \ml{unit -> simpset}, so
that one must type \ml{srw\_ss()} in order to get a \simpset{} value.
This use of a function type allows the underlying \simpset{} to be
stored in an \ML{} reference, and allows it to be updated
dynamically.  In this way, referential transparency is deliberately
broken.  All of the other \simpset{}s will always behave identically:
\ml{SIMP\_CONV~bool\_ss} is the same simplification routine wherever
and whenever it is called.

In contrast, \ml{srw\_ss} is designed to be updated.  When a theory is
loaded, when a new type is defined, the value behind \ml{srw\_ss()}
changes, and the behaviour of \ml{SIMP\_CONV} applied to
\ml{(srw\_ss())} changes with it.  The design philosophy behind
\ml{srw\_ss} is that it should always be a reasonable first choice in
all situations where the simplifier is used.

This versatility is illustrated in the following example:
\begin{session}
\begin{verbatim}
- Hol_datatype `tree = Leaf | Node of num => tree => tree`;
<<HOL message: Defined type: "tree">>
> val it = () : unit

- SIMP_CONV (srw_ss()) [] ``Node x Leaf Leaf = Node 3 t1 t2``;
<<HOL message: Initialising SRW simpset ... done>>
> val it =
    |- (Node x Leaf Leaf = Node 3 t1 t2) =
       (x = 3) /\ (Leaf = t1) /\ (Leaf = t2) : thm

- load "pred_setTheory";
> val it = () : unit

- SIMP_CONV (srw_ss()) [] ``x IN { y | y < 6}``;
> val it = |- x IN {y | y < 6} = x < 6 : thm
\end{verbatim}
\end{session}
%
Users can augment the stateful \simpset{} themselves with the function
%
\begin{boxed}
\index{export_rewrites@\ml{export\_rewrites}}
\begin{verbatim}
   BasicProvers.export_rewrites : string list -> unit
\end{verbatim}
\end{boxed}
The strings passed to \ml{export\_rewrites} are the names of theorems
in the current segment (those that will be exported when
\ml{export\_theory} is called).  Not only are these theorems added to
the underlying \simpset{} in the current session, but they will be
added in future sessions when the current theory is reloaded.
\begin{session}
\begin{verbatim}
- val tsize_def = Define`
  (tsize Leaf = 0) /\
  (tsize (Node n t1 t2) = n + tsize t1 + tsize t2)
`;
Definition has been stored under "tsize_def".
> val tsize_def =
    |- (tsize Leaf = 0) /\
       !n t1 t2. tsize (Node n t1 t2) = n + tsize t1 + tsize t2 : thm

- val _ = BasicProvers.export_rewrites ["tsize_def"];

- SIMP_CONV (srw_ss()) [] ``tsize (Node 4 (Node 6 Leaf Leaf) Leaf)``;
> val it = |- tsize (Node 4 (Node 6 Leaf Leaf) Leaf) = 10 : thm
\end{verbatim}
\end{session}

As a general rule, \ml{(srw\_ss())} includes all of its context's
``obvious rewrites'', as well as code to do standard calculations
(such as the arithmetic performed in the above example).  It does not
include decision procedures that may exhibit occasional poor
performance, so the \simpset{} fragments containing these procedures
should be added manually to those simplification invocations that need them.

\subsection{\Simpset{} fragments}
\label{sec:simpset-fragments}

\subsection{Rewriting with the simplifier}
\label{sec:simplifier-rewriting}

\subsection{Advanced features}
\label{sec:advanced-simplifier}

\index{simplification|)}

\section{Efficient Applicative Order Reduction---\texttt{computeLib}}
\label{sec:computeLib}

\section{Arithmetic Libraries---\texttt{numLib}, \texttt{intLib} and \texttt{realLib}}
\label{sec:numLib}
\index{decision procedures!Presburger arithmetic over natural numbers}

\section{The \texttt{HolSat} library}\label{sec:HolSatLib}
\section{The \texttt{HolBdd} library}\label{sec:HolBddLib}
\section{The \texttt{HolCheck} library}\label{sec:HolCheckLib}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "description"
%%% End:
