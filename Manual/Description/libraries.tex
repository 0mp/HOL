\chapter{Libraries}\label{HOLlibraries}

% LaTeX macros in HOL manuals
%
% \holtxt{..}     for typewriter text that is HOL types or terms.  To
%                 produce backslashes, for /\, \/ and \x. x + 1, use \bs
% \ml{..}         for typewriter text that is ML input, including the
%                 names of HOL API functions, such as mk_const
% \theoryimp{..}  for names of HOL theories.

% text inside \begin{verbatim} should be indented three spaces, unless
% the verbatim is in turn inside a \begin{session}, in which case it
% should be flush with the left margin.


\newcommand{\simpset}{simpset}
\newcommand{\Simpset}{Simpset}

{
 \newcommand{\term}      {\mbox{\it term}}
 \newcommand{\vstr}      {\mbox{\it vstr}}

A \emph{library} is an abstraction intended to provide a higher level
of organization for \HOL{} applications. In general, a library can
contain a collection of theories, proof procedures, and supporting
material, such as documentation. Some libraries simply provide proof
procedures, such as \ml{simpLib}, while others provide theories and
proof procedures, such as \ml{intLib}. Libraries can include other
libraries.

In the \HOL{} system, libraries are represented by \ML{} structures
named following the convention that library \emph{x} will be found in
the \ML{} structure \ml{xLib}. Loading this structure should load all
the relevant sub-components of the library and set whatever system
parameters are suitable for use of the library.

When the \HOL{} system is invoked in its normal configuration, several
useful libraries are automatically loaded. The most basic \HOL{}
library is \ml{boolLib}, which supports the definitions of
the \HOL{} logic, found in the theory \theoryimp{bool}, and provides a
useful suite of definition and reasoning tools.

Another pervasively used library is found in the structure \ml{Parse}
(the reader can see that we are not strictly faithful to our
convention about library naming). The parser library provides support
for parsing and `pretty-printing' of \HOL{} types, terms, and
theorems.

The \ml{boss} library provides a basic collection of standard
theories and high-level proof procedures, and serves as a standard
platform on which to work. It is preloaded and opened when the \HOL{}
system starts up. It includes \ml{boolLib} and
\ml{Parse}. Theories provided include \theoryimp{pair},
\theoryimp{sum}, \theoryimp{option}; the arithmetic theories
\theoryimp{num}, \theoryimp{prim\_rec}, \theoryimp{arithmetic},
and \theoryimp{numeral}; and \theoryimp{list}. Other libraries
included in \ml{bossLib} are \ml{goalstackLib}, which provides
a proof manager for tactic proofs; \ml{simpLib}, which provides
a variety of simplifiers; \ml{numLib}, which provides a decision
procedure for arithmetic; \ml{Datatype}, which provides
high-level support for defining algebraic datatypes; and \ml{tflLib},
which provides support for defining recursive functions.


\section{Parsing and Prettyprinting}
\label{sec:parsing-printing}

Every type and term in \HOL{} is ultimately built by application of
the primitive (abstract) constructors for types and terms. However, in
order to accomodate a wide variety of mathematical expression, \HOL{}
provides flexible infrastracture for parsing and prettyprinting types
and terms through the \ml{Parse} structure.

The term parser supports type inference, overloading, binders, and
various fixity declaration (infix, prefix, postfix, and
combinations). There are also flags for controlling the behaviour
of the parser. Further, the structure of the parser is exposed so that
new parsers can be quickly constructed to support user applications.

The parser is parameterized by grammars for types and terms. The
behaviour of the parser and prettyprinter is therefore usually altered
by grammar manipulations. These can be of two kinds: \emph{temporary}
or \emph{permanent}.  Temporary changes do not persist after the
current session, while permanent changes will be in force in all
descendant theories.  Functions making temporary alterations are
signified by a leading \ml{temp\_} in their names.

\subsection{The type parser}

The language of types is a simple one.  An abstract grammar for the
language is presented in Figure~\ref{fig:abstract-type-grammar}.  The
actual grammar (with concrete values for the infix symbols and type
operators) can be inspected using the function \ml{type\_grammar}.
\begin{figure}[tbhp]
\newcommand{\nt}[1]{\mathit{#1}}
\newcommand{\tok}[1]{\texttt{\bfseries #1}}
\renewcommand{\bar}{\;\;|\;\;}
\[
\begin{array}{lcl}
\tau &::=& \tau \odot \tau \bar \nt{vtype} \bar \nt{tyop} \bar
           \tok{(} \;\nt{tylist}\;\tok{)} \;\nt{tyop}\bar \tau \;\nt{tyop}
           \bar \tok{(}\;\tau\;\tok{)}\\
\odot &::=& \tok{->} \bar \tok{\#} \bar \tok{+} \bar \cdots\\
\nt{vtype} &::=& \tok{'a} \bar \tok{'b} \bar \tok{'c} \bar \cdots\\
\nt{tylist} &::=& \tau \bar \tau \;\tok{,}\;\nt{tylist}\\
\nt{tyop} &::=& \tok{bool} \bar \tok{list} \bar \tok{num} \bar
           \tok{fun} \bar \cdots
\end{array}
\]
\caption{An abstract grammar for \HOL{} types ($\tau$).  Infixes ($\odot$)
  always bind more weakly than type operators ($\nt{tyop}$), so that
  $\tau_1 \,\odot\, \tau_2 \,\nt{tyop}$ is always parsed as $\tau_1\, \odot\,
  (\tau_2 \,\nt{tyop})$.  Different infixes can have different
  priorities, and infixes at different priority levels can associate
  differently (to the left, to the right, or not at all).  Users can
  extend the categories $\odot$ and $\nt{tyop}$ by making new type
  definitions, and by directly manipulating the grammar.}
\label{fig:abstract-type-grammar}
\end{figure}

\paragraph{Type infixes}
Infixes may be introduced with the function \ml{add\_infix\_type}.
This sets up a mapping from an infix symbol (such as \texttt{->}) to
the name of an existing type operator (such as \texttt{fun}).  The
binary symbol needs to be given a precedence level and an
associativity. See \REFERENCE{} for more details.

\paragraph{Type abbreviations}
\index{type abbreviations}

Users can abbreviate common type patterns with \emph{abbreviations}.
This is done with the \ML{} function \ml{type\_abbrev}:
\begin{hol}
\begin{verbatim}
   type_abbrev : string * hol_type -> unit
\end{verbatim}
\end{hol}
An abbreviation is a new type operator, of any number of arguments,
that expands into an existing type.  For example, one might develop a
light-weight theory of numbers extended with an infinity, where the
represeting type was \holtxt{num option} (\holtxt{NONE} would
represent the infinity value).  One might set up an abbreviation
\holtxt{infnum} that expanded to this underlying type.

Polymorphic patterns are supported as well.  For example, as described
in Section~\ref{sec:theory-of-sets}, the abbreviation \holtxt{set}, of
one argument, is such that \holtxt{:'a set} expands into the type
\holtxt{:'a -> bool}, for any type \holtxt{:'a}.

When types come to be printed, the expansion of abbreviations done by
the parser is reversed.  For more information see the documentation of
\ml{type\_abbrev} in the \REFERENCE.

\subsection{The term parser}

The term parser provides a grammar-based infrastructure for supporting
concrete syntax for formalizations. Usually, the \HOL{} grammar gets
extended when a new definition or constant specification is made. (The
introduction of new constants is discussed in
Sections~\ref{sec:constant-definitions} and \ref{conspec}.) However,
any identifier can have a parsing status attached at any time.
In the following, we explore some of the capabilities of the
\HOL{} term parser.


\subsubsection{Type constraints}

A term can be constrained to be of a certain type.  For example,
\holtxt{X:bool} constrains the variable \holtxt{X} to have type
\holtxt{bool}. An attempt to constrain a
term inappropriately will raise an exception: for example,
\begin{hol}
\begin{verbatim}
   if T then (X:ind) else (Y:bool)
\end{verbatim}
\end{hol}
will fail because both branches of a conditional must be of the same
type.  Type constraints can be seen as a suffix that binds more
tightly than everything except function application.  Thus $\term\
\ldots\ \term \ : \type$ is equal to $(\term\ \ldots\ \term)\ :
\type$, but $x < y:\holtxt{num}$ is a legitimate constraint on just
the variable $y$.

The inclusion of \holtxt{:} in the symbolic identifiers means that some
constraints may need to be separated by white space. For example,
\begin{hol}
\begin{verbatim}
   $=:bool->bool->bool
\end{verbatim}
\end{hol}
will be broken up by the \HOL{} lexer as
\begin{hol}
\begin{verbatim}
   $=: bool -> bool -> bool
\end{verbatim}
\end{hol}
and parsed as an application of the symbolic identifier \holtxt{\$=:} to
the argument list of terms [\holtxt{bool}, \holtxt{->}, \holtxt{bool},
\holtxt{->}, \holtxt{bool}]. A well-placed space will avoid this problem:
\begin{hol}
\begin{verbatim}
   $= :bool->bool->bool
\end{verbatim}
\end{hol}
is parsed as the symbolic identifier ``='' constrained by a type.
Instead of the \holtxt{\$}, one can also use parentheses to remove
special parsing behaviour from lexemes:
\begin{hol}
\begin{verbatim}
   (=):bool->bool->bool
\end{verbatim}
\end{hol}

\subsubsection{Type inference}

Consider the term \holtxt{x = T}: it (and all of its subterms)
has a type in the \HOL{} logic. Now, \holtxt{T} has type \holtxt{bool}. This
means that the constant \holtxt{=} has type \holtxt{xty -> bool -> bool},
for some type \holtxt{xty}. Since the type scheme for \holtxt{=} is
\holtxt{'a -> 'a -> bool}, we know that \holtxt{xty} must in fact be
\holtxt{bool} in order for the type instance to be well-formed. Knowing
this, we can deduce that the type of \holtxt{x} must be \holtxt{bool}.

Ignoring the jargon (``scheme'' and ``instance'') in the previous
paragraph, we have conducted a type assignment to the term structure,
ending up with a well-typed term. It would be very tedious for users
to conduct such argumentation by hand for each term entered to \HOL{}.
Thus, \HOL{} uses an adaptation of Milner's type inference algorithm
for \ML{} when constructing terms via parsing. At the end of type
inference, unconstrained type variables get assigned names by the system.
Usually, this assignment does the right thing. However, at times, the
most general type is not what is desired and the user must add type
constraints to the relevant subterms. For tricky situations, the
global variable \ml{show\_types} can be assigned. When this flag is
set, the prettyprinters for terms and theorems will show how types
have been assigned to subterms. If you do not want the system to
assign type variables for you, the global variable
\ml{guessing\_tyvars} can be set to \ml{false}, in which case the
existence of unassigned type variables at the end of type inference
will raise an exception.


\subsubsection{Overloading}

A limited amount of overloading resolution is performed by the term
parser. For example, the `tilde' symbol (\holtxt{\~{}})
denotes boolean negation in the initial theory of \HOL, and it also denotes
the additive inverse in the \ml{integer} and
\ml{real} theories. If we load the \ml{integer}
theory and enter an ambiguous term featuring \holtxt{\~{}}, the
system will inform us that overloading resolution is being performed.

\setcounter{sessioncount}{1}
\begin{session}
\begin{hol}
\begin{verbatim}
- load "integerTheory";
> val it = () : unit

- Term `~~x`;
<<HOL message: more than one resolution of overloading was possible.>>
> val it = `~~x` : term

- type_of it;
> val it = `:bool` : hol_type
\end{verbatim}
\end{hol}
\end{session}

A priority mechanism is used to resolve multiple possible choices. In
the example, \holtxt{\~{}} could be consistently chosen to have type
\holtxt{:bool -> bool} or \holtxt{:int -> int}, and the
mechanism has chosen the former. For finer control, explicit type
constraints may be used. In the following session, the
\holtxt{\~{}\~{}x} in the first quotation has type \holtxt{:bool},
while in the second, a type constraint ensures that \holtxt{\~{}\~{}x} has
type \holtxt{:int}.

\begin{session}
\begin{hol}
\begin{verbatim}
- show_types := true;
> val it = () : unit

- Term `~(x = ~~x)`;
<<HOL message: more than one resolution of overloading was possible.>>
> val it = `~((x :bool) = ~~x)` : term

- Term `~(x:int = ~~x)`;
> val it = `~((x :int) = ~~x)` : term
\end{verbatim}
\end{hol}
\end{session}

Note that the symbol \holtxt{\~{}} stands for two different constants in
the second quotation; its first occurrence is boolean negation, while
the other two occurrences are the additive inverse operation for integers.
For more information on how to set up and use overloading, consult
\REFERENCE.

\subsubsection{Fixities}

In order to provide some notational flexibility, constants come in
various flavours or {\it fixities}: besides being an ordinary constant
(with a fixity of {\sf Prefix}), constants can also be {\it binders},
{\it true prefixes}\footnote{The use of the term ``true prefix'' is
forced upon us by the history of the system, which reserved the
classification ``prefix'' for terms without any special syntactic
features.}, {\it suffixes}, {\it infixes}, or {\it closefixes}.  More
generally, terms can also be represented using reasonably arbitrary
{\it mixfix} specifications.  The degree to which terms bind their
associated arguments is known as precedence.  The higher this number,
the tighter the binding.  For example, when introduced, \verb-+- has a
precedence of 500, while the tighter binding multiplication (\verb+*+)
has a precedence of 600.

\paragraph{Binders}

A binder is a construct that binds a variable; for example, the
universal quantifier. In \HOL, this is represented using a trick that
goes back to Alonzo Church: a binder is a constant that takes a lambda
abstraction as its argument. The lambda binding is used to implement
the binding of the construct. This is an elegant and uniform solution.
Thus the concrete syntax \verb+!v. M+ is represented by the
application of the constant \verb+!+ to the abstraction \verb+(\v. M)+.

The most common binders are \verb+!+, \verb+?+, \verb+?!+, and
\verb+@+. Sometimes one wants to iterate applications of the same
binder, \eg,
\begin{alltt}
   !x. !y. ?p. ?q. ?r. \term.
\end{alltt}
This can instead be rendered
\begin{alltt}
   !x y. ?p q r. \term.
\end{alltt}

\paragraph{Infixes}

Infix constants can associate in one of three different ways: right,
left or not at all.  (If \holtxt{+} were non-associative, then
\holtxt{3 + 4 + 5} would fail to parse; one would have to write
\holtxt{(3 + 4) + 5} or \holtxt{3 + (4 + 5)} depending on the desired
meaning.)  The precedence ordering for the initial set of infixes is
\holtxt{/\bs}, \holtxt{\bs/}, \holtxt{==>}, \holtxt{=},
\begin{Large}\holtxt{,}\end{Large} (comma\footnote{When
  \theoryimp{pairTheory} has been loaded.}). Moreover, all of these
constants are right associative. Thus
\begin{hol}
\begin{verbatim}
   X /\ Y ==> C \/ D, P = E, Q
\end{verbatim}
\end{hol}
%
is equal to
%
\begin{hol}
\begin{verbatim}
   ((X /\ Y) ==> (C \/ D)), ((P = E), Q).
\end{verbatim}
\end{hol}
%
\noindent An expression
\[
\term \; \holtxt{<infix>}\; \term
\]
is internally represented as
\[
((\holtxt{<infix>}\; \term)\; \term)
\]

\paragraph{True prefixes}

Where infixes appear between their arguments, true prefixes appear
before theirs.  This might initially appear to be the same thing as
happens with normal function application (is $f$ in $f(x)$ not acting
as a prefix?), but in fact, it is useful to allow for prefixes to have
binding power less than that associated with function application.  An
example of this is \verb+~+, logical negation.  This is a prefix with
lower precedence than function application.  Normally
\[
   f\;x\; y\qquad \mbox{is parsed as}\qquad (f\; x)\; y
\] but \[
  \holtxt{\~{}}\; x\; y\qquad\mbox{is parsed as}\qquad
  \holtxt{\~{}}\; (x\; y)
\] because the precedence of \verb+~+ is lower than that of function
application.  The unary negation symbol would also typically be
defined as a true prefix, if only to allow one to write \[ {\it
  negop}\,{\it negop}\,3
\] (whatever {\it negop} happened to be) without needing extra parentheses.

\paragraph{Suffixes}

Suffixes appear after their arguments.  There are no suffixes
introduced into the standard theories available in \HOL{}, but users
are always able to introduce their own if they choose.  Suffixes are
associated with a precedence just as infixes and true prefixes are.
If \holtxt{p} is a true prefix, \holtxt{i} an infix, and \holtxt{s} a
suffix, then there are six possible orderings for the three different
operators based on their precedences, giving five parses for
$\holtxt{p}\; t_1\; \holtxt{i}\; t_2\; \holtxt{s}$ depending on the
relative precedences:
\[
\begin{array}{cl}
\mbox{\begin{tabular}{c}Precedences\\(lowest to highest)\end{tabular}} &
\multicolumn{1}{c}{\mbox{Parses}}\\
\hline
p,\;i,\;s & \holtxt{p}\;(t_1\;\holtxt{i}\;(t_2\;\holtxt{s}))\\
p,\;s,\;i & \holtxt{p}\;((t_1\;\holtxt{i}\;t_2)\;\holtxt{s})\\
i,\;p,\;s & (\holtxt{p}\;t_1)\;\holtxt{i}\;(t_2\;\holtxt{s})\\
i,\;s,\;p & (\holtxt{p}\;t_1)\;\holtxt{i}\;(t_2\;\holtxt{s})\\
s,\;p,\;i & (\holtxt{p}\;(t_1\;\holtxt{i}\;t_2))\;\holtxt{s}\\
s,\;i,\;p & ((\holtxt{p}\;t_1)\;\holtxt{i}\;t_2)\;\holtxt{s}\\
\end{array}
\]

\paragraph{Closefixes}

Closefix terms are operators that completely enclose their arguments.
An example one might use in the development of a theory of
denotational semantics is semantic brackets.  Thus, the \HOL{} parsing
facilities can be configured to allow one to write \holtxt{denotation x}
as \holtxt{[| x |]}.  Closefixes are not associated with precedences
because they can not compete for arguments with other operators.


\subsubsection{Parser tricks and magic}

Here we describe how to achieve some useful effects with the
parser in \HOL{}.

\begin{description}

\item[Aliasing] If one wants a special syntax to be an ``alias'' for a
  normal \HOL{} form, this is easy to achieve; both examples so far
  have effectively done this.  However, if one just wants to have a
  normal one-for-one substitution of one string for another, one can't
  use the grammar/syntax phase of parsing to do this.  Instead, one
  can use the overloading mechanism.  For example, let us alias
  \texttt{MEM} for \texttt{IS\_EL}.  We need to use the function
  \texttt{overload\_on} to overload the original constant for the new
  name:
\begin{hol}
\begin{verbatim}
   val _ = overload_on ("MEM", Term`IS_EL`);
\end{verbatim}
\end{hol}

\item[Making addition right associative] If one has a number of old
  scripts that assume addition is right associative because this is
  how \HOL{} used to be, it might be too much pain to convert.  The trick
  is to remove all of the rules at the given level of the grammar, and
  put them back as right associative infixes.  The easiest way to tell
  what rules are in the grammar is by inspection (use
  \ml{term\_grammar()}).  With just \ml{arithmeticTheory}
  loaded, the only infixes at level 500 are \holtxt{+} and
  \holtxt{-}.  So, we remove the rules for them:
\begin{hol}
\begin{verbatim}
   val _ = app temp_remove_rules_for_term ["+", "-"];
\end{verbatim}
\end{hol}
  \noindent And then we put them back with the appropriate
  associativity:
\begin{hol}
\begin{verbatim}
   val _ = app (fn s => temp_add_infix(s, 500, RIGHT)) ["+", "-"];
\end{verbatim}
\end{hol}
\noindent Note that we use the \ml{temp\_} versions of these two
functions so that other theories depending on this one won't be
affected.  Further note that we can't have two infixes at the same
level of precedence with different associativities, so we have to
remove both operators, not just addition.

\item[Mix-fix syntax for {\it if-then-else}:]
\index{conditionals, in HOL logic@conditionals, in \HOL{} logic!printing of}
%
The first step in bringing this about is to look at the general shape
of expressions of this form.  In this case, it will be:
%
\[
  \holtxt{if}\;\; \dots \;\;\holtxt{then}\;\;\dots\;\;
  \holtxt{else}\;\;\dots
  \]
%
 Because there needs to be a ``dangling'' term to the right, the
  appropriate fixity is \ml{TruePrefix}.  Knowing that the underlying
  term constant is called \holtxt{COND}, the simplest way to achieve
  the desired syntax is:
\begin{hol}
\begin{verbatim}
  val _ = add_rule
        {term_name = "COND", fixity = TruePrefix 70,
         pp_elements = [TOK "if", BreakSpace(1,0), TM, BreakSpace(1,0),
                        TOK "then", BreakSpace(1,0), TM, BreakSpace(1,0),
                        TOK "else", BreakSpace(1,0)],
         paren_style = Always,
         block_style = (AroundEachPhrase, (PP.CONSISTENT, 0))};
\end{verbatim}
\end{hol}
\noindent The actual rule is slightly more complicated, and
may be found in the sources for the theory \theoryimp{bool}.

\item[Mix-fix syntax for term substitution:]

Here the desire is to be able to write something like:
\[
  \mbox{\texttt{[}}\,t_1\,\mbox{\texttt{/}}\,t_2\,\mbox{\texttt{]}}\,t_3
\]
denoting the substitution of $t_1$ for $t_2$ in $t_3$, perhaps
translating to \holtxt{SUB $t_1$ $t_2$ $t_3$}.  This looks
like it should be another \ml{TruePrefix}, but the choice of the
square brackets (\holtxt{[} and \holtxt{]}) as delimiters would
conflict with the concrete syntax for list literals if this was done.
Given that list literals are effectively of the \ml{CloseFix}
class, the new syntax must be of the same class.  This is easy enough
to do: we set up syntax
\[
\holtxt{[}\,t_1\,\holtxt{/}\,t_2\,\holtxt{]}
\]
to map to \holtxt{SUB $t_1$ $t_2$}, a value of a functional
type, that when applied to a third argument will look
right.\footnote{Note that doing the same thing for the
  \textit{if-then-else} example in the previous example would be
  inappropriate, as it would allow one to write
\[ \holtxt{if}\;P\;\holtxt{then}\;Q\;\holtxt{else} \]
without the trailing argument.}
The rule for this is thus:
\begin{hol}
\begin{verbatim}
  val _ = add_rule
           {term_name = "SUB", fixity = Closefix,
            pp_elements = [TOK "[", TM, TOK "/", TM, TOK "]"],
            paren_style = OnlyIfNecessary,
            block_style = (AroundEachPhrase, (PP.INCONSISTENT, 2))};
\end{verbatim}
\end{hol}

\end{description}

\subsubsection{Hiding constants}
\label{hidden}

\index{parsing, of HOL logic@parsing, of \HOL{} logic!hiding constant status in|(}
\index{HOL system@\HOL{} system!hiding constants in|(}
\index{constants, in HOL logic@constants, in \HOL{} logic!hiding status of}
%
The following function can be used to hide the constant status of a
name from the quotation parser.

\begin{boxed}
\index{hide@\ml{hide}|pin}
\begin{verbatim}
  val hide   : string -> ({Name : string, Thy : string} list *
                          {Name : string, Thy : string} list)
\end{verbatim}
\end{boxed}

\noindent Evaluating \ml{hide "$x$"}
makes the quotation parser treat $x$ as a variable (lexical
rules permitting), even if $x$ is the name of a constant in the current theory
(constants and variables can have the same name).
This is useful if one wants to use variables
%
\index{variables, in HOL logic@variables, in \HOL{} logic!with constant names}
%
with the same names as previously declared (or built-in) constants
(\eg\ \ml{o}, \ml{I}, \ml{S} \etc).  The name $x$ is still a constant
for the constructors, theories, etc; \ml{hide} affects only parsing.
See the \REFERENCE{} entry for \ml{hide} for more details, including
an explanation of the return type.

The function

\begin{boxed}
\index{reveal@\ml{reveal}|pin}
\begin{verbatim}
   reveal : string -> unit
\end{verbatim}
\end{boxed}

\noindent undoes hiding.

The function

\begin{boxed}
\index{hidden@\ml{hidden}|pin}
\begin{verbatim}
   hidden : string -> bool
\end{verbatim}
\end{boxed}

\noindent tests whether a string is the name of a hidden constant.
\index{HOL system@\HOL{} system!adjustment of user interface of}
\index{HOL system@\HOL{} system!hiding constants in|)}
\index{parsing, of HOL logic@parsing, of \HOL{} logic!hiding constant status in|)}

\subsubsection{Adjusting the pretty-print depth}
\index{printing, in HOL logic@printing, in \HOL{} logic!structural depth adjustment in}

The following \ML{} reference can be used to adjust the maximum depth
of printing

\begin{boxed}
\index{max_print_depth@\ml{max\_print\_depth}|pin}
\begin{verbatim}
   max_print_depth : int ref
\end{verbatim}
\end{boxed}

\index{default print depth, for HOL logic@default print depth, for \HOL{} logic|(}

\noindent The default print depth is $-1$, which is interpreted as
meaning no maximum.  Subterms nested more deeply than the maximum
print depth are printed as \holtxt{...}. For example:

\setcounter{sessioncount}{0}
\begin{session}
\begin{hol}
\begin{verbatim}
- ADD_CLAUSES;
> val it =
    |- (0 + m = m) /\ (m + 0 = m) /\ (SUC m + n = SUC (m + n)) /\
       (m + SUC n = SUC (m + n)) : thm

- max_print_depth := 3;
> val it = () : unit
- ADD_CLAUSES;
> val it = |- (... + ... = m) /\ (... = ...) /\ ... /\ ... : thm
\end{verbatim}
\end{hol}
\end{session}
\index{default print depth, for HOL logic@default print depth, for \HOL{} logic|)}

\subsection{Quotations and Antiquotation}
\label{sec:quotation-antiquotation}

\index{quotation, in HOL logic@quotation, in \HOL{} logic!parser for}
\index{parsing, of HOL logic@parsing, of \HOL{} logic!of quotation syntax|(}
Logic-related syntax in the HOL system is typically passed to the
parser in special forms known as \emph{quotations}.  A basic quotation
is delimited by single back-ticks (i.e., \ml{`}, ASCII character~96).  When
quotation values are printed out by the ML interactive loop, they look
rather ugly because of the special filtering that is done to these
values before the ML interpreter even sees them:
\setcounter{sessioncount}{0}
\begin{session}
\begin{hol}
\begin{verbatim}
- val q = `f x = 3`;
> val 'a q = [QUOTE " (*#loc 1 11*)f x = 3"] : 'a frag list
\end{verbatim}
\end{hol}
\end{session}
Quotations (Moscow ML prints the type as \ml{'a frag list}) are the
raw input form expected by the various HOL parsers.  They are also
polymorphic (to be explained below).  Thus the function
\ml{Parse.Term} function takes a (term) quotation and returns a term,
and is thus of type \[ \ml{term quotation -> term}
\]

The term and type parsers can also be called implicitly by using
double back-ticks as delimiters.  For the type parser, the first
non-space character after the leading delimiter must also be a colon.
Thus:
\begin{session}
\begin{hol}
\begin{verbatim}
- val t = ``p /\ q``;
> val t = ``p /\ q`` : term

- val ty = ``:'a -> bool``;
> val ty = ``:'a -> bool`` : hol_type
\end{verbatim}
\end{hol}
\end{session}

The expression bound to ML variable \ml{t} above is actually expanded
to an application of the function \ml{Parse.Term} to the quotation
argument \ml{`p /\bs{} q`}.  Similarly, the second expression expands
into an application of \ml{Parse.Type} to the quotation \ml{`:'a -> bool`}.

The significant advantage of quotations over normal \ML{} strings is
that they can include new-line and backslash characters without
requiring special quoting.  Newlines occur whenever terms get beyond
the trivial in size, while backslashes occur in not just the
representation of $\lambda$, but also the syntax for conjunction and
disjunction.

If a quotation is to include a back-quote character, then this should
be done by using the quotation syntax's own escape character, the
caret (\ml{\^}, ASCII character~94).  To get a bare caret, things are
slightly more complicated.  If a sequence of carets is followed by
white-space (including a newline), then that sequence of carets is
passed to the HOL parser unchanged.  Otherwise, one caret can be
obtained by writing two in a row. (This last rule is analogous to the
way in \ML{} string syntax treats the back-slash.) Thus:
\begin{session}
\begin{hol}
\begin{verbatim}
- ``f ^` x ``;
<<HOL message: inventing new type variable names: 'a, 'b, 'c>>
> val it = ``f ` x`` : term

- ``f ^ x``;
<<HOL message: inventing new type variable names: 'a, 'b, 'c>>
> val it = ``f ^ x`` : term
\end{verbatim}
\end{hol}
\end{session}

The rule for carets not followed by white-space is illustrated here,
including an example of what happens when the quoting rule is not
followed:
\begin{session}
\begin{hol}
\begin{verbatim}
- ``f ^^+ x``;
<<HOL message: inventing new type variable names: 'a, 'b, 'c>>
> val it = ``f ^+ x`` : term

- ``f ^+ x``;
! Toplevel input:
! (Parse.Term [QUOTE " (*#loc 2 3*)f ", ANTIQUOTE (+),
!              QUOTE " (*#loc 2 7*) x"]);
!                                                  ^
! Ill-formed infix expression
\end{verbatim}
\end{hol}
\end{session}

The main use of the caret is to introduce \emph{antiquotations} (as
suggested in the last example above).  Within a quotation, expressions
of the form {\small\verb+^(+}$t${\small\verb+)+}
%
\index{ antiquotation, in HOL logic@{\small\verb+^+} (antiquotation, in \HOL{} logic)}
%
(where $t$ is an \ML\ expression of type
%
\index{type checking, in HOL logic@type checking, in \HOL{} logic!antiquotation in}
%
\ml{term} or \ml{type}) are called antiquotations.
%
\index{terms, in HOL logic@terms, in \HOL{} logic!antiquotation}
\index{antiquotation, in HOL logic terms@antiquotation, in \HOL{} logic terms}
%
An antiquotation \holtxt{\^{}($t$)} evaluates to the
\ML{} value of $t$. For example,{\small\verb+``x \/ ^(mk_conj(``y:bool``, ``z:bool``))``+}
evaluates to the same term as {\small\verb+``x \/ (y /\ z)``+}. The
most common use of antiquotation is when the term $t$ is bound to an \ML\
variable $x$. In this case {\small\verb+^(+}$x${\small\verb+)+} can be
abbreviated by {\small\verb+^+}$x$.

The following session illustrates antiquotation.

\setcounter{sessioncount}{0}
\begin{session}
\begin{hol}
\begin{verbatim}
- val y = ``x+1``;
> val y = ``x + 1`` : term

val z = ``y = ^y``;
> val z = ``y = x + 1`` : term

- ``!x:num.?y:num.^z``;
> val it = ``!x. ?y. y = x + 1`` : term
\end{verbatim}
\end{hol}
\end{session}

\noindent Types may be antiquoted as well:

\begin{session}
\begin{hol}
\begin{verbatim}
- val pred = ``:'a -> bool``;
> val pred = ``:'a -> bool`` : hol_type

- ``:^pred -> bool``;
> val it = ``:('a -> bool) -> bool`` : hol_type
\end{verbatim}
\end{hol}
\end{session}

\noindent Quotations are polymorphic, and the type variable of a
quotation corresponds to the type of entity that can be antiquoted
into that quotation.  Because the term parser expects only antiquoted
terms, antiquoting a type into a term quotation requires the use of
\holtxt{ty\_antiq}. For example,%
%
\index{ty_antiq@\ml{ty\_antiq}}

\begin{session}
\begin{hol}
\begin{verbatim}
- ``!P:^pred. P x ==> Q x``;

! Toplevel input:
! Term `!P:^pred. P x ==> Q x`;
!           ^^^^
! Type clash: expression of type
!   hol_type
! cannot have type
!   term

- ``!P:^(ty_antiq pred). P x ==> Q x``;
> val it = `!P. P x ==> Q x` : term
\end{verbatim}
\end{hol}
\end{session}
%
\index{parsing, of HOL logic@parsing, of \HOL{} logic!of quotation syntax|)}



\subsection{Backwards compatibility of syntax}

This section of the manual documents the (extensive) changes made to
the parsing of \HOL{} terms and types in the Taupo release (one of the
HOL3 releases) and beyond from the point of view of a user who doesn't
want to know how to use the new facilities, but wants to make sure
that their old code continues to work cleanly.

The changes which may cause old terms to fail to parse are:
\begin{itemize}
\newcommand\condexp{\holtxt{$p$ => $q$ | $r$}}
\item The precedence of type annotations has completely changed.  It
  is now a very tight suffix (though with a precedence weaker than
  that associated with function application), instead of a weak one.
  This means that \mbox{\tt (x,y:bool \# bool)} should now be written
  as \mbox{\tt (x,y):bool \# bool}. The previous form will now be
  parsed as a type annotation applying to just the \verb+y+.  This
  change brings the syntax of the logic closer to that of SML and
  should make it generally easier to annotate tuples, as one can now
  write \[ (x\,:\,\tau_1,\;y\,:\,\tau_2,\dots z\,:\,\tau_n)
  \] instead of \[
  (x\,:\,\tau_1, \;(y\,:\,\tau_2, \dots (z\,:\,\tau_n)))
  \] where extra parentheses have had to be added just to allow one to
  write a frequently occurring form of constraint.
\item Most arithmetic operators are now left associative instead of
  right associative.  In particular, $+$, $-$, $*$ and {\tt DIV} are
  all left associative.  Similarly, the analogous operators in other
  numeric theories such as {\tt integer} and {\tt real} are also left
  associative.  This brings the \HOL{} parser in line with standard
  mathematical practice.
\item The binding equality in {\tt let} expressions is treated exactly
  the same way as equalities in other contexts.  In previous versions
  of \HOL, equalities in this context have a different, weak binding
  precedence.  This difference can be seen in the following expression
  which parses successfully in the old version:
  \[ {\tt let} \; x \; = \; \condexp \; {\tt
  in} \;Q \] In Taupo releases and later, this expression will not
  parse because the conditional expression binds to the left more
  weakly than the equality binds to the right, and the parser ends up
  believing that the binding between the \verb+let+ and the \verb+in+
  is not an equality after all, as it should be.
\item Old style conditional expressions in the right half of set
  comprehensions have to be parenthesised to avoid confusing the
  parser.  Thus \[
  \{ \; x \; | \; \condexp \; \}
   \qquad\mbox{must be written} \qquad
  \{ \; x \; | \; (\condexp) \; \}
  \] Better yet, {\tt if}-{\tt then}-{\tt else} syntax could be used
  for the conditional expression.
\item Some lexical categories are more strictly policed.  String
  literals (strings inside double quotes) and numerals can't be used
  unless the relevant theories have been loaded.  Nor can these
  literals be used as variables inside binding scopes.
\end{itemize}


\section{A Simple Interactive Proof Manager}\label{sec:goalstack}

The \emph{goal stack} provides a simple interface to tactic-based
interactive proof. When one uses tactics to decompose a proof, many
intermediate states arise; the goalstack takes care of the necessary
bookeeping. The implementation of goalstacks reported here is a
re-design of Larry Paulson's original conception.

The goalstack library is automatically loaded when \HOL{} starts up.

The abstract types \ml{goalstack} and \ml{proofs} are the
focus of backwards proof operations. The type \ml{proofs} can be
regarded as a list of independent goalstacks. Most operations act on
the head of the list of goalstacks; there are also operations so that
the focus can be changed.

\subsection{Starting a goalstack proof}

\begin{hol}
\begin{verbatim}
   g        : term quotation -> proofs
   set_goal : goal -> proofs
\end{verbatim}
\end{hol}

Recall that the type \ml{goal} is an abbreviation for
\ml{term list * term}. To start on a new goal, one gives
\ml{set\_goal} a goal. This creates a new goalstack and makes it the
focus of further operations.

A shorthand for \ml{set\_goal} is the function \ml{g}: it
invokes the parser automatically, and it doesn't allow the the goal to
have any assumptions.

Calling \ml{set\_goal}, or \ml{g}, adds a new proof attempt to the
existing ones, \textit{i.e.}, rather than overwriting the current
proof attempt, the new attempt is stacked on top.

\subsection{Applying a tactic to a goal}

\begin{hol}
\begin{verbatim}
   expandf : tactic -> goalstack
   expand  : tactic -> goalstack
   e       : tactic -> goalstack
\end{verbatim}
\end{hol}

How does one actually do a goalstack proof then? In most cases, the
application of tactics to the current goal is done with the function
\verb+expand+. In the rare case that one wants to apply an
{\it invalid\/} tactic, then \verb+expandf+ is used. (For an
explanation of invalid tactics, see Chapter 24 of Gordon \& Melham.) The
abbreviation \verb+e+ may also be used to expand a tactic.


\subsection{Undo}

\begin{hol}
\begin{verbatim}
   b          : unit -> goalstack
   drop       : unit -> proofs
   dropn      : int  -> proofs
   backup     : unit -> goalstack
   restart    : unit -> goalstack
   set_backup : int  -> unit
\end{verbatim}
\end{hol}

Often (we are tempted to say {\it usually}!) one takes a wrong path
in doing a proof, or makes a mistake when setting a goal. To undo a step
in the goalstack, the function \ml{backup} and its abbreviation
\ml{b} are used. This will restore the goalstack to its previous
state.


To directly back up all the way to the original goal, the function
\ml{restart} may be used. Obviously, it is also important to get
rid of proof attempts that are wrong; for that there is \ml{drop},
which gets rid of the current proof attempt, and \ml{dropn}, which
eliminates the top $n$ proof attempts.


Each proof attempt has its own \emph{undo-list} of previous
states. The undo-list for each attempt is of fixed size (initially
12). If you wish to set this value for the current proof attempt, the
function \ml{set\_backup} can be used. If the size of the backup
list is set to be smaller than it currently is, the undo list will be
immediately truncated. You can not undo a ``proofs-level'' operation, such
as \ml{set\_goal} or \ml{drop}.

\subsection{Viewing the state of the proof manager}

\begin{hol}
\begin{verbatim}
   p            : unit -> goalstack
   status       : unit -> proofs
   top_goal     : unit -> goal
   top_goals    : unit -> goal list
   initial_goal : unit -> goal
   top_thm      : unit -> thm
\end{verbatim}
\end{hol}

To view the state of the proof manager at any time, the functions
\ml{p} and \ml{status} can be used. The former only shows
the top subgoals in the current goalstack, while the second gives a
summary of every proof attempt.

To get the top goal or goals of a proof attempt, use \ml{top\_goal}
and \ml{top\_goals}. To get the original goal of a proof attempt,
use \ml{initial\_goal}.

Once a theorem has been proved, the goalstack that was used to derive it
still exists (including its undo-list): its main job now is to
hold the theorem. This theorem can be retrieved with
\ml{top\_thm}.

\subsection{Switch focus to a different subgoal or proof attempt}

\begin{hol}
\begin{verbatim}
   r             : int -> goalstack
   R             : int -> proofs
   rotate        : int -> goalstack
   rotate_proofs : int -> proofs
\end{verbatim}
\end{hol}

Often we want to switch our attention to a different goal in the current
proof, or a different proof. The functions that do this are
\ml{rotate} and \ml{rotate\_proofs}, respectively. The abbreviations
\ml{r} and \ml{R} are simpler to type in.

\section{High Level Proof---\texttt{bossLib}}
% would use \ml{boss} above but it puts LaTeX into fits
\label{sec:bossLib}
\newcommand\bossLib{\ml{bossLib}}

\index{bossLib@\ml{bossLib}}
The library \bossLib\ marshalls some of the most widely used theorem
proving tools in \HOL{} and provides them with a convenient interface
for interaction. The library currently focuses on three things:
definition of datatypes and functions; high-level interactive proof
operations, and composition of automated reasoners. Loading \bossLib\
commits one to working in a context that already supplies the theories
of booleans, pairs, sums, the option type, arithmetic, and lists.


\subsection{Support for high-level proof steps}
\label{sec:high-level-proof-steps}

The following functions use information in the database to ease the
application of \HOL's underlying functionality:

\begin{verbatim}
   type_rws     : hol_type -> thm list
   Induct       : tactic
   Cases        : tactic
   Cases_on     : term quotation -> tactic
   Induct_on    : term quotation -> tactic
\end{verbatim}

\index{type_rws@\ml{type\_rws}}
\index{TypeBase@\ml{TypeBase}}
%
The function \ml{type\_rws} will search for the given type in the
underlying \ml{TypeBase} database and return useful rewrite rules for
that type. The rewrite rules of the datatype are built from the
injectivity and distinctness theorems, along with the case constant
definition. The simplification tactics \ml{RW\_TAC}, \ml{SRW\_TAC},
and the \simpset{} \ml{(srw\_ss())} automatically include these
theorems.  Other tactics used with other \simpset{}s will need these
theorems to be manually added.

\index{induction theorems, in HOL logic@induction theorems, in \HOL{} logic!for algebraic data types}
%
The \ml{Induct} tactic makes it convenient to invoke induction. When
it is applied to a goal, the leading universal quantifier is examined;
if its type is that of a known datatype, the appropriate structural
induction tactic is extracted and applied.

The \ml{Cases} tactic makes it convenient to invoke case
analysis. The leading universal quantifier in the goal is examined; if
its type is that of a known datatype, the appropriate structural
case analysis theorem is extracted and applied.

The \ml{Cases\_on} tactic takes a quotation, which is
parsed into a term $M$, and then $M$ is searched for in the goal. If $M$
is a variable, then a variable with the same name is searched for. Once
the term to split over is known, its type and the associated facts are
obtained from the underlying database and used to perform the case
split. If some free variables of $M$ are bound in the goal, an attempt
is made to remove (universal) quantifiers so that the case split has
force. Finally, $M$ need not appear in the goal, although it should at
least contain some free variables already appearing in the goal. Note
that the \ml{Cases\_on} tactic is more general than \ml{Cases}, but
it does require an explicit term to be given.

The \ml{Induct\_on} tactic takes a quotation, which is parsed into a
term $M$, and then $M$ is searched for in the goal. If $M$ is a
variable, then a variable with the same name is searched for. Once the
term to induct on is known, its type and the associated facts are
obtained from the underlying database and used to perform the
induction.  If $M$ is not a variable, a new variable $v$ not already
occurring in the goal is created, and used to build a term $v = M$
which the goal is made conditional on before the induction is
performed. First however, all terms containing free variables from $M$
are moved from the assumptions to the conclusion of the goal, and all
free variables of $M$ are universally quantified. \ml{Induct\_on} is
more general than \ml{Induct}, but it does require an explicit term to
be given.

Three supplementary entrypoints have been provided for more exotic
inductions:
\begin{description}
\item [\ml{completeInduct\_on}] performs complete induction on the
  term denoted by the given quotation. Complete induction allows a
  seemingly \footnote{Complete induction and ordinary mathematical
    induction are each derivable from the other.} stronger induction
  hypothesis than ordinary mathematical induction: to wit, when
  inducting on $n$, one is allowed to assume the property holds for
  \emph{all} $m$ smaller than $n$. Formally: $\forall P.\ (\forall x.\
  (\forall y.\ y < x \supset P\, y) \supset P\,x) \supset \forall x.\
  P\,x$. This allows the inductive hypothesis to be used more than
  once, and also allows instantiating the inductive hypothesis to
  other than the predecessor.

\item [\ml{measureInduct\_on}] takes a quotation, and breaks it
  apart to find a term and a measure function with which to induct.
  For example, if one wanted to induct on the length of a list
  \holtxt{L}, the invocation \ml{measureInduct\_on~`LENGTH L`}
  would be be appropriate.

\item [\ml{recInduct}] takes a induction theorem generated by
\ml{Define} or \ml{Hol\_defn} and applies it to the current goal.

\end{description}


\subsection{Automated reasoners}
\label{sec:automated-reasoners}

\ml{bossLib} brings together the most powerful reasoners in \HOL{} and
tries to make it easy to compose them in a simple way. We take our basic
reasoners from \ml{mesonLib}, \ml{simpLib}, and \ml{numLib},
but the point of \ml{bossLib} is to provide a layer of abstraction so
the user has to know only a few entrypoints.\footnote{In the mid 1980's
Graham Birtwistle advocated such an approach, calling it `Ten Tactic
HOL'.} (These underlying libraries, and others providing similarly
powerful tools are described in detail in sections below.)
\begin{hol}
\begin{verbatim}
   PROVE      : thm list -> term -> thm
   PROVE_TAC  : thm list -> tactic

   METIS_TAC  : thm list -> tactic
   METIS_PROVE: thm list -> term -> thm

   DECIDE     : term quotation -> thm
   DECIDE_TAC : tactic
\end{verbatim}
\end{hol}
The inference rule \texttt{PROVE} (and the corresponding tactic
\texttt{PROVE\_TAC}) takes a list of theorems and a term, and attempts
to prove the term using a first order reasoner.  The two \ml{METIS}
functions perform the same functionality but use a different
underlying proof method.  The \texttt{PROVE} entry-points refer to the
\texttt{meson} library, which is further described in
Section~\ref{sec:mesonLib} below. The \ml{METIS} system is described
in Section~\ref{sec:metisLib}.  The inference rule \texttt{DECIDE}
(and the corresponding tactic \texttt{DECIDE\_TAC}) applies a decision
procedure that (at least) handles statements of linear arithmetic.

\begin{hol}
\begin{verbatim}
   RW_TAC   : simpset -> thm list -> tactic
   SRW_TAC  : ssfrag list -> thm list -> tactic
   &&       : simpset * thm list -> simpset  (* infix *)
   std_ss   : simpset
   arith_ss : simpset
   list_ss  : simpset
   srw_ss   : unit -> simpset
\end{verbatim}
\end{hol}
%
\index{RW_TAC@\ml{RW\_TAC}} The rewriting tactic \ml{RW\_TAC} works by
first adding the given theorems into the given \simpset; then it
simplifies the goal as much as possible; then it performs case splits
on any conditional expressions in the goal; then it repeatedly (1)
eliminates all hypotheses of the form $v = M$ or $M = v$ where $v$ is
a variable not occurring in $M$, (2) breaks down any equations between
constructor terms occurring anywhere in the goal. Finally,
\ml{RW\_TAC} lifts \holtxt{let}-expressions within the goal so that
the binding equations appear as
abbreviations\index{abbreviations!tactic-based proof} in the
assumptions.

\index{SRW_TAC@\ml{SRW\_TAC}} The tactic \ml{SRW\_TAC} is similar to
\ml{RW\_TAC}, but works with respect to an underlying \simpset{}
(accessible through the function \ml{srw\_ss}) that is updated as new
context is loaded.  This \simpset{} can be augmented through the
addition of ``\simpset{} fragments'' (\ml{ssfrag} values) and
theorems.  In situations where there are many large types stored in
the system, \ml{RW\_TAC}'s performance can suffer because it
repeatedly adds all of the rewrite theorems for the known types into a
\simpset{} before attacking the goal.  On the other hand,
\ml{SRW\_TAC} loads rewrites into the \simpset{} underneath
\ml{srw\_ss()} just once, making for faster operation in this
situation.

\ml{bossLib} provides a number of simplification sets. The
simpset for pure logic, sums, pairs, and the \ml{option} type is
named \ml{std\_ss}. The simpset for arithmetic is named
\ml{arith\_ss}, and the simpset for lists is named \ml{list\_ss}.
The simpsets provided by \bossLib{} strictly increase in strength:
\ml{std\_ss} is contained in \ml{arith\_ss}, and \ml{arith\_ss} is
contained in \ml{list\_ss}.  The infix combinator \ml{\&\&} is used
to build a new \simpset{} from a given \simpset{} and a list of
theorems. \HOL's simplification technology is described further in
Section~\ref{sec:simpLib} below and in the \REFERENCE.

\begin{hol}
\begin{verbatim}
   by : term quotation * tactic -> tactic (* infix 8 *)
   SPOSE_NOT_THEN : (thm -> tactic) -> tactic
\end{verbatim}
\end{hol}
The function \ml{by} is an infix operator that takes a quotation
and a tactic $tac$. The quotation is parsed into a term $M$. When the
invocation ``\ml{$M$ by $\mathit{tac}$}'' is applied to a goal
$(A,g)$, a new subgoal $(A,M)$ is created and $tac$ is applied to it.
If the goal is proved, the resulting theorem is broken down and added
to the assumptions of the original goal; thus the proof proceeds with
the goal $((M::A), g)$. (Note however, that case-splitting will happen
if the breaking-down of $\ \vdash M$ exposes disjunctions.) Thus
\ml{by} allows a useful style of `assertional' or `Mizar-like'
reasoning to be mixed with ordinary tactic proof.\footnote{Proofs in
  the Mizar system are readable documents, unlike most
  tactic-based proofs.}

The \ml{SPOSE\_NOT\_THEN} entrypoint initiates a proof by
contradiction by assuming the negation of the goal and driving the
negation inwards through quantifiers. It provides the resulting
theorem as an argument to the supplied function, which will use the
theorem to build and apply a tactic.

\section{First Order Proof---\texttt{mesonLib} and \texttt{metisLib}}
\label{sec:first-order-proof}
\index{decision procedures!first-order logic}

First order proof is a powerful theorem-proving technique that can
finish off complicated goals.  Unlike tools such as the simplifier, it
either proves a goal outright, or fails.  It can not transform a goal
into a different (and more helpful) form.

\subsection{Model elimination---\texttt{mesonLib}}
\label{sec:mesonLib}

\index{meson (model elimination) procedure@\ml{meson} (model elimination) procedure}
\index{model elimination method for first-order logic}

The \ml{meson} library is an implementation of the
model-elimination method for finding proofs of goals in first-order
logic.  There are three main entry-points:
\begin{hol}
\begin{verbatim}
   MESON_TAC     : thm list -> tactic
   ASM_MESON_TAC : thm list -> tactic
   GEN_MESON_TAC : int -> int -> int -> thm list -> tactic
\end{verbatim}
\end{hol}

Each of these tactics attempts to prove the goal.  They will either
succeed in doing so, or fail with a ``depth exceeded'' exception.  If
the branching factor in the search-space is high, the \texttt{meson}
tactics may also take a very long time to reach the maximum depth.

All of the \texttt{meson} tactics take a list of theorems.  These
extra facts are used by the decision procedure to help prove the goal.
\texttt{MESON\_TAC} ignores the goal's assumptions; the other two
entry-points include the assumptions as part of the sequent to be
proved.

The extra parameters to \ml{GEN\_MESON\_TAC} provide extra control of
the behaviour of the iterative deepening that is at the heart of the
search for a proof.  In any given iteration, the algorithm searches
for a proof of depth no more than a parameter $d$.  The default
behaviour for \ml{MESON\_TAC} and \ml{ASM\_MESON\_TAC} is to start $d$
at 0, to increment it by one each time a search fails, and to fail if
$d$ exceeds the value stored in the reference value
\ml{mesonLib.max\_depth}.  By way of contrast,
\ml{GEN\_MESON\_TAC~min~max~step} starts $d$ at \ml{min}, increments
it by \ml{step}, and gives up when $d$ exceeds \ml{max}.

The \ml{PROVE\_TAC} function from \ml{bossLib} performs some
normalisation, before passing a goal and its assumptions to
\ml{ASM\_MESON\_TAC}.  Because of this normalisation, in most
circumstances, \ml{PROVE\_TAC} should be preferred to
\ml{ASM\_MESON\_TAC}.

\subsection{Resolution---\texttt{metisLib}}
\label{sec:metisLib}

\index{metis (resolution) procedure@\ml{metis} (resolution) procedure}
\index{resolution method for first-order logic}

The \ml{metis} library is an implementation of the resolution method
for finding proofs of goals in first-order logic. There are two main
entry-points:

\begin{hol}
\begin{verbatim}
   METIS_TAC   : thm list -> tactic
   METIS_PROVE : thm list -> term -> thm
\end{verbatim}
\end{hol}

Both functions take a list of theorems, and these are used as lemmas
in the proof. \texttt{METIS\_TAC} is a tactic, and will either succeed
in proving the goal, or if unsuccessful will either fail or loop
forever. \texttt{METIS\_PROVE} takes a term $t$ and tries to prove a
theorem with conclusion $t$: if successful, the theorem $\vdash t$ is
returned. As for \texttt{METIS\_TAC}, it might fail or loop forever if
the proof search is unsuccessful.

The \texttt{metisLib} family of proof tools implement the ordered
resolution and ordered paramodulation calculus for first order logic,
which usually makes them better suited to goals requiring non-trivial
equality reasoning than the tactics in \texttt{mesonLib}.


\section{Simplification---\texttt{simpLib}}
\label{sec:simpLib}
\index{simplification|(}

The simplifier is \HOL's most sophisticated rewriting engine.  It is
recommended as a general purpose work-horse during interactive
theorem-proving.  As a rewriting tool, the simplifier's general role
is to apply theorems of the general form
\[
\vdash l = r
\]
to terms, replacing instances of $l$ in the term with $r$. Thus, the
basic simplification routine is a \emph{conversion}, taking a term
$t$, and returning a theorem $\vdash t = t'$, or the exception
\ml{UNCHANGED}.

The basic conversion is
\begin{hol}
\begin{verbatim}
   simpLib.SIMP_CONV : simpLib.simpset -> thm list -> term -> thm
\end{verbatim}
\end{hol}
The first argument, a \simpset, is the standard way of providing a
collection of rewrite rules (and other data, to be explained below) to
the simplifier.  There are \simpset{}s accompanying most of \HOL's
major theories.  For example, the \simpset{} \ml{boolSimps.bool\_ss}
embodies all of the usual rewrite theorems one would want over boolean
formulas:
\setcounter{sessioncount}{0}
\begin{session}
\begin{hol}
\begin{verbatim}
- SIMP_CONV bool_ss [] ``p /\ T \/ ~(q /\ r)``;
> val it = |- p /\ T \/ ~(q /\ r) = p \/ ~q \/ ~r : thm
\end{verbatim}
\end{hol}
\end{session}
In addition to rewriting with the obvious theorems, \ml{bool\_ss} is
also capable of performing simplifications that are not expressible as
simple theorems:
\begin{session}
\begin{hol}
\begin{verbatim}
- SIMP_CONV bool_ss [] ``?x. (\y. P (f y)) x /\ (x = z)``;
> val it = |- (?x. (\y. P (f y)) x /\ (x = z)) = P (f z) : thm
\end{verbatim}
\end{hol}
\end{session}
In this example, the simplifier performed a $\beta$-reduction in the
first conjunct under the existential quantifier, and then did an
``unwinding'' or ``one-point'' reduction, recognising that the only
possible value for the quantified variable \holtxt{x} was the value
\holtxt{z}.

The second argument to \ml{SIMP\_CONV} is a list of theorems to be
added to the provided \simpset, and used as additional rewrite rules.
In this way, users can temporarily augment standard \simpset{}s with
their own rewrites.  If a particular set of theorems is often used as
such an argument, then it is possible to build a \simpset{} value to
embody these new rewrites.

For example, the rewrite \ml{arithmeticTheory.LEFT\_ADD\_DISTRIB}, which
states that $p(m + n) = pm + pn$ is not part of any of \HOL's standard
\simpset{}s.  This is because it can cause an unappealing increase in
term size (there are two occurrences of $p$ on the right hand
side of the theorem).  Nonetheless, it is clear that this theorem may
be appropriate on occasion:
\begin{session}
\begin{hol}
\begin{verbatim}
- SIMP_CONV bossLib.arith_ss [LEFT_ADD_DISTRIB] ``p * (n + 1)``;
> val it = |- p * (n + 1) = p + n * p : thm
\end{verbatim}
\end{hol}
\end{session}
Note how the \ml{arith\_ss} \simpset{} has not only simplified the
intermediate \ml{(p * 1)} term, but also re-ordered the addition to
put the simpler term on the left, and sorted the multiplication's
arguments.


\subsection{Simplification tactics}
\label{sec:simplification-tactics}
\index{simplification!tactics}

The simplifier is implemented around the conversion \ml{SIMP\_CONV},
which is a function for `converting' terms into theorems.  To apply
the simplifier to goals (alternatively, to perform tactic-based proofs
with the simplifier), \HOL{} provides five tactics, all of which are
available in \ml{bossLib}.

\subsubsection{\ml{SIMP\_TAC : simpset -> thm list -> tactic}}
\index{SIMP_TAC@\ml{SIMP\_TAC}}

\ml{SIMP\_TAC} is the simplest simplification tactic: it attempts to
simplify the current goal (ignoring the assumptions) using the given
\simpset{} and the additional theorems.  It is no more than the
lifting of the underlying \ml{SIMP\_CONV} conversion to the tactic
level through the use of the standard function \ml{CONV\_TAC}.

\subsubsection{\ml{ASM\_SIMP\_TAC : simpset -> thm list -> tactic}}
\index{ASM_SIMP_TAC@\ml{ASM\_SIMP\_TAC}}

Like \ml{SIMP\_TAC}, \ml{ASM\_SIMP\_TAC} simplifies the current goal
(leaving the assumptions untouched), but includes the goal's
assumptions as extra rewrite rules.  Thus:
\begin{session}
\begin{hol}
\begin{verbatim}
1 subgoal:
> val it =
    P x
    ------------------------------------
      x = 3
     : goalstack

- e (ASM_SIMP_TAC bool_ss []);
OK..
1 subgoal:
> val it =
    P 3
    ------------------------------------
      x = 3
     : goalstack
\end{verbatim}
\end{hol}
\end{session}
\noindent
In this example, \ml{ASM\_SIMP\_TAC} used \holtxt{x = 3} as an
additional rewrite rule, and replaced the \holtxt{x} of \holtxt{P x}
with \holtxt{3}.  When an assumption is used by \ml{ASM\_SIMP\_TAC} it
is converted into rewrite rules in the same way as theorems passed in
the list given as the tactic's second argument.  For example, an
assumption \holtxt{\~{}P} will be treated as the rewrite \holtxt{|- P = F}.

\subsubsection{\ml{FULL\_SIMP\_TAC : simpset -> thm list -> tactic}}
\index{FULL_SIMP_TAC@\ml{FULL\_SIMP\_TAC}}

\noindent
The tactic \ml{FULL\_SIMP\_TAC} simplifies not only a goal's
conclusion but its assumptions as well.  It proceeds by simplifying
each assumption in turn, additionally using earlier assumptions in the
simplification of later assumptions.  After being simplified, each
assumption is added back into the goal's assumption list with the
tactic \ml{STRIP\_ASSUME\_TAC}.  This means that assumptions that
become conjunctions will have each conjunct assumed separately.
Assumptions that become disjunctions will cause one new sub-goal to be
created for each disjunct.  If an assumption is simplified to false,
this will solve the goal.

\ml{FULL\_SIMP\_TAC} attacks the assumptions in the order in which
they appear in the list of terms that represent the goal's
assumptions.  Typically then, the first assumption to be simplified
will be the assumption most recently added.  Viewed in the light of
\ml{goalstackLib}'s printing of goals, \ml{FULL\_SIMP\_TAC} works its
way up the list of assumptions, from bottom to top.

The following demonstrates a simple use of \ml{FULL\_SIMP\_TAC}:
\begin{session}
\begin{hol}
\begin{verbatim}
    x + y < z
    ------------------------------------
      0.  f x < 10
      1.  x = 4
     : goalstack

- e (FULL_SIMP_TAC bool_ss []);
OK..
1 subgoal:
> val it =
    4 + y < z
    ------------------------------------
      0.  f 4 < 10
      1.  x = 4
     : goalstack
\end{verbatim}
\end{hol}
\end{session}
In this example, the assumption \holtxt{x = 4} caused the \holtxt{x}
in the assumption \holtxt{f x < 10} to be replaced by \holtxt{4}.  The
\holtxt{x} in the goal was similarly replaced.  If the assumptions had
appeared in the opposite order, only the \holtxt{x} of the goal would
have changed.

The next session more demonstrates more interesting behaviour:
\begin{session}
\begin{hol}
\begin{verbatim}
> val it =
    f x + 1 < 10
    ------------------------------------
      x <= 4
     : goalstack

- e (FULL_SIMP_TAC bool_ss [arithmeticTheory.LESS_OR_EQ]);
OK..
2 subgoals:
> val it =
    f 4 + 1 < 10
    ------------------------------------
      x = 4

    f x + 1 < 10
    ------------------------------------
      x < 4
     : goalstack
\end{verbatim}
\end{hol}
\end{session}
In this example, the goal was rewritten with the theorem stating
\[
\vdash x \leq y \equiv x < y \lor x = y
\]
Turning the assumption into a disjunction resulted in two sub-goals.
In the second of these, the assumption \holtxt{x = 4} further
simplified the rest of the goal.

\subsubsection{\ml{RW\_TAC : simpset -> thm list -> tactic}}
\index{RW_TAC@\ml{RW\_TAC}}

Though its type is the same as the simplification tactics already
described, \ml{RW\_TAC} is an ``augmented'' tactic.  It is augmented
in two ways:
\begin{itemize}
\item When simplifying the goal, the provided \simpset{} is augmented
  not only with the theorems explicitly passed in the second argument,
  but also with all of the rewrite rules from the \ml{TypeBase}, and
  also with the goal's assumptions.
%
  \index{TypeBase@\ml{TypeBase}}
\item \ml{RW\_TAC} also does more than just perform simplification.
  It also repeatedly ``strips'' the goal.  For example, it moves the
  antecedents of implications into the assumptions, splits
  conjunctions, and case-splits on conditional expressions.  This
  behaviour can rapidly remove a lot of syntactic complexity from
  goals, revealing the kernel of the problem.  On the other hand, this
  aggressive splitting can also result in a large number of
  sub-goals.  \ml{RW\_TAC}'s augmented behaviours are intertwined with
  phases of simplification in a way that is difficult to describe.
\end{itemize}

\subsubsection{\ml{SRW\_TAC : ssfrag list -> thm list -> tactic}}
\index{SRW_TAC@\ml{SRW\_TAC}}

The tactic \ml{SRW\_TAC} has a different type from the other
simplification tactics.  It does not take a \simpset{} as an argument.
Instead its operation always builds on the built-in \simpset{}
\ml{srw\_ss()} (further described in Section~\ref{sec:srw_ss}).  The
theorems provided as \ml{SRW\_TAC}'s second argument are treated in
the same way as the by the other simplification tactics.  Finally, the
list of \simpset{} fragments are merged into the underlying
\simpset{}, allowing the user to merge in additional simplification
capabilities if desired.

For example, to include the Presburger decision procedure, one could
write
\begin{hol}
\begin{verbatim}
   SRW_TAC [ARITH_ss][]
\end{verbatim}
\end{hol}
\Simpset{} fragments are described below in
Section~\ref{sec:simpset-fragments}.

\ml{SRW\_TAC} performs the same mixture of simplification and
goal-splitting as does \ml{RW\_TAC}.  The main differences between the
two tactics lie in the fact that the latter can be inefficient when
working with a large \ml{TypeBase}, and in the fact that working with
\ml{SRW\_TAC} saves one from having to explicitly construct
\simpset{}s that include all of the current context's ``appropriate''
rewrites.  The latter ``advantage'' is based on the assumption that
\ml{(srw\_ss())} never includes inappropriate rewrites.  The presence
of unused rewrites is never a concern: the presence of rewrites that
do the wrong thing can be a major irritation.

\subsection{The standard \simpset{}s}
\label{sec:standard-simpsets}

\HOL{} comes with a number of standard \simpset{}s.  All of these are
accessible from within \ml{bossLib}, though some originate in other
structures.

\subsubsection{\ml{pure\_ss} and \ml{bool\_ss}}
\label{sec:purebool-ss}
%
\index{pure_ss@\ml{pure\_ss}}
%
The \ml{pure\_ss} \simpset{} (defined in structure \ml{pureSimps})
contains no rewrite theorems at all, and plays the role of a blank
slate within the space of possible \simpset{}s.  When constructing a
completely new \simpset, \ml{pure\_ss} is a possible starting point.
The \ml{pure\_ss} \simpset{} has just two components: congruence rules
for specifying how to traverse terms, and a function that turns
theorems into rewrite rules.  Congruence rules are further described
in Section~\ref{sec:advanced-simplifier}; the generation of rewrite
rules from theorems is described in
Section~\ref{sec:simplifier-rewriting}.

\index{bool_ss (simplification set)@\ml{bool\_ss} (simplification set)}
%
The \ml{bool\_ss} \simpset{} (defined in structure \ml{boolSimps}) is
often used when other \simpset{}s might do too much.  It contains
rewrite rules for the boolean connectives, and little more.  It
contains all of the de~Morgan theorems for moving negations in over
the connectives (conjunction, disjunction, implication and conditional
expressions), including the quantifier rules that have $\neg(\forall
x.\,P(x))$ and $\neg(\exists x.\,P (x))$ on their left-hand sides.  It
also contains the rules specifying the behaviour of the connectives
when the constants \holtxt{T} and \holtxt{F} appear as their
arguments.  (One such rule is \holtxt{|- T /\bs{} p = p}.)

As in the example above, \ml{bool\_ss} also performs
$\beta$-reductions and one-point unwindings.  The latter turns terms
of the form \[
\exists x.\;P(x)\land\dots (x = e) \dots\land Q(x)
\]
into
\[
P(e) \land \dots \land Q(e)
\]
Similarly, unwinding will turn $\forall x.\;(x = e)
\Rightarrow P(x)$ into $P(e)$.

Finally, \ml{bool\_ss} also includes congruence rules that allow
the simplifier to make additional assumptions when simplifying
implications and conditional expressions.  This feature is further
explained in Section~\ref{sec:simplifier-rewriting} below, but can be
illustrated by some examples (the first also demonstrates unwinding
under a universal quantifier):
\begin{session}
\begin{hol}
\begin{verbatim}
- SIMP_CONV bool_ss [] ``!x. (x = 3) /\ P x ==> Q x /\ P 3``;
> val it = |- (!x. (x = 3) /\ P x ==> Q x /\ P 3) = P 3 ==> Q 3 : thm

- SIMP_CONV bool_ss [] ``if ~(x = 3) then P x else Q x``;
> val it = |- (if ~(x = 3) then P x else Q x) =
              (if ~(x = 3) then P x else Q 3) : thm
\end{verbatim}
\end{hol}
\end{session}

\subsubsection{\ml{std\_ss}}
%
\index{std_ss (simplification set)@\ml{std\_ss} (simplification set)}
%
The \ml{std\_ss} \simpset{} is defined in \ml{bossLib}, and adds
rewrite rules pertinent to the types of sums, pairs, options and
natural numbers to \ml{bool\_ss}.
\begin{session}
\begin{hol}
\begin{verbatim}
- SIMP_CONV std_ss [] ``FST (x,y) + OUTR (INR z)``;
<<HOL message: inventing new type variable names: 'a, 'b>>
> val it = |- FST (x,y) + OUTR (INR z) = x + z : thm

- SIMP_CONV std_ss [] ``case SOME x of NONE -> P || SOME y -> f y``;
> val it = |- (case SOME x of NONE -> P || SOME v -> f v) = f x : thm
\end{verbatim}
\end{hol}
\end{session}

With the natural numbers, the \ml{std\_ss} \simpset{} can calculate
with ground values, and also includes a suite of ``obvious rewrites''
for formulas including variables.
\begin{session}
\begin{hol}
\begin{verbatim}
- SIMP_CONV std_ss [] ``P (0 <= x) /\ Q (y + x - y)``;
> val it = |- P (0 <= x) /\ Q (y + x - y) = P T /\ Q x : thm

- SIMP_CONV std_ss [] ``23 * 6 + 7 ** 2 - 31 DIV 3``;
> val it = |- 23 * 6 + 7 ** 2 - 31 DIV 3 = 177 : thm
\end{verbatim}
\end{hol}
\end{session}

\subsubsection{\ml{arith\_ss}}
%
\index{arith_ss (simplification set)@\ml{arith\_ss} (simplification set)}
%
The \ml{arith\_ss} \simpset{} (defined in \ml{bossLib}) extends
\ml{std\_ss} by adding the ability to decide formulas of Presburger
arithmetic, and to normalise arithmetic expressions (collecting
coefficients, and re-ordering summands).  The underlying natural
number decision procedure is that described in
Section~\ref{sec:numLib} below.

These two facets of the \ml{arith\_ss} \simpset{} are demonstrated
here:
\begin{session}
\begin{hol}
\begin{verbatim}
- SIMP_CONV arith_ss [] ``x < 3 /\ P x ==> x < 20 DIV 2``;
> val it = |- x < 3 /\ P x ==> x < 20 DIV 2 = T : thm

- SIMP_CONV arith_ss [] ``2 * x + y - x + y``;
> val it = |- 2 * x + y - x + y = x + 2 * y : thm
\end{verbatim}
\end{hol}
\end{session}
Note that subtraction over the natural numbers works in ways that can
seem unintuitive.  In particular, coefficient normalisation may not
occur when first expected:
\begin{session}
\begin{hol}
\begin{verbatim}
- SIMP_CONV arith_ss [] ``2 * x + y - z + y``;
! Uncaught exception:
! UNCHANGED
\end{verbatim}
\end{hol}
\end{session}
Over the natural numbers, the expression $2 x + y - z + y$ is not
equal to $2 x + 2 y - z$.  In particular, these expressions are not
equal when $2x + y < z$.

\subsubsection{\ml{list\_ss}}
%
\index{list_ss (simplification set)@\ml{list\_ss} (simplification set)}
%
The last pure \simpset{} value in \ml{bossLib}, \ml{list\_ss} adds
rewrite theorems about the type of lists to \ml{arith\_ss}.  These
rewrites include the obvious facts about the list type's constructors
\holtxt{NIL} and \holtxt{CONS}, such as the fact that \holtxt{CONS} is
injective:
\begin{hol}
\begin{verbatim}
   (h1 :: t1 = h2 :: t2) = (h1 = h2) /\ (t1 = t2)
\end{verbatim}
\end{hol}
Conveniently, \ml{list\_ss} also includes rewrites for the functions
defined by primitive recursion over lists.  Examples include
\holtxt{MAP}, \holtxt{FILTER} and \holtxt{LENGTH}.  Thus:
\begin{session}
\begin{hol}
\begin{verbatim}
- SIMP_CONV list_ss [] ``MAP (\x. x + 1) [1;2;3;4]``;
> val it = |- MAP (\x. x + 1) [1; 2; 3; 4] = [2; 3; 4; 5] : thm

- SIMP_CONV list_ss [] ``FILTER (\x. x < 4) [1;2;y + 4]``;
> val it = |- FILTER (\x. x < 4) [1; 2; y + 4] = [1; 2] : thm

- SIMP_CONV list_ss [] ``LENGTH (FILTER ODD [1;2;3;4;5])``;
> val it = |- LENGTH (FILTER ODD [1; 2; 3; 4; 5]) = 3 : thm
\end{verbatim}
\end{hol}
\end{session}
These examples demonstrate how the simplifier can be used as a general
purpose symbolic evaluator for terms that look a great deal like those
that appear in a functional programming language.  Note that
this functionality is also provided by \ml{computeLib} (see
Section~\ref{sec:computeLib} below); \ml{computeLib} is more
efficient, but less general than the simplifier.  For example:
\begin{session}
\begin{hol}
\begin{verbatim}
- EVAL ``FILTER (\x. x < 4) [1;2;y + 4]``;
> val it =
    |- FILTER (\x. x < 4) [1; 2; y + 4] =
       1::2::(if y + 4 < 4 then [y + 4] else []) : thm
\end{verbatim}
\end{hol}
\end{session}

\subsubsection{The ``stateful'' \simpset---\ml{srw\_ss()}}
\label{sec:srw_ss}
\index{srw_ss (simplification set)@\ml{srw\_ss} (simplification set)}

The last \simpset{} exported by \ml{bossLib} is hidden behind a
function.  The \ml{srw\_ss} value has type \ml{unit -> simpset}, so
that one must type \ml{srw\_ss()} in order to get a \simpset{} value.
This use of a function type allows the underlying \simpset{} to be
stored in an \ML{} reference, and allows it to be updated
dynamically.  In this way, referential transparency is deliberately
broken.  All of the other \simpset{}s will always behave identically:
\ml{SIMP\_CONV~bool\_ss} is the same simplification routine wherever
and whenever it is called.

In contrast, \ml{srw\_ss} is designed to be updated.  When a theory is
loaded, when a new type is defined, the value behind \ml{srw\_ss()}
changes, and the behaviour of \ml{SIMP\_CONV} applied to
\ml{(srw\_ss())} changes with it.  The design philosophy behind
\ml{srw\_ss} is that it should always be a reasonable first choice in
all situations where the simplifier is used.

This versatility is illustrated in the following example:
\begin{session}
\begin{hol}
\begin{verbatim}
- Hol_datatype `tree = Leaf | Node of num => tree => tree`;
<<HOL message: Defined type: "tree">>
> val it = () : unit

- SIMP_CONV (srw_ss()) [] ``Node x Leaf Leaf = Node 3 t1 t2``;
<<HOL message: Initialising SRW simpset ... done>>
> val it =
    |- (Node x Leaf Leaf = Node 3 t1 t2) =
       (x = 3) /\ (Leaf = t1) /\ (Leaf = t2) : thm

- load "pred_setTheory";
> val it = () : unit

- SIMP_CONV (srw_ss()) [] ``x IN { y | y < 6}``;
> val it = |- x IN {y | y < 6} = x < 6 : thm
\end{verbatim}
\end{hol}
\end{session}
%
Users can augment the stateful \simpset{} themselves with the function
%
\begin{boxed}
\index{export_rewrites@\ml{export\_rewrites}}
\begin{hol}
\begin{verbatim}
   BasicProvers.export_rewrites : string list -> unit
\end{verbatim}
\end{hol}
\end{boxed}
The strings passed to \ml{export\_rewrites} are the names of theorems
in the current segment (those that will be exported when
\ml{export\_theory} is called).  Not only are these theorems added to
the underlying \simpset{} in the current session, but they will be
added in future sessions when the current theory is reloaded.
\begin{session}
\begin{hol}
\begin{verbatim}
- val tsize_def = Define`
  (tsize Leaf = 0) /\
  (tsize (Node n t1 t2) = n + tsize t1 + tsize t2)
`;
Definition has been stored under "tsize_def".
> val tsize_def =
    |- (tsize Leaf = 0) /\
       !n t1 t2. tsize (Node n t1 t2) = n + tsize t1 + tsize t2 : thm

- val _ = BasicProvers.export_rewrites ["tsize_def"];

- SIMP_CONV (srw_ss()) [] ``tsize (Node 4 (Node 6 Leaf Leaf) Leaf)``;
> val it = |- tsize (Node 4 (Node 6 Leaf Leaf) Leaf) = 10 : thm
\end{verbatim}
\end{hol}
\end{session}

As a general rule, \ml{(srw\_ss())} includes all of its context's
``obvious rewrites'', as well as code to do standard calculations
(such as the arithmetic performed in the above example).  It does not
include decision procedures that may exhibit occasional poor
performance, so the \simpset{} fragments containing these procedures
should be added manually to those simplification invocations that need
them.

\subsection{\Simpset{} fragments}
\label{sec:simpset-fragments}
\index{simplification!simpset fragments}

The \simpset{} fragment is the basic building block that is used to
construct \simpset{} values.  There is one basic function that
performs this construction:
\begin{hol}
\begin{verbatim}
   op ++  : simpset * ssfrag -> simpset
\end{verbatim}
\end{hol}
where \ml{++} is an infix.  In general, it is best to build on top of
the \ml{pure\_ss} \simpset{} or one of its descendents in order to
pick up the default ``filter'' function for converting theorems to
rewrite rules.  (This filtering process is described below in
Section~\ref{sec:generating-rewrite-rules}.)

For major theories (or groups thereof), a collection of relevant
\simpset{} fragments is usually found in the module \ml{<thy>Simps},
with \ml{<thy>} the name of the theory.  For example, \simpset{}
fragments for the theory of natural numbers are found in
\ml{numSimps}, and fragments for lists are found in \ml{listSimps}.

Some of the distribution's standard \simpset{} fragments are described
in Table~\ref{table:ssfrags}.  These and other \simpset{} fragments
are described in more detail in the \REFERENCE.

\begin{table}[htbp]
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lp{0.65\textwidth}}
\ml{BOOL\_ss} &
Standard rewrites for the boolean operators
(conjunction, negation \&c), as well as a conversion for performing
$\beta$-reduction.  (In \ml{boolSimps}.)
\\
\ml{CONG\_ss} & Congruence rules for implication and conditional
expressions. (In \ml{boolSimps}.)
\\
\ml{ARITH\_ss} &
The natural number decision
procedure for universal Presburger arithmetic. (In \ml{numSimps}.)
\\
\ml{PRED\_SET\_AC\_ss} & AC-normalisation for unions and intersections
over sets. (In \ml{pred\_setSimps}.)
\end{tabular}
\end{center}
\caption{Some of \HOL's standard \simpset{} fragments}
\label{table:ssfrags}
\end{table}

\Simpset{} fragments are ultimately constructed with the \ml{SSFRAG}
constructor:
\begin{hol}
\begin{verbatim}
   SSFRAG : { convs  : convdata list,
              rewrs  : thm list,
              ac     : (thm * thm) list,
              filter : (thm -> thm list) option,
              dprocs : Traverse.reducer list,
              congs  : thm list }
           -> ssfrag
\end{verbatim}
\end{hol}
A complete description of the various fields of the record passed to
\ml{SSFRAG}, and their meaning is given in \REFERENCE.  The
\ml{rewrites} function provides an easy route to constructing a
fragment that just includes a list of rewrites:
\begin{hol}
\begin{verbatim}
   rewrites : thm list -> ssfrag
\end{verbatim}
\end{hol}

\subsection{Rewriting with the simplifier}
\label{sec:simplifier-rewriting}

Rewriting is the simplifier's ``core operation''.  This section
describes the action of rewriting in more detail.


\subsubsection{Basic rewriting}
\label{sec:basic-rewriting}

Given a rewrite rule of the form \[
\vdash \ell = r
\]
the simplifier will perform a top-down scan of the input term $t$,
looking for \emph{matches}~(see Section~\ref{sec:simp-homatch} below)
of $\ell$ inside $t$.  This match will occur at a sub-term of $t$
(call it $t_0$) and will return an instantiation.  When this
instantiation is applied to the rewrite rule, the result will be a new
equation of the form \[
\vdash t_0 = r'
\]
Because the system then has a theorem expressing an equivalence for
$t_0$ it can create the new equation \[
  \vdash \underbrace{(\dots t_0\dots)}_t = (\dots r' \dots)
\]
The traversal of the term to be simplified is repeated until no
further matches for the simplifier's rewrite rules are found.  The
traversal strategy is
\begin{enumerate}
\item \label{enum:simp-traverse-toplevel}%
  While there are any matches for stored rewrite rules at this level,
  continue to apply them.  The order in which rewrite rules are
  applied can \emph{not} be relied on, except that when a \simpset{}
  includes two rewrites with exactly the same left-hand sides, the
  rewrite added later will get matched in preference.  (This allows a
  certain amount of rewrite-overloading in the construction of
  \simpset{}s.)
%% may not wish to own up to above detail
\item \label{enum:simp-traverse-recurse}%
  Recurse into the term's sub-terms.  The way in which terms are
  traversed at this step can be controlled by \emph{congruence
    rules}~(an advanced feature, see Section~\ref{sec:simp-congruences}
  below)
\item If step~\ref{enum:simp-traverse-recurse} changed the term at
  all, try another phase of rewriting at this level.  If this fails,
  or if there was no change from the traversal of the sub-terms, try
  any embedded decision procedures (see
  Section~\ref{sec:simp-embedding-code}).  If the rewriting phase or
  any of the decision procedures altered the term, return to
  step~\ref{enum:simp-traverse-toplevel}.  Otherwise, finish.
\end{enumerate}

\subsubsection{Conditional rewriting}
\index{simplification!conditional rewriting}

The above description is a slight simplification of the true state of
affairs.  One particularly powerful feature of the simplifier is that
it really uses \emph{conditional} rewrite rules.  These are theorems
of the form
\[
\vdash P \Rightarrow (\ell = r)
\]
When the simplifier finds a match for term $\ell$ during its traversal
of the term, it attempts to discharge the condition $P$.  If the
simplifier can simplify the term $P$ to truth, then the instance of
$\ell$ in the term being traversed can be replaced by the appropriate
instantiation of $r$.

When simplifying $P$ (a term that does not necessarily even occur in
the original), the simplifier may find itself applying another
conditional rewrite rule.  In order to stop excessive recursive
applications, the simplifier keeps track of a stack of all the
side-conditions it is working on.  The simplifier will give up on
side-condition proving if it notices a repetition in this stack.
There is also a user-accessible variable, \ml{Cond\_rewr.stack\_limit}
which specifies the maximum size of stack the simplifier is allowed to
use.

Conditional rewrites can be extremely useful.  For example, theorems
about division and modulus are frequently accompanied by conditions
requiring the divisor to be non-zero.  The simplifier can often
discharge these, particularly if it includes an arithmetic decision
procedure.  For example, the theorem \ml{MOD\_MOD} from the theory
\ml{arithmetic} states
\[
\vdash 0 < n \;\Rightarrow \; (k\,\textsf{MOD}\,n)\,\textsf{MOD}\,n = k
\,\textsf{MOD}\,n
\]
The simplifier (specifically, \ml{SIMP\_CONV~arith\_ss~[MOD\_MOD]})
can use this theorem to simplify the term
\holtxt{(k~MOD~(x~+~1))~MOD~(x~+~1)}: the arithmetic decision
procedure can prove that \holtxt{0 < x + 1}, justifying the rewrite.

Though conditional rewrites are powerful, not every theorem of the
form described above is an appropriate choice.  A badly chosen rewrite
may cause the simplifier's performance to degrade considerably, as it
wastes time attempting to prove impossible side-conditions.  For
example, the simplifier is not very good at finding existential
witnesses.  This means that the conditional rewrite \[
\vdash x < y \land y < z \Rightarrow (x < z = \top)
\]
will not work as one might hope.  In general, the simplifier is not a
good tool for performing transitivity reasoning.  (Try first-order
tools such as \ml{PROVE\_TAC} instead.)

\subsubsection{Generating rewrite rules from theorems}
\label{sec:generating-rewrite-rules}

There are two routes by which a theorem for rewriting can be passed to
the simplifier: either as an explicit argument to one of the \ML{}
functions (\ml{SIMP\_CONV}, \ml{ASM\_SIMP\_TAC} etc) that take theorem
lists as arguments, or by being included in a \simpset{} fragment
which is merged into a \simpset.  In both cases, these theorems are
transformed before being used.  The transformations applied are
designed to make interactive use as convenient as possible.

In particular, it is not necessary to pass the simplifier theorems
that are exactly of the form
\[
\vdash P \Rightarrow (\ell = r)
\]
Instead, the simplifier will transform its input theorems to extract
rewrites of this form itself.  The exact transformation performed is
dependent on the \simpset{} being used: each \simpset{} contains its
own ``filter'' function which is applied to theorems that are added to
it.  Most \simpset{}s use the filter function from the \ml{pure\_ss}
\simpset{} (see Section~\ref{sec:purebool-ss}).  However, when a
\simpset{} fragment is added to a full simpset, the fragment can
specify an additional filter component.  If specified, this function
is of type \ml{thm~->~thm~list}, and is applied to each of the
theorems produced by the existing \simpset's filter.

The rewrite-producing filter in \ml{pure\_ss} strips away
conjunctions, implications and universal quantifications until it has
either an equality theorem, or some other boolean form.  For example,
the theorem \ml{ADD\_MODULUS} states
\[
\vdash
\begin{array}[t]{l}
(\forall n\;x.\;\;0 < n \Rightarrow ((x + n)\,\textsf{MOD}\,n =
 x\,\textsf{MOD}\,n)) \;\;\land\\
(\forall n\;x.\;\;0 < n \Rightarrow ((n + x)\,\textsf{MOD}\,n =
 x\,\textsf{MOD}\,n))
\end{array}
\]
This theorem becomes two rewrite rules \[
\begin{array}{l}
\vdash 0 < n \Rightarrow ((x + n)\,\textsf{MOD}\,n)\\
\vdash 0 < n \Rightarrow ((n + x)\,\textsf{MOD}\,n)
\end{array}
\]

If looking at an equality where there are variables on the
right-hand side that do not occur on the left-hand side, the
simplifier transforms this to the rule \[
\vdash (\ell = r) = \top
\]
Similarly, if a boolean negation $\neg P$, becomes the rule \[
\vdash P = \bot
\]
and other boolean formulas $P$ become \[
\vdash P = \top
\]

Finally, if looking at an equality whose left-hand side is itself an
equality, and where the right-hand side is not an equality as well,
the simplifier transforms $(x = y) = P$ into the two rules
\[
\begin{array}{l}
\vdash (x = y) = P\\
\vdash (y = x) = P
\end{array}
\]
This is generally useful.  For example, a theorem such as
\[
\vdash \neg(\textsf{SUC}\,n = 0)
\]
will cause the simplifier to rewrite both $(\textsf{SUC}\,n = 0)$ and
$(0 = \textsf{SUC}\,n)$ to false.

The restriction that the right-hand side of such a rule not itself be
an equality is a simple heuristic that prevents some forms of looping.


\subsubsection{Matching rewrite rules}
\label{sec:simp-homatch}

Given a rewrite theorem $\vdash \ell = r$, the first stage of
performing a rewrite is determining whether or not $\ell$ can be
instantiated so as to make it equal to the term that is being
rewritten.  This process is known as matching.  For example, if $\ell$
is the term $\textsf{SUC}(n)$, then matching it against the term
$\textsf{SUC}(3)$ will succeed, and find the instantiation $n\mapsto
3$. In contrast with unification, matching is not symmetrical: a
pattern $\textsf{SUC}(3)$ will not match the term $\textsf{SUC}(n)$.

The simplifier uses a special form of higher-order matching.  If a
pattern includes a variable of some function type ($f$ say), and that
variable is applied to an argument $a$ that includes no variables
except those that are bound by an abstraction at a higher scope, then
the combined term $f(a)$ will match any term of the appropriate type
as long as the only occurrences of the bound variables in $a$ are in
sub-terms matching $a$.

Assume for the following examples that the variable $x$ is bound at a
higher scope.  Then, if $f(x)$ is to match $x + 4$, the variable $f$
will be instantiated to $(\lambda x.\; x + 4)$.  If $f(x)$ is to match
$3 + z$, then $f$ will be instantiated to $(\lambda x.\;3 + z)$.
Further $f(x + 1)$ matches $x + 1 < 7$, but does not match $x + 2 <
7$.

Higher-order matching of this sort makes it easy to express quantifier
movement results as rewrite rules, and have these rules applied by the
simplifier.  For example, the theorem
\[
\vdash (\exists x. \;P(x)\lor Q(x)) = (\exists x.\;P(x)) \lor (\exists
x.\;Q(x))
\]
has two variables of a function-type ($P$ and $Q$), and both are
applied to the bound variable $x$.  This means that when applied to
the input \[
\exists z. \;z < 4 \lor z + x = 5 * z
\]
the matcher will find the instantiation \[
\begin{array}{l}
P \mapsto (\lambda z.\;z < 4)\\
Q \mapsto (\lambda z.\;z + x = 5 * z)
\end{array}
\]

Performing this instantiation, and then doing some $\beta$-reduction
on the rewrite rule, produces the theorem\[
\vdash (\exists z. \;z < 4 \lor z + x = 5 * z) =
(\exists z. \;z < 4) \lor (\exists z.\;z + x = 5 * z)
\]
as required.

Another example of a rule that the simplifier will use successfully is
\[
\vdash f \circ (\lambda x.\; g(x)) = (\lambda x.\;f(g(x)))
\]
The presence of the abstraction on the left-hand side of the rule
requires an abstraction to appear in the term to be matched, so this
rule can be seen as an implementation of a method to move abstractions
up over function compositions.

An example of a possible left-hand side that will \emph{not} match as
generally as might be liked is $(\exists x.\;P(x + y))$.  This is
because the predicate $P$ is applied to an argument that includes the
free variable $y$.

\subsection{Advanced features}
\label{sec:advanced-simplifier}

This section describes some of the simplifier's advanced features.

\subsubsection{Congruence rules}
\label{sec:simp-congruences}
\index{simplification!congruence rules}

Congruence rules control the way the simplifier traverses a term.
They also provide a mechanism by which additional assumptions can be
added to the simplifier's context, representing information about the
containing context.  The simplest congruence rules are built into the
\ml{pure\_ss} simpset.  They specify how to traverse application and
abstraction terms.  At this fundamental level, these congruence rules
are little more than the rules of inference \ml{ABS}
\[
\Gamma \turn t_1 = t_2
\over
\Gamma \turn (\lquant{x}t_1) = (\lquant{x}t_2)
\]
(where $x\not\in\Gamma$) and \ml{MK\_COMB}
\[
\Gamma \turn f = g \qquad \qquad \Delta \turn x = y
\over
\Gamma \cup \Delta \turn f(x) = g(y)
\]
When specifying the action of the simplifier, these rules should be
read upwards.  With \ml{ABS}, for example, the rule says ``when
simplifying an abstraction, simplify the body $t_1$ to some new $t_2$,
and then the result is formed by re-abstracting with the bound
variable~$x$.''

Further congruence rules should be added to the simplifier in the form
of theorems, via the \ml{congs} field of the records passed to the
\ml{SSFRAG} constructor.  Such congruence rules should be of the form
\[
\mathit{cond_1} \Rightarrow \mathit{cond_2} \Rightarrow \dots (E_1 =
E_2)
\]
where $E_1$ is the form to be rewritten.  Each $\mathit{cond}_i$ can
either be an arbitrary boolean formula (in which case it is treated as
a side-condition to be discharged) or an equation of the general form
\[
\forall \vec{v}. \;\mathit{ctxt}_1 \Rightarrow \mathit{ctxt}_2
\Rightarrow \dots (V_1(\vec{v}) = V_2(\vec{v}))
\]
where the variable $V_2$ must occur free in $E_2$.

For example, the theorem form of \ml{MK\_COMB} would be
\[
\vdash (f = g) \Rightarrow (x = y) \Rightarrow (f(x) = g(y))
\]
and the theorem form of \ml{ABS} would be
\[
\vdash (\forall x. \;f (x) = g (x)) \Rightarrow (\lambda x. \;f(x)) = (\lambda
x.\;g(x))
\]
The form for \ml{ABS} demonstrates how it is possible for congruence
rules to handle bound variables.  Because the congruence rules are
matched with the higher-order match of Section~\ref{sec:simp-homatch},
this rule will match all possible abstraction terms.

These simple examples have not yet demonstrated the use of
$\mathit{ctxt}$ conditions on sub-equations.  An example of this is
the congruence rule (found in \ml{CONG\_ss}) for implications.  This
states
\[
\vdash (P = P') \Rightarrow (P' \Rightarrow (Q = Q')) \Rightarrow
(P \Rightarrow Q = P' \Rightarrow Q')
\]
This rule should be read: ``When simplifying $P\Rightarrow Q$, first
simplify $P$ to $P'$.  Then assume $P'$, and simplify $Q$ to $Q'$.
Then the result is $P' \Rightarrow Q'$.

The rule for conditional expressions is
\[
\vdash \begin{array}[t]{l}
  (P = P') \Rightarrow (P' \Rightarrow (x = x')) \Rightarrow
  (\neg P' \Rightarrow (y = y')) \;\Rightarrow\\
       (\textsf{if}\;P\;\textsf{then}\;x\;\textsf{else}\;y =
       \textsf{if}\;P'\;\textsf{then}\;x'\;\textsf{else}\;y')
\end{array}
\]
This rule allows the guard to be assumed when simplifying the
true-branch of the conditional, and its negation to be assumed when
simplifying the false-branch.

The contextual assumptions from congruence rules are turned into
rewrites using the mechanisms described in
Section~\ref{sec:generating-rewrite-rules}.

Congruence rules can be used to achieve a number of interesting
effects.  For example, congruence can specify that sub-terms
\emph{not} be simplified if desired.  This might be used to prevent
simplification of the branches of conditional expressions:
\[
\vdash (P = P') \Rightarrow
       (\textsf{if}\;P\;\textsf{then}\;x\;\textsf{else}\;y =
       \textsf{if}\;P'\;\textsf{then}\;x\;\textsf{else}\;y)
\]
If added to the simplifier, this rule will take precedence over any
other rules for conditional expressions (masking the one above from
\ml{CONG\_ss}, say), and will cause the simplifier to only descend
into the guard.  With the standard rewrites (from \ml{BOOL\_ss}):
\[
\begin{array}{l}
\vdash \;\textsf{if}\;\top\;\textsf{then}\;x\;\textsf{else}\;y \,\;=\,\; x\\
\vdash \;\textsf{if}\;\bot\;\textsf{then}\;x\;\textsf{else}\;y \,\;=\,\; y
\end{array}
\]
users can choose to have the simplifier completely ignore
a conditional's branches until that conditional's guard is simplified
to either true or false.


\subsubsection{AC-normalisation}
\index{simplification!AC-normalisation}

The simplifier can be used to normalise terms involving associative
and commutative constants.  This process is known as
\emph{AC-normalisation}.  The simplifier will perform AC-normalisation
for those constants which have their associativity and commutativity
theorems provided in a constituent \simpset{} fragment's \ml{ac}
field.

For example, the following \simpset{} fragment will cause
AC-normalisation of disjunctions
\begin{hol}
\begin{verbatim}
   SSFRAG {ac = [(DISJ_ASSOC, DISJ_COMM)],
           rewrs = [], filter = NONE, convs = [],
           dprocs = [], congs = []}
\end{verbatim}
\end{hol}
The pair of provided theorems must state
\begin{eqnarray*}
x \oplus y &=& y \oplus x\\
x \oplus (y \oplus z) &=& (x \oplus y) \oplus z
\end{eqnarray*}
for a constant $\oplus$.  The theorems may be universally quantified,
and the associativity theorem may be oriented either way.  Further,
either the associativity theorem or the commutativity theorem may be
the first component of the pair.  Assuming the \simpset{} fragment
above is bound to the \ML{} identifier \ml{DISJ\_ss}, its behaviour is
demonstrated in the following example:
\begin{session}
\begin{verbatim}
- SIMP_CONV (bool_ss ++ DISJ_ss) [] ``p /\ q \/ r \/ P z``;
<<HOL message: inventing new type variable names: 'a>>
> val it = |- p /\ q \/ r \/ P z = r \/ P z \/ p /\ q : thm
\end{verbatim}
\end{session}

\index{arith_ss (simplification set)@\ml{arith\_ss} (simplification set)}
The order of operands in the AC-normal form that the simplifer's
AC-normalisation works toward is unspecified.  However, the normal
form is always right-associated.  Note also that the \ml{arith\_ss}
\simpset, and the \ml{ARITH\_ss} fragment which is its basis, have
their own bespoke normalisation procedures for addition over the
natural numbers.  Mixing AC-normalisation, as described here, with
\ml{arith\_ss} can cause the simplifier to go into an infinite loop.

AC theorems can also be added to \simpset{}s via the theorem-list part
of the tactic and conversion interface, using the special rewrite form
\ml{AC}:
\begin{session}
\begin{hol}
\begin{verbatim}
- SIMP_CONV bool_ss [AC DISJ_ASSOC DISJ_COMM] ``p /\ q \/ r \/ P z``;
<<HOL message: inventing new type variable names: 'a>>
> val it = |- p /\ q \/ r \/ P z = r \/ P z \/ p /\ q : thm
\end{verbatim}
\end{hol}
\end{session}
See Section~\ref{sec:simp-special-rewrite-forms} for more on special
rewrite forms.

\subsubsection{Embedding code}
\label{sec:simp-embedding-code}

The simplifier features two different ways in which user-code can be
embedded into its traversal and simplification of input terms.  By
embedding their own code, users can customise the behaviour of the
simplifier to a significant extent.

\paragraph{User conversions}
The simpler of the two methods allows the simplifier to include
user-supplied conversions.  These are added to \simpset{}s in the
{convs} field of \simpset{} fragments.  This field takes lists of
values of type
\begin{hol}
\begin{verbatim}
   { name: string,
    trace: int,
      key: (term list * term) option,
     conv: (term list -> term -> thm) -> term list -> term -> thm}
\end{verbatim}
\end{hol}

The \ml{name} and \ml{trace} fields are used when simplifier tracing
is turned on.  If the conversion is applied, and if the simplifier
trace level is greater than or equal to the \ml{trace} field, then a
message about the conversion's application (including its \ml{name})
will be emitted.

The \ml{key} field of the above record is used to specify the
sub-terms to which the conversion should be applied.  If the value is
\ml{NONE}, then the conversion will be tried at every position.
Otherwise, the conversion is applied at term positions matching the
provided pattern.  The first component of the pattern is a list of
variables that should be treated as constants when finding pattern
matches.  The second component is the term pattern itself.  Matching
against this component is \emph{not} done by the higher-order match of
Section~\ref{sec:simp-homatch}, but by a higher-order ``term-net''.
This form of matching does not aim to be precise; it is used to
efficiently eliminate clearly impossible matches.  It does not check
types, and does not check multiple bindings.  This means that the
conversion will not only be applied to terms that are exact matches
for the supplied pattern.

Finally, the conversion itself.  Most uses of this facility are to add
normal \HOL{} conversions (of type \ml{term->thm}), and this can be
done by ignoring the \ml{conv} field's first two parameters.  For a
conversion \ml{myconv}, the standard idiom is to write
\ml{K~(K~myconv)}.  If the user desires, however, their code
\emph{can} refer to the first two parameters.  The second parameter is
the stack of side-conditions that have been attempted so far.  The
first enables the user's code to call back to the simplifier, passing
the stack of side-conditions, and a new side-condition to solve.  The
\ml{term} argument must be of type \holtxt{:bool}, and the recursive
call will simplify it to true (and call \ml{EQT\_ELIM} to turn a term
$t$ into the theorem $\vdash t$).  This restriction may be lifted in a
future version of \HOL{} but as it stands, the recursive call can
\emph{only} be used for side-condition discharge.  Note also that it
is the user's responsibility to pass an appropriately updated stack of
side-conditions to the recursive invocation of the simplifier.

A user-supplied conversion should never return the reflexive identity
(an instance of $\vdash t = t$).  This will cause the simplifier to
loop.  Rather than return such a result, raise a \ml{HOL\_ERR} or
\ml{Conv.UNCHANGED} exception.  (Both are treated the same by the simplifier.)

\paragraph{Context-aware decision procedures}
Another, more involved, method for embedding user code into the
simplifier is \emph{via} the \ml{dprocs} field of the \simpset{}
fragment structure.  This method is more general than adding
conversions, and also allows user code to construct and maintain its
own bespoke logical contexts.

The \ml{dprocs} field requires lists of values of the type
\ml{Traverse.reducer}.  These values are constructed with the
constructor \ml{REDUCER}:
\begin{hol}
\begin{verbatim}
   REDUCER : {initial : context,
              addcontext : context * thm list -> context,
              apply : {solver : term list -> term -> thm,
                       context : context,
                       stack : term list} -> term -> thm}
          -> reducer
\end{verbatim}
\end{hol}
The \ml{context} type is an alias for the built-in \ML{} type
\ml{exn}, that of exceptions.  The exceptions here are used as a
``universal type'', capable of storing data of any type.  For example,
if the desired data is a pair of an integer and a boolean, then the
following declaration could be made:
\begin{hol}
\begin{verbatim}
   exception my_data of int * bool
\end{verbatim}
\end{hol}
It is not necessary to make this declaration visible with a wide
scope.  Indeed, only functions accessing and creating contexts of this
form need to see it. For example:
\begin{hol}
\begin{verbatim}
  fun get_data c = (raise c) handle my_data (i,b) => (i,b)
  fun mk_ctxt (i,b) = my_data(i,b)
\end{verbatim}
\end{hol}

When creating a value of \ml{reducer} type, the user must provide an
initial context, and two functions.  The first, \ml{addcontext}, is
called by the simplifier's traversal mechanism to give every embedded
decision procedure access to theorems representing new context
information.  For example, this function is called with theorems from
the current assumptions in \ml{ASM\_SIMP\_TAC}, and with the theorems
from the theorem-list arguments to all of the various simplification
functions.  As a term is traversed, the congruence rules governing
this traversal may also provide additional theorems; these will also
be passed to the \ml{addcontext} function.  (Of course, it is entirely
up to the \ml{addcontext} function as to how these theorems will be
handled; they may even be ignored entirely.)

When an embedded reducer is applied to a term, the provided \ml{apply}
function is called.  As well as the term to be transformed, the
\ml{apply} function is also passed a record containing a
side-condition solver, the decision procedure's current context, and
the stack of side-conditions attempted so far.  The stack and solver
are the same as the additional arguments provided to user-supplied
conversions.  The power of the reducer abstraction is having access to
a context that can be built appropriately for each decision procedure.

Decision procedures are applied last when a term is encountered by the
simplifier.  More, they are applied \emph{after} the simplifier has
already recursed into any sub-terms and tried to do as much rewriting
as possible.  This means that although simplifier rewriting occurs in
a top-down fashion, decision procedures will be applied bottom-up and
only as a last resort.

As with user-conversions, decision procedures must raise an exception
rather than return instances of reflexivity.

\subsubsection{Special rewrite forms}
\label{sec:simp-special-rewrite-forms}

Some of the simplifier's features can be accessed in a relatively
simple way by using \ML{} functions to construct special theorem
forms.  These special theorems can then be passed in the
simplification tactics' theorem-list arguments.

Two of the simplifier's advanced features, AC-normalisation and
congruence rules can be accessed in this way.  Rather than construct a
custom \simpset{} fragment including the required AC or congruence
rules, the user can instead use the functions \ml{AC} or \ml{Cong}:
\begin{hol}
\begin{verbatim}
   AC : thm -> thm -> thm
   Cong : thm -> thm
\end{verbatim}
\end{hol}
For example, if the theorem value
\begin{hol}
\begin{verbatim}
   AC DISJ_ASSOC DISJ_COMM
\end{verbatim}
\end{hol}
appears amongst the theorems passed to a simplification tactic, then
the simplifier will perform AC-normalisation of disjunctions.  The
\ml{Cong} function provides a similar interface for the addition of
new congruence rules.

Two other functions provide a crude mechanism for controlling the
number of times an individual rewrite will be applied.
\begin{hol}
\begin{verbatim}
   Once : thm -> thm
   Ntimes : thm -> int -> thm
\end{verbatim}
\end{hol}
A theorem ``wrapped'' in the \ml{Once} function will only be applied
once when the simplifier runs.  A theorem wrapped in \ml{Ntimes} will
be applied as many times as given in the integer parameter.  These
restrictions apply across multiple applications of the simplifier to
different goals.  This means that if \ml{tac} in
\begin{hol}
\begin{verbatim}
   tac THEN SIMP_TAC bool_ss [Once th]
\end{verbatim}
\end{hol}
results in two sub-goals, then \ml{th} will only be used as a rewrite
in one of those goals.

\index{simplification|)}

\section{Efficient Applicative Order Reduction---\texttt{computeLib}}
\label{sec:computeLib}

Section \ref{sec:datatype} and Section \ref{TFL} show the ability of
\HOL{} to represent many of the standard constructs of functional
programming. If one then wants to `run' functional programs on
arguments, there are several choices. First, one could apply the
simplifier, as demonstrated in Section \ref{sec:simpLib}. This allows
all the power of the rewriting process to be brought to bear,
including, for example, the application of decision procedures to
prove constraints on conditional rewrite rules.  Second, one could
write the program, and all the programs it transitively depends on,
out to a file in a suitable concrete syntax, and invoke a compiler or
interpreter. This functionality is available in \HOL{} via use of
\ml{EmitML.exportML}.

Third, \ml{computeLib} can be used. This library supports call-by-value
evaluation of \HOL{} functions by deductive steps. In other words, it
is quite similar to having an \ML{} interpreter inside the \HOL{} logic,
working by forward inference. When used in this way, functional
programs can be executed more quickly than by using the simplifier.

The most accessible entrypoints for using the \ml{computeLib} library
are the conversion \ml{EVAL} and its tactic counterpart
\ml{EVAL\_TAC}.  These depend on an internal database that stores
function definitions. In the following example, loading \ml{sortingTheory}
augments this database with relevant definitions, that of Quicksort
(\holtxt{QSORT}) in particular, and then we can evaluate
\holtxt{QSORT} on a concrete list.
%
\setcounter{sessioncount}{0}
\begin{session}
\begin{hol}
\begin{verbatim}
  - load "sortingTheory";

  - EVAL ``QSORT (<=) [76;34;102;3;4]``;
  > val it = |- QSORT $<= [76; 34; 102; 3; 4] = [3; 4; 34; 76; 102] : thm
\end{verbatim}
\end{hol}
\end{session}
Often, the argument to a function has no variables: in that case
application of \ml{EVAL} ought to return a ground result,
as in the above example. However, \ml{EVAL} can also evaluate functions on
arguments with variables---so-called \emph{symbolic} evaluation---and
in that case, the behaviour of \ml{EVAL} depends on the structure of the
recursion equations. For example, in the following session, if there is
sufficient information in the input, symbolic execution can deliver
an interesting result. However, if there is not enough information
in the input to allow the algorithm any traction, no expansion will
take place.
%
\begin{session}
\begin{hol}
\begin{verbatim}
  - EVAL ``REVERSE [u;v;w;x;y;z]``;
  > val it = |- REVERSE [u; v; w; x; y; z] = [z; y; x; w; v; u] : thm

  - EVAL ``REVERSE alist``;
  > val it = |- REVERSE alist = REVERSE alist : thm
\end{verbatim}
\end{hol}
\end{session}
%

\subsection{Dealing with Divergence}

The major difficulty with using \ml{EVAL} is termination. All too
often, symbolic evaluation with \ml{EVAL} will diverge, or generate
enormous terms. The usual cause is conditionals with variables in the
test. For example, the following definition is provably equal to \holtxt{FACT},
%
\begin{session}
\begin{hol}
\begin{verbatim}
  Define `fact n = if n=0 then 1 else n * fact (n-1)`;
  > val it = |- fact n = (if n = 0 then 1 else n * fact (n - 1)) : thm
\end{verbatim}
\end{hol}
\end{session}
%
But the the two definitions evaluate completely differently.
%
\begin{session}
\begin{hol}
\begin{verbatim}
  EVAL ``FACT n``;
  > val it = |- FACT n = FACT n : thm

  - EVAL ``fact n``;
  <.... interrupt key struck ...>
  > Interrupted.
\end{verbatim}
\end{hol}
\end{session}
%
The primitive-recursive definition of \holtxt{FACT} does not expand
at all, while the destructor-style recursion of \holtxt{fact} never stops
expanding. A rudimentary monitoring facility shows the behaviour, first
on a ground argument, then on a symbolic argument.
%
\begin{session}
\begin{hol}
\begin{verbatim}
  - val [fact] = decls "fact";
  - computeLib.monitoring := SOME (same_const fact);

  - EVAL ``fact 4``;
  fact 4 = (if 4 = 0 then 1 else 4 * fact (4 - 1))
  fact 3 = (if 3 = 0 then 1 else 3 * fact (3 - 1))
  fact 2 = (if 2 = 0 then 1 else 2 * fact (2 - 1))
  fact 1 = (if 1 = 0 then 1 else 1 * fact (1 - 1))
  fact 0 = (if 0 = 0 then 1 else 0 * fact (0 - 1))
  > val it = |- fact 4 = 24 : thm

  - EVAL ``fact n``;
  fact n = (if n = 0 then 1 else n * fact (n - 1))
  fact (n - 1) = (if n - 1 = 0 then 1 else (n - 1) * fact (n - 1 - 1))
  fact (n - 1 - 1) =
  (if n - 1 - 1 = 0 then 1 else (n - 1 - 1) * fact (n - 1 - 1 - 1))
  fact (n - 1 - 1 - 1) =
  (if n - 1 - 1 - 1 = 0 then
     1
   else
     (n - 1 - 1 - 1) * fact (n - 1 - 1 - 1 - 1))
     .
     .
     .
\end{verbatim}
\end{hol}
\end{session}
%
In each recursive expansion, the test involves a variable, and hence
cannot be reduced to either \holtxt{T} or \holtxt{F}. Thus, expansion
never stops.

Some simple remedies can be adopted in trying to deal with
non-terminating symbolic evaluation.
\begin{itemize}
\item \ml{RESTR\_EVAL\_CONV} behaves like \ml{EVAL} except
  it takes an extra list of constants. During
  evaluation, if one of the supplied constants is encountered, it will
  not be expanded. This allows evaluation down to a specified level,
  and can be used to cut-off some looping evaluations.
\item \ml{set\_skip} can also be used to control
 evaluation. See the \REFERENCE{} entry for \ml{CBV\_CONV} for
 discussion of \ml{set\_skip}.

\end{itemize}

\paragraph{Custom evaluators}

For some problems, it is desirable to construct a customized
evaluator, specialized to a fixed set of definitions. The \ml{compset}
type found in \ml{computeLib} is the type of definition databases. The
functions \ml{new\_compset}, \ml{bool\_compset}, \ml{add\_funs}, and
\ml{add\_convs} provide the standard way to build up such
databases. Another quite useful \holtxt{compset} is
\ml{reduceLib.num\_compset}, which may be used for evaluating
terms with numbers and booleans.  Given a \ml{compset}, the function
\ml{CBV\_CONV} generates an evaluator: it is used to implement \ml{EVAL}.
See \REFERENCE{} for more details.

\paragraph{Dealing with Functions over Peano Numbers}

A function defined by pattern-matching over Peano-style numbers is not
in the right format for \ml{EVAL}, since the calculations will be
asymptotically inefficient. Instead, the same definition should be
used over numerals, which is a positional notation described in
Section \ref{numeral}. However, it is preferable for proofs to work
over Peano numbers. In order to bridge this gap, the function
\ml{numLib.SUC\_TO\_NUMERAL\_DEFN\_CONV} is used to convert a function
over Peano numbers to one over numerals, which is the format that
\ml{EVAL} prefers. \ml{Define} will automatically call
\ml{SUC\_TO\_NUMERAL\_DEFN\_CONV} on its result.

\paragraph{Storing definitions}

\ml{Define} automatically adds its definition to the global compset
used by \ml{EVAL} and \ml{EVAL\_TAC}. However, when \ml{Hol\_defn} is
used to define a function, its defining equations are not added to the
global compset until \ml{tprove} is used to prove the termination
constraints.  Moreover, \ml{tprove} does not add the definition
persistently into the global compset. Therefore, one must use
\ml{add\_persistent\_funs} in a theory to be sure that definitions
made by \ml{Hol\_defn} are available to \ml{EVAL} in descendant
theories.  Another point: one must call \ml{add\_persistent\_funs}
before \ml{export\_theory} is called.


\section{Arithmetic Libraries---\texttt{numLib}, \texttt{intLib} and \texttt{realLib}}
\label{sec:numLib}
\index{decision procedures!Presburger arithmetic over natural numbers}

Each of the arithmetic libraries of \HOL{} provide a
suite of definitions and theorems as well as automated inference support.

\paragraph{numLib}

The most basic numbers in \HOL{} are the natural numbers. The
\ml{numLib} library encompasses the theories \ml{numTheory},
\ml{prim\_recTheory}, \ml{arithmeticTheory}, and \ml{numeralTheory}.
This library also incorporates an evaluator for numeric expression
from \ml{reduceLib} and a decision procedure for linear arithmetic
\ml{ARITH\_CONV}. The evaluator and the decision procedure are
integrated into the simpset \ml{arith\_ss} used by the simplifier.
As well, the linear arithmetic decision procedure can be directly
invoked through \ml{DECIDE} and \ml{DECIDE\_TAC}, both found in
\ml{bossLib}.


\index{decision procedures!Presburger arithmetic over integers}
\paragraph{intLib}

The \ml{intLib} library comprises \ml{integerTheory}, an extensive
theory of the integers, plus two decision procedures
for full Presburger arithmetic. These are available as
\ml{intLib.COOPER\_CONV} and \ml{intLib.ARITH\_CONV}. These
decision procedures are able to deal with linear arithmetic
over the integers and the natural numbers, as well as dealing
with arbitrary alternation of quantifiers.  The \ml{ARITH\_CONV}
procedure is an implementation of the Omega Test, and seems to
generally perform better than Cooper's algorithm.  There are problems
for which this is not true however, so it is useful to have both
procedures available.

\paragraph{realLib}

The \ml{realLib} library provides a foundational development
of the real numbers and analysis. See Section \ref{reals}
for a quick description of the theories.
Also provided is a theory of polynomials, in \theoryimp{polyTheory}.
A decision procedure for linear arithmetic on the real numbers
is also provided by \ml{realLib}, under the name \ml{REAL\_ARITH\_CONV}
and \ml{REAL\_ARITH\_TAC}.


\section{The \texttt{HolSat} library}\label{sec:HolSatLib}
\input{HolSat.tex}

\section{The \texttt{HolBdd} library}\label{sec:HolBddLib}
\input{HolBdd.tex}

\section{The \texttt{HolCheck} library}\label{sec:HolCheckLib}
\input{holCheck.tex}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "description"
%%% End:
