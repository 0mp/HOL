\chapter{Libraries}\label{HOLlibraries}

\newcommand{\simpset}{simpset}

{
 \newcommand{\term}      {\mbox{\it term}}
 \newcommand{\IMP}       {\mbox{\tt ==>}}
 \newcommand{\ALL}       {\mbox{\tt !}}
 \newcommand{\EXISTS}    {\mbox{\tt ?}}
 \newcommand{\CHOOSE}    {\mbox{\tt @}}
 \newcommand{\EXISTSONE} {\mbox{\tt ?!}}
 \newcommand{\LET}       {\mbox{\tt let}}
 \newcommand{\und}       {\mbox{\tt and}}
 \newcommand{\IN}        {\mbox{\tt in}}
 \newcommand{\CONS}      {\mbox{\tt CONS}}
 \newcommand{\INSERT}    {\mbox{\tt INSERT}}
 \newcommand{\SUC}       {\mbox{\tt SUC}}
 \newcommand{\vstr}      {\mbox{\it vstr}}
 \newcommand{\numeral}   {\mbox{\it numeral}}
 \newcommand{\charseq}   {\mbox{\it charseq}}

A \emph{library} is intended to provide a higher level of
organization for HOL applications. In general, a library 
can contain a collection of theories, proof procedures, 
and supporting material, such as documentation. Some libraries
simply provide proof procedures, such as {\small\verb+simpLib+},
while others provide theories and proof procedures, such as 
{\small\verb+intLib+}. Libraries can include other libraries.

In the \HOL{} system, libraries are represented by ML structures named
following the convention that library \emph{x} will be found in the ML
structure {\small\verb+xLib+}. Loading this structure should load all
the relevant sub-components of the library and set whatever system
parameters are suitable for use of the library.

When the \HOL{} system is invoked in its normal configuration, several
useful libraries are automatically loaded. The most basic \HOL{}
library is {\small\verb+boolLib+}, which supports the definitions of
the \HOL{} logic, found in the theory \theoryimp{bool}, and provides a
useful suite of definition and reasoning tools. 

Another pervasively used library is found in the structure
{\small\verb+Parse+} (the reader can see that we are not strictly
faithful to our convention about library naming). The parser library
provides support for parsing and `pretty-printing' of HOL types,
terms, and theorems.

The verb+boss+ library provides a basic collection of standard
theories and high-level proof procedures, and serves as a standard
platform on which to work. It is preloaded and opened when the \HOL{}
system starts up. It includes {\small\verb+boolLib+} and
{\small\verb+Parse+}. Theories provided include \theoryimp{pair},
\theoryimp{sum}, \theoryimp{option}; the arithmetic theories
\theoryimp{num}, \theoryimp{prim\_rec}, \theoryimp{arithmetic},
and \theoryimp{numeral}; and \theoryimp{list}. Other libraries 
included in \verb+bossLib+ are \verb+goalstackLib+, which provides 
a proof manager for tactic proofs; \verb+simpLib+, which provides
a variety of simplifiers; \verb+numLib+, which provides a decision
procedure for arithmetic; \verb+Datatype+, which provides 
high-level support for defining algebraic datatypes; \verb+tflLib+,
which provides support for defining recursive functions.


\section{Parsing and Prettyprinting}

\HOL{} types and terms must be well-typed; the \HOL{} implementation
depends on the strong type system of SML to ensure this property.
Every type and term is ultimately built by application of the
primitive (abstract) constructors for types and terms. However, in
order to accomodate the wide variety of mathematical expression,
\HOL{} provides flexible infrastracture for parsing and prettyprinting
types and terms through the \verb+Parse+ structure.

The term parser supports type inference, overloading, binders, and
various fixity declaration (infix, prefix, postfix, and
combinations). There are also many flags for controlling the behaviour
of the parser. Further, the structure of the parser is exposed so that
new parsers can be quickly altered to support applications.

The parser is parameterized by grammars for types and terms. The
behaviour of the parser is therefore usually altered by grammar
manipulations. These can be of two kinds: \emph{temporary} or
\emph{permanent}.  Temporary changes do not persist after the current
session, while permanent changes will be in force in all descendant
theories.  Functions making temporary alterations are signified 
by a leading \verb+temp_+.

\subsection{Parsing Types}

There is little to say about the type parser. Infixes may be introduced.
The function \verb+type_grammar+ may be used to see the current grammar
for types.

\subsection{Parsing Terms}

The HOL grammar gets extended when a new constant is introduced. The
introduction of new constants is discussed in
Sections~\ref{sec:constant-definitions} and \ref{conspec}. 

\subsection{Type constraints}

A term can be constrained to be of a certain type.  For example,
\verb+X:bool+ constrains the variable \verb+X+ to have type
\verb+bool+. Similarly, \verb+T:bool+ performs a (vacuous) constraint
of the constant \verb+T+ to \verb+bool+. An attempt to constrain a
term inappropriately will raise an exception: for example,
\begin{verbatim}
  if T then (X:ind) else (Y:bool)
\end{verbatim}
will fail because both branches of a conditional must be of the same
type.  Type constraints can be seen as a suffix that binds more
tightly than everything except function application.  Thus $\term\
\ldots\ \term \ : \type$ is equal to $(\term\ \ldots\ \term)\ :
\type$, but $x < y:\mbox{\tt num}$ is a legitimate (though, again
redundant) constraint on just the variable $y$.

The inclusion of \verb+:+ in the symbolic identifiers means that some
constraints may need to be separated by white space. For example,
\begin{verbatim}
    $=:bool->bool->bool
\end{verbatim}
will be broken up by the HOL lexer as
\begin{verbatim}
    $=: bool -> bool -> bool
\end{verbatim}
and parsed as an application of the symbolic identifier \verb+$=:+ to
the argument list of terms [\verb+bool+, \verb+->+, \verb+bool+,
\verb+->+, \verb+bool+]. A well-placed space will avoid this problem:
\begin{verbatim}
    $= :bool->bool->bool
\end{verbatim}
is parsed as the symbolic identifier "=" constrained by a type.

\subsubsection{Type inference}

Consider the term \verb+x = T+.  Each term (and all of its subterms),
has a type in the HOL logic. Now, \verb+T+ has type \verb+bool+. This
means that the constant \verb+=+ has type \verb+xty -> bool -> bool+,
for some type \verb+xty+. Since the type scheme for \verb+=+ is
\verb+'a -> 'a -> bool+, we know that \verb+xty+ must in fact be
\verb+bool+ in order for the type instance to be well-formed. Knowing
this, we can deduce that the type of `x' must be \verb+bool+.

Ignoring the jargon ("scheme" and "instance") in the previous
paragraph, we have conducted a type assignment to the term structure,
ending up with a well-typed term. It would be very tedious for users
to conduct such argumentation by hand for each term entered to \HOL{}.
Thus, \HOL{} uses an adaptation of Milner's type inference algorithm
for ML when constructing terms via parsing. At the end of type
inference, unconstrained type variables get assigned by the system.
Usually, this assignment does the right thing. However, at times, the
most general type is not what is desired and the user must add type
constraints to the relevant subterms. For tricky situations, the
global variable \verb+show_types+ can be assigned. When this flag is
set, the prettyprinters for terms and theorems will show how types
have been assigned to subterms. If you do not want the system to
assign type variables for you, the global variable
\verb+guessing_tyvars+ can be set to \verb+false+, in which case the
existence of unassigned type variables at the end of type inference
will raise an exception.

\subsection{Overloading}

A limited amount of overloading resolution is performed by the quotation
parser for terms. For example, the tilde symbol ({\small\verb+~+})
denotes boolean negation in the initial theory of \HOL\, and it also denotes
the additive inverse in the {\small\verb+integer+} and
{\small\verb+real+} theories. If we load the {\small\verb+integer+}
theory and enter an ambiguous term featuring {\small\verb+~+}, the
system will inform us that overloading resolution is being performed.

\setcounter{sessioncount}{1}
\begin{session}
\begin{verbatim}
- load "integerTheory";
> val it = () : unit

- Term `~~x`;
<<HOL message: more than one resolution of overloading was possible.>>
> val it = `~~x` : term

- type_of it;
> val it = `:bool` : hol_type
\end{verbatim}
\end{session}

A priority mechanism is used to resolve multiple possible choices. In
the example, {\small\verb+~+} could be consistently chosen to have type
{\small\verb+:bool -> bool+} or {\small\verb+:int -> int+}, and the
mechanism has chosen the former. For finer control, explicit type
constraints may be used. In the following session, the
{\small\verb+~~x+} in the first quotation has type {\small\verb+:bool+},
while in the second, a type constraint ensures that {\small\verb+~~x+} has
type {\small\verb+:int+}.

\begin{session}
\begin{verbatim}
- show_types := true;
> val it = () : unit

- Term `~(x = ~~x)`;
<<HOL message: more than one resolution of overloading was possible.>>
> val it = `~((x :bool) = ~~x)` : term

- Term `~(x:int = ~~x)`;
> val it = `~((x :int) = ~~x)` : term
\end{verbatim}
\end{session}

Note that the symbol {\small\verb+~+} stands for two different constants in
the second quotation; its first occurrence is boolean negation, while
the other two occurrences are the additive inverse operation for integers.
For more information on how to set up and use overloading, consult
\REFERENCE.

In order to provide some notational flexibility, constants come in
various flavours or {\it fixities}: besides being an ordinary constant
(with a fixity of {\sf Prefix}), constants can also be {\it binders},
{\it true prefixes}\footnote{The use of the term ``true prefix'' is
forced upon us by the history of the system, which reserved the
classification ``prefix'' for terms without any special syntactic
features.}, {\it suffixes}, {\it infixes}, or {\it closefixes}.  More
generally, terms can also be represented using reasonably arbitrary
{\it mixfix} specifications.  The degree to which terms bind their
associated arguments is known as precedence.  The higher this number,
the tighter the binding.  For example, when introduced, \verb-+- has a
precedence of 500, while the tighter binding multiplication (\verb+*+)
has a precedence of 600.

\subsubsection{Binders}

A binder is a construct that binds a variable; for example, the
universal quantifier. In HOL, this is represented using a trick that
goes back to Alonzo Church: a binder is a constant that takes a lambda
abstraction as its argument. The lambda binding is used to implement
the binding of the construct. This is an elegant and uniform solution.
Thus the concrete syntax \verb+!v. M+ is represented by the
application of the constant \verb+!+ to the abstraction \verb+(\v. M)+.

The most common binders are \verb+!+, \verb+?+, \verb+?!+, and
\verb+@+. Sometimes one wants to iterate applications of the same
binder, \eg,
\begin{alltt}
  !x. !y. ?p. ?q. ?r. \term.
\end{alltt}
This can instead be rendered
\begin{alltt}
  !x y. ?p q r. \term.
\end{alltt}

\subsubsection{Infixes}

Infix constants can associate in one of three different ways: right,
left or not at all.  (If \verb-+- were non-associative, then {\tt 3 +
  4 + 5} would fail to parse; one would have to write {\tt (3 + 4) +
  5} or {\tt 3 + (4 + 5)} depending on the desired meaning).  The
precedence ordering for the initial set of infixes is \verb+/\+,
\verb+\/+, \verb+==>+, \verb+=+,
 \begin{Large}\verb+,+\end{Large} (comma\footnote{When {\tt pairTheory} has
   been loaded.}). Moreover, all of these constants are right
 associative. Thus
\begin{verbatim}
     X /\ Y ==> C \/ D, P = E, Q
\end{verbatim}
 is equal to
\begin{verbatim}
     ((X /\ Y) ==> (C \/ D)), ((P = E), Q).
\end{verbatim}

\noindent An expression \[\term\ \verb+<infix>+\ \term\] is internally
represented as \[((\verb+<infix>+\ \term)\ \term)\].

\subsubsection{True prefixes}

Where infixes appear between their arguments, true prefixes appear
before theirs.  This might initially appear to be the same thing as
happens with normal function application (is $f$ in $f(x)$ not acting
as a prefix?), but in fact, it is useful to allow for prefixes to have
binding power less than that associated with function application.  An
example of this is \verb+~+, logical negation.  This is a prefix with
lower precedence than function application.  Normally
\[
   f\;x\; y\qquad \mbox{is parsed as}\qquad (f\; x)\; y
\] but \[
  \mbox{\tt \~{}}\; x\; y\qquad\mbox{is parsed as}\qquad
  \mbox{\tt \~{}}\; (x\; y)
\] because the precedence of \verb+~+ is lower than that of function
application.  The unary negation symbol would also typically be
defined as a true prefix, if only to allow one to write \[ {\it
  negop}\,{\it negop}\,3
\] (whatever {\it negop} happened to be) without needing extra parentheses.

\subsubsection{Suffixes}

Suffixes appear after their arguments.  There are no suffixes
introduced into the standard theories available in HOL, but users are
always able to introduce their own if they choose.  Suffixes are
associated with a precedence just as infixes and true prefixes are.
If \verb+p+ is a true prefix, \verb+i+ an infix, and \verb+s+ a
suffix, then there are six possible orderings for the three different
operators based on their precedences, giving five parses for
$\verb+p+\; t_1\; \verb+i+\; t_2\; \verb+s+$ depending on the relative
precedences:
\[
\begin{array}{cl}
\mbox{\begin{tabular}{c}Precedences\\(lowest to highest)\end{tabular}} &
\multicolumn{1}{c}{\mbox{Parses}}\\
\hline
p,\;i,\;s & \verb+p+\;(t_1\;\verb+i+\;(t_2\;\verb+s+))\\
p,\;s,\;i & \verb+p+\;((t_1\;\verb+i+\;t_2)\;\verb+s+)\\
i,\;p,\;s & (\verb+p+\;t_1)\;\verb+i+\;(t_2\;\verb+s+)\\
i,\;s,\;p & (\verb+p+\;t_1)\;\verb+i+\;(t_2\;\verb+s+)\\
s,\;p,\;i & (\verb+p+\;(t_1\;\verb+i+\;t_2))\;\verb+s+\\
s,\;i,\;p & ((\verb+p+\;t_1)\;\verb+i+\;t_2)\;\verb+s+\\
\end{array}
\]

\subsubsection{Closefixes}

Closefix terms are operators that completely enclose their arguments.
An example one might use in the development of a theory of
denotational semantics is semantic brackets.  Thus, the HOL parsing
facilities can be configured to allow one to write \texttt{denotation x}
as \texttt{[| x |]}.  Closefixes are not associated with precedences
because they can not compete for arguments with other operators.

\subsection{Parser tricks and magic}

Here we describe how to achieve some useful effects with the
parser in \HOL{}.

\begin{description}
\item [Mix-fix syntax for {\it if-then-else}:] The first step in
  bringing this about is to look at the general shape of expressions
  of this form.  In this case, it will be:
  \[
  \mbox{\tt if}\;\; \dots \;\;\mbox{\tt then}\;\;\dots\;\;
  \mbox{\tt else}\;\;\dots
  \]
  Because there needs to be a ``dangling'' term to the right, the
  appropriate fixity is \texttt{TruePrefix}.  Knowing that the
  underlying term constant is called \texttt{COND}, the simplest way
  to achieve the desired syntax is:
  \begin{verbatim}
val _ = add_rule{term_name = "COND", fixity = TruePrefix 70,
                 pp_elements = [TOK "if", BreakSpace(1,0), TM,
                                BreakSpace(1,0),
                                TOK "then", BreakSpace(1,0), TM,
                                BreakSpace(1,0),
                                TOK "else", BreakSpace(1,0)],
                 paren_style = Always,
                 block_style =
                   (AroundEachPhrase, (PP.CONSISTENT, 0))};
\end{verbatim}
  \noindent The actual rule is slightly more complicated, and is
  in \texttt{src/bool/boolScript.sml}.


\item[Mix-fix syntax for term substitution:] Here the desire is to be able
  to write something like:
  \[
  \mbox{\texttt{[}}\,t_1\,\mbox{\texttt{/}}\,t_2\,\mbox{\texttt{]}}\,t_3
  \]
  denoting the substitution of $t_1$ for $t_2$ in $t_3$, perhaps
  translating to \mbox{\texttt{SUB} $t_1$ $t_2$ $t_3$}.  This looks
like it should be another \texttt{TruePrefix}, but the choice of the
square brackets (\texttt{[} and \texttt{]}) as delimiters would
conflict with the concrete syntax for list literals if this was done.
Given that list literals are effectively of the \texttt{CloseFix}
class, the new syntax must be of the same class.  This is easy enough
to do: we set up syntax \[
\mbox{\texttt{[}}\,t_1\,\mbox{\texttt{/}}\,t_2\,\mbox{\texttt{]}}
  \] to map to \mbox{\texttt{SUB} $t_1$ $t_2$} a value of a functional
  type, that when applied to a third argument will look
  right.\footnote{Note that doing the same thing for the
    \textit{if-then-else} example in the previous example would be
    inappropriate, as it would allow one to write \[
    \texttt{if}\,P\,\texttt{then}\,Q\,\texttt{else}
    \] without the trailing argument}
  The rule for this is thus:
  \begin{verbatim}
val _ = add_rule {term_name = "SUB", fixity = Closefix,
                  pp_elements = [TOK "[", TM, TOK "/", TM, TOK "]"],
                  paren_style = OnlyIfNecessary,
                  block_style =
                    (AroundEachPhrase, (PP.INCONSISTENT, 2))};
\end{verbatim}

\item[Aliasing] If one wants a special syntax to be an ``alias'' for a
  normal HOL form, this is easy to achieve; both examples so far have
  effectively done this.  However, if one just wants to have a normal
  one-for-one substitution of one string for another, one can't use
  the grammar/syntax phase of parsing to do this.  Instead, one can
  use the overloading mechanism.  For example, let us alias
  \texttt{MEM} for \texttt{IS\_EL}.  We need to use the function
  \texttt{overload\_on} to overload the original constant for the new
  name:
  \begin{verbatim}
val _ = overload_on ("MEM", Term`IS_EL`);
\end{verbatim}

\item[Making addition right associative] If one has a number of old
  scripts that assume addition is right associative because this is
  how HOL used to be, it might be too much pain to convert.  The trick
  is to remove all of the rules at the given level of the grammar, and
  put them back as right associative infixes.  The easiest way to tell
  what rules are in the grammar is by inspection (use
  \texttt{term\_grammar()}).  With just \texttt{arithmeticTheory}
  loaded, the only infixes at level 500 are \texttt{+} and
  \texttt{-}.  So, we remove the rules for them:
  \begin{verbatim}
val _ = app temp_remove_rules_for_term ["+", "-"];
\end{verbatim}
  \noindent And then we put them back with the appropriate
  associativity:
\begin{verbatim}
val _ = app (fn s => temp_add_infix(s, 500, RIGHT)) ["+", "-"];
\end{verbatim}
\noindent Note that we use the \texttt{temp\_} versions of these two
functions so that other theories depending on this one won't be
affected.  Further note that we can't have two infixes at the same
level of precedence with different associativities, so we have to
remove both operators, not just addition.

\end{description}

}

\subsection{Backwards compatibility of syntax}

This section of the manual documents the (extensive) changes made to
the parsing of HOL terms and types in the Taupo release (one of the
HOL3 releases) and beyond from the point of view of a user who doesn't
want to know how to use the new facilities, but wants to make sure
that their old code continues to work cleanly.

The changes which may cause old terms to fail to parse are:
\begin{itemize}
\newcommand\condexp{\mbox{$p \; {\tt =>} \; q \; {\tt |} \; r$}}
\item The precedence of type annotations has completely changed.  It
  is now a very tight suffix (though with a precedence weaker than
  that associated with function application), instead of a weak one.
  This means that \mbox{\tt (x,y:bool \# bool)} should now be written
  as \mbox{\tt (x,y):bool \# bool}. The previous form will now be
  parsed as a type annotation applying to just the \verb+y+.  This
  change brings the syntax of the logic closer to that of SML and
  should make it generally easier to annotate tuples, as one can now
  write \[ (x\,:\,\tau_1,\;y\,:\,\tau_2,\dots z\,:\,\tau_n)
  \] instead of \[
  (x\,:\,\tau_1, \;(y\,:\,\tau_2, \dots (z\,:\,\tau_n)))
  \] where extra parentheses have had to be added just to allow one to
  write a frequently occurring form of constraint.
\item Most arithmetic operators are now left associative instead of
  right associative.  In particular, $+$, $-$, $*$ and {\tt DIV} are
  all left associative.  Similarly, the analogous operators in other
  numeric theories such as {\tt integer} and {\tt real} are also left
  associative.  This brings the HOL parser in line with standard
  mathematical practice.
\item The binding equality in {\tt let} expressions is treated exactly
  the same way as equalities in other contexts.  In previous versions
  of HOL, equalities in this context have a different, weak binding
  precedence.  This difference can be seen in the following expression
  which parses successfully in the old version:
  \[ {\tt let} \; x \; = \; \condexp \; {\tt
  in} \;Q \] In Taupo releases and later, this expression will not
  parse because the conditional expression binds to the left more
  weakly than the equality binds to the right, and the parser ends up
  believing that the binding between the \verb+let+ and the \verb+in+
  is not an equality after all, as it should be.
\item Old style conditional expressions in the right half of set
  comprehensions have to be parenthesised to avoid confusing the
  parser.  Thus \[
  \{ \; x \; | \; \condexp \; \}
   \qquad\mbox{must be written} \qquad
  \{ \; x \; | \; (\condexp) \; \}
  \] Better yet, {\tt if}-{\tt then}-{\tt else} syntax could be used
  for the conditional expression.
\item Some lexical categories are more strictly policed.  String
  literals (strings inside double quotes) and numerals can't be used
  unless the relevant theories have been loaded.  Nor can these
  literals be used as variables inside binding scopes.
\end{itemize}


\section{A simple proof manager}\label{sec:goalstack}

The \texttt{goal stack} provides a simple interface to tactic-based
proof. When one uses tactics to decompose a proof, many intermediate
states arise; the goalstack takes care of the necessary bookeeping. The
implementation of goalstacks reported here is a re-design of Larry
Paulson's original conception.

The goalstack library is automatically loaded when \HOL\ starts up.

The abstract types \textit{goalstack} and \textit{proofs} are the
focus of backwards proof operations. The type \verb+proofs+ can be
regarded as a list of independent goalstacks. Most operations act on
the head of the list of goalstacks; there are operations so that the
focus can be changed.

\subsection{Starting a goalstack proof}

\begin{verbatim}
    g        : term quotation -> proofs
    set_goal : goal -> proofs
\end{verbatim}

Recall that the type \verb+goal+ is an abbreviation for
\verb+term list * term+. To start on a new goal, one gives
\verb+set_goal+ a goal. This creates a new goalstack and makes it the
focus of further operations.

A shorthand for \verb+set_goal+ is the function \verb+g+: it
invokes the parser automatically, and it doesn't allow the the goal to
have any assumptions.

Calling \verb+set_goal+, or \verb+g+, adds a new proof attempt to the
existing ones, \textit{i.e.}, rather than overwriting the current
proof attempt, the new attempt is stacked on top.

\subsection{Applying a tactic to a goal}

\begin{verbatim}
    expandf : tactic -> goalstack
    expand  : tactic -> goalstack
    e       : tactic -> goalstack
\end{verbatim}

How does one actually do a goalstack proof then? In most cases, the
application of tactics to the current goal is done with the function
\verb+expand+. In the rare case that one wants to apply an
{\it invalid\/} tactic, then \verb+expandf+ is used. (For an
explanation of invalid tactics, see Chapter 24 of Gordon \& Melham.) The
abbreviation \verb+e+ may also be used to expand a tactic.


\subsection{Undo}

\begin{verbatim}
    b          : unit -> goalstack
    drop       : unit -> proofs
    dropn      : int  -> proofs
    backup     : unit -> goalstack
    restart    : unit -> goalstack
    set_backup : int  -> unit
\end{verbatim}

Often (we are tempted to say {\it usually}!) one takes a wrong path
in doing a proof, or makes a mistake when setting a goal. To undo a step
in the goalstack, the function \verb+backup+ and its abbreviation
\verb+b+ are used. This will restore the goalstack to its previous
state.


To directly back up all the way to the original goal, the function
\verb+restart+ may be used. Obviously, it is also important to get
rid of proof attempts that are wrong; for that there is \verb+drop+,
which gets rid of the current proof attempt, and \verb+dropn+, which
eliminates the top $n$ proof attempts.


Each proof attempt has its own {\it undo-list\/} of previous
states. The undo-list for each attempt is of fixed size (initially
12). If you wish to set this value for the current proof attempt, the
function \verb+set_backup+ can be used. If the size of the backup
list is set to be smaller than it currently is, the undo list will be
immediately truncated. You can not undo a ``proofs-level'' operation, such
as \verb+set_goal+ or \verb+drop+.

\subsection{Viewing the state of the proof manager}

\begin{verbatim}
    p            : unit -> goalstack
    status       : unit -> proofs
    top_goal     : unit -> goal
    top_goals    : unit -> goal list
    initial_goal : unit -> goal
    top_thm      : unit -> thm
\end{verbatim}

To view the state of the proof manager at any time, the functions
\verb+p+ and \verb+status+ can be used. The former only shows
the top subgoals in the current goalstack, while the second gives a
summary of every proof attempt.

To get the top goal or goals of a proof attempt, use \verb+top_goal+
and \verb+top_goals+. To get the original goal of a proof attempt,
use \verb+initial_goal+.

Once a theorem has been proved, the goalstack that was used to derive it
still exists (including its undo-list): its main job now is to
hold the theorem. This theorem can be retrieved with
\verb+top_thm+.

\subsection{Switch focus to a different subgoal or proof attempt}

\begin{verbatim}
    r             : int -> goalstack
    R             : int -> proofs
    rotate        : int -> goalstack
    rotate_proofs : int -> proofs
\end{verbatim}

Often we want to switch our attention to a different goal in the current
proof, or a different proof. The functions that do this are
\verb+rotate+ and \verb+rotate_proofs+, respectively. The abbreviations
\verb+r+ and \verb+R+ are simpler to type in.

\section{The {\tt boss} library}
\label{sec:bossLib}
\newcommand\bossLib{{\tt bossLib}}

The library \bossLib\ marshalls some of the most widely used theorem
proving tools in \HOL\ and provides them with a convenient interface
for interaction. The library currently focuses on three things:
definition of datatypes and functions; high-level interactive proof
operations, and composition of automated reasoners. Loading \bossLib\
commits one to working in a context that already supplies the theories
of booleans, pairs, sums, the option type, arithmetic, and lists.


\subsection{Support for high-level proof steps}
\label{sec:high-level-proof-steps}

The following functions use information in the database to ease the
application of \HOL's underlying functionality:

\begin{verbatim}
     type_rws     : string -> thm list
     Induct       : tactic
     Cases        : tactic
     Cases_on     : term quotation -> tactic
     Induct_on    : term quotation -> tactic
\end{verbatim}

\begin{itemize}

\item
The function \texttt{type\_rws} will search for the given type
by name in the underlying database and return useful
rewrite rules for that type. The rewrite rules of the
datatype are built from the injectivity and distinctness theorems, along
with the case constant definition. The pre-existing rewrite rules in the
database are already integrated into the simplification sets provided
by \verb+bossLib+; however rewrite rules arising from an invocation of
\verb+Hol_datatype+, or which come from a user-defined theory, will have
to be manually added into the \simpset{}s used by the simplifier.

\item
The \texttt{Induct} tactic makes it convenient to invoke
induction. When it is applied to a goal, the leading universal
quantifier is examined; if its type is that of a known datatype, the
appropriate structural induction tactic is extracted and applied.

\item
The \texttt{Cases} tactic makes it convenient to invoke case
analysis. The leading universal quantifier in the goal is examined; if
its type is that of a known datatype, the appropriate structural
case analysis theorem is extracted and applied.

\item The \texttt{Cases\_on} tactic takes a quotation, which is
parsed into a term $M$, and then $M$ is searched for in the goal. If $M$
is a variable, then a variable with the same name is searched for. Once
the term to split over is known, its type and the associated facts are
obtained from the underlying database and used to perform the case
split. If some free variables of $M$ are bound in the goal, an attempt
is made to remove (universal) quantifiers so that the case split has
force. Finally, $M$ need not appear in the goal, although it should at
least contain some free variables already appearing in the goal. Note
that the \verb+Cases_on+ tactic is more general than \verb+Cases+, but
it does require an explicit term to be given.

\item The \texttt{Induct\_on} tactic takes a quotation, which is
parsed into a term $M$, and then $M$ is searched for in the goal. If $M$
is a variable, then a variable with the same name is searched for. Once
the term to induct on is known, its type and the associated facts are
obtained from the underlying database and used to perform the induction.
If $M$ is not a variable, a new variable $v$ not already occurring in
the goal is created, and used to build a term $v = M$ which the goal is
made conditional on before the induction is performed. First however,
all terms containing free variables from $M$ are moved from the
assumptions to the conclusion of the goal, and all free variables of $M$
are universally quantified. \verb+Induct_on+ is more general than
\verb+Induct+, but it does require an explicit term to be given.

\end{itemize}

Two supplementary entrypoints have been provided for more exotic
inductions:
\begin{description}
\item [\texttt{completeInduct\_on}] performs complete induction on the
  term denoted by the given quotation. Complete induction allows a
  seemingly \footnote{Complete induction and ordinary mathematical
    induction are each derivable from the other.} stronger induction
  hypothesis than ordinary mathematical induction: to wit, when
  inducting on $n$, one is allowed to assume the property holds for
  \emph{all} $m$ smaller than $n$. Formally: $\forall P.\ (\forall x.\
  (\forall y.\ y < x \supset P\, y) \supset P\,x) \supset \forall x.\
  P\,x$. This allows the inductive hypothesis to be used more than
  once, and also allows instantiating the inductive hypothesis to
  other than the predecessor.

\item [\texttt{measureInduct\_on}] takes a quotation, and breaks it
  apart to find a term and a measure function with which to induct.
  For example, if one wanted to induct on the length of a list
  \verb+L+, the invocation \ml{measureInduct\_on~`LENGTH L`}
  would be be appropriate.

\end{description}


\subsection{Automated reasoners}
\label{sec:automated-reasoners}

\verb+bossLib+ brings together the most powerful reasoners in \HOL{} and
tries to make it easy to compose them in a simple way. We take our basic
reasoners from \verb+mesonLib+, \verb+simpLib+, and \verb+numLib+,
but the point of \verb+bossLib+ is to provide a layer of abstraction so
the user has to know only a few entrypoints.\footnote{In the mid 1980's
Graham Birtwistle advocated such an approach, calling it `Ten Tactic
HOL'.} (These underlying libraries, and others providing similarly
powerful tools are described in detail in sections below.)

\begin{verbatim}
    PROVE      : thm list -> term quotation -> thm
    PROVE_TAC  : thm list -> tactic

    DECIDE     : term quotation -> thm
    DECIDE_TAC : tactic
\end{verbatim}

The inference rule \texttt{PROVE} (and the corresponding tactic
\texttt{PROVE\_TAC}) takes a list of theorems and a quotation, and
attempts to prove the term using a first order reasoner.  The
\texttt{PROVE} entry-points refer to the \texttt{meson} library, which
is further described in Section~\ref{sec:mesonLib} below. The
inference rule \texttt{DECIDE} (and the corresponding tactic
\texttt{DECIDE\_TAC}) applies a decision procedure that (at least)
handles statements of linear arithmetic.

\begin{verbatim}
    RW_TAC   : simpset -> thm list -> tactic
    SRW_TAC  : ssdata list -> thm list -> tactic
    &&       : simpset * thm list -> simpset  (* infix *)
    std_ss   : simpset
    arith_ss : simpset
    list_ss  : simpset
    srw_ss   : unit -> simpset
\end{verbatim}

The rewriting tactic \texttt{RW\_TAC} works by first adding the given
theorems into the given \simpset; then it simplifies the goal as much
as possible; then it performs case splits on any conditional
expressions in the goal; then it repeatedly (1) eliminates all
hypotheses of the form $v = M$ or $M = v$ where $v$ is a variable not
occurring in $M$, (2) breaks down any equations between constructor
terms occurring anywhere in the goal. Finally, \texttt{RW\_TAC} lifts
\texttt{let}-expressions within the goal so that the binding equations
appear as abbreviations\index{abbreviations!tactic-based proof} in the
assumptions.

The tactic \texttt{SRW\_TAC} is similar to \texttt{RW\_TAC}, but works
with respect to an underlying \simpset (accessible through the
function \texttt{srw\_ss}) that is updated as new context is loaded.
This \simpset{} can be augmented through the addition of ``\simpset{}
fragments'' (\texttt{ssdata} values) and theorems.  In situations
where there are many large types stored in the system,
\texttt{RW\_TAC}'s performance can suffer because it repeatedly adds
all of the rewrite theorems for the known types into a \simpset{}
before attacking the goal.  On the other hand, \texttt{SRW\_TAC} loads
rewrites into the \simpset{} underneath \texttt{srw\_ss()} just once,
making for faster operation in this situation.

\ml{bossLib} provides a number of simplification sets. In general,
these are extended versions of those found in \verb+simpLib+. The
simpset for pure logic, sums, pairs, and the \verb+option+ type is
named \verb+std_ss+. The simpset for arithmetic is named
\verb+arith_ss+, and the simpset for lists is named \verb+list_ss+.
The simpsets provided by {\tt bossLib} strictly increase in strength:
{\tt std\_ss} is contained in {\tt arith\_ss}, and {\tt arith\_ss} is
contained in {\tt list\_ss}.  The infix combinator \verb+&&+ is used
to build a new \simpset{} from a given \simpset{} and a list of
theorems. \HOL's simplification technology is described further in
Section~\ref{sec:simpLib} below and in the \REFERENCE.

\begin{verbatim}
    STP_TAC  : simpset -> tactic -> tactic
    ZAP_TAC  : simpset -> thm list -> tactic
\end{verbatim}

The compound reasoners of \verb+bossLib+ take a basic approach: they
simplify the goal as much as possible with \verb+RW_TAC+ and then a
`finishing' tactic is applied. The primitive entrypoint for this is
\texttt{STP\_TAC}. Currently, the most powerful reasoner is
\texttt{ZAP\_TAC}, which features a finishing tactic that first
tries a tautology checking tactic; if that fails, \verb+DECIDE_TAC+ is
called; if that fails, \verb+PROVE_TAC+ is called with the second
argument. Although this general approach (simplify as much as possible,
then apply automated reasoners in sequence) is crude, we have found that
it allows one to make good progress in a high percentage of proof
situations.

\begin{verbatim}
    by : term quotation * tactic -> tactic (* infix 8 *)
    SPOSE_NOT_THEN : (thm -> tactic) -> tactic
\end{verbatim}

The function \texttt{by} is an infix operator that takes a quotation
and a tactic $tac$. The quotation is parsed into a term $M$. When the
invocation ``$M\texttt{ by } \mathit{tac}$'' is applied to a goal
$(A,g)$, a new subgoal $(A,M)$ is created and $tac$ is applied to it.
If the goal is proved, the resulting theorem is broken down and added
to the assumptions of the original goal; thus the proof proceeds with
the goal $((M::A), g)$. (Note however, that case-splitting will happen
if the breaking-down of $\ \vdash M$ exposes disjunctions.) Thus
\texttt{by} allows a useful style of `assertional' or `Mizar-like'
reasoning to be mixed with ordinary tactic proof.\footnote{Proofs in
  the Mizar system are readable documents, unlike almost all
  tactic-based proofs.}


\texttt{SPOSE\_NOT\_THEN} initiates a proof by
contradiction by assuming the negation of the goal and driving the
negation inwards through quantifiers. It provides the resulting theorem
as an argument to the supplied function, which will use the theorem to
build and apply a tactic.

\section{The \texttt{meson} library}
\label{sec:mesonLib}

\index{decision procedures!first-order logic, meson@first-order logic, \ml{meson}}
The \texttt{meson} library is an implementation of the
model-elimination method for finding proofs of goals in first-order
logic.  There are three main entry-points:

\begin{verbatim}
   MESON_TAC     : thm list -> tactic
   ASM_MESON_TAC : thm list -> tactic
   GEN_MESON_TAC : int -> int -> int -> thm list -> tactic
\end{verbatim}

Each of these tactics attempts to prove the goal.  They will either
succeed in doing so, or fail with a ``depth exceeded'' exception.  If
the branching factor in the search-space is high, the \texttt{meson}
tactics may also take a very long time to reach the maximum depth.

All of the \texttt{meson} tactics take a list of theorems.  These
extra facts are used by the decision procedure to help prove the goal.
\texttt{MESON\_TAC} ignores the goal's assumptions; the other two
entry-points include the assumptions as part of the sequent to be
proved.

The extra parameters to \ml{GEN\_MESON\_TAC} provide extra control of
the behaviour of the iterative deepening that is at the heart of the
search for a proof.  In any given iteration, the algorithm searches
for a proof of depth no more than a parameter $d$.  The default
behaviour for \ml{MESON\_TAC} and \ml{ASM\_MESON\_TAC} is to start $d$
at 0, to increment it by one each time a search fails, and to fail if
$d$ exceeds the value stored in the reference value
\ml{mesonLib.max\_depth}.  By way of contrast,
\ml{GEN\_MESON\_TAC~min~max~step} starts $d$ at \ml{min}, increments
it by \ml{step}, and gives up when $d$ exceeds \ml{max}.



\section{The \texttt{simp} library}
\label{sec:simpLib}

\section{The \texttt{compute} library}
\label{sec:computeLib}

\section{The {\tt num} library}
\label{sec:numLib}
\index{decision procedures!Presburger arithmetic over natural numbers}

\section{The \texttt{HolSat} library}\label{sec:HolSatLib}
\section{The \texttt{HolBdd} library}\label{sec:HolBddLib}
\section{The \texttt{HolCheck} library}\label{sec:HolCheckLib}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "description"
%%% End:
